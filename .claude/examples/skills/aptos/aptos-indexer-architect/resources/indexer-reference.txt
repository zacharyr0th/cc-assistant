# Indexer

> Query indexed blockchain data using GraphQL API, create custom processors with the Indexer SDK, or stream raw transactions from Aptos blockchain

import { Aside, CardGrid, LinkCard } from '@astrojs/starlight/components';

import { GraphQLEditor } from '~/components/react/GraphQLEditor';

We have several offerings for getting indexed data from the Aptos blockchain.

1. Query the [Indexer API](/build/indexer/indexer-api) to get basic data about transactions, fungible assets, and tokens
2. Index your custom contract with the [Indexer SDK](/build/indexer/indexer-sdk)
3. Stream raw transactions from [Transaction Stream Service](/build/indexer/txn-stream) to your processor or service

## Indexer API

The Aptos Indexer is an API you can use to get:

1. Aggregate data (ex. How many NFTs exist?)
2. Historical data (ex. What transactions has this account submitted?)
3. Data that is hard to get from the simpler [Aptos Node API](/build/apis/fullnode-rest-api) (ex. What account owns a token named "ExampleToken"?).

For example, you can use the Indexer API to look up the fungible asset balances of any account like so:

<GraphQLEditor
  query={`query GetFungibleAssetBalances($address: String, $offset: Int) {
  current_fungible_asset_balances(
    where: { owner_address: { _eq: $address } }
    offset: $offset
    limit: 100
    order_by: { amount: desc }
  ) {
    asset_type
    amount
    __typename
  }
}`}
  variables={`{
  "address": "0x0000000000000000000000000000000000000000000000000000000000000001",
  "offset": 0
}`}
/>

<Aside type="note">
  The Indexer tracks every transaction that happens on-chain, then exposes that data through a GraphQL API.
</Aside>

## Using the Indexer API

Learn how to use the Indexer API, what each table represents, and the architecture.

<CardGrid>
  <LinkCard href="/build/indexer/indexer-api" title="Accessing the API" description="Learn how to query the Indexer API." />

  <LinkCard href="/build/indexer/indexer-api/indexer-reference" title="Indexer Table Reference" description="Detailed reference for Indexer tables and their schemas." />

  <LinkCard href="/build/indexer/indexer-api/architecture" title="Architecture" description="Detailed layout of the Indexer's architecture." />

  <LinkCard href="/build/indexer/indexer-api/self-hosted" title="Self-hosted Indexer API" description="Host your own Indexer API" />
</CardGrid>

### Example Queries

To help get you started, here are the most common queries the Indexer is used for.

<CardGrid>
  <LinkCard href="/build/indexer/indexer-api/fungible-asset-balances" title="Get Fungible Asset Balances" description="Get all fungible assets an account currently owns." />

  <LinkCard href="/build/indexer/indexer-api/account-transactions" title="Get Account Transactions" description="Get all transactions impacting an account." />

  <LinkCard href="/build/indexer/indexer-api/ans-lookup" title="Get Aptos Name" description="Retrieve the Aptos name associated with an account (via the ANS)." />

  <LinkCard href="/build/indexer/indexer-api/fungible-asset-info" title="Get Fungible Asset Info" description="Get detailed information about a specific fungible asset." />

  <LinkCard href="/build/indexer/indexer-api/get-nft-collections" title="Get NFT Collections" description="Retrieve NFT collections owned by a specific account." />

  <LinkCard href="/build/indexer/indexer-api/get-nfts" title="Get NFTs" description="Retrieve individual NFTs owned by a specific account." />

  <LinkCard href="/build/indexer/indexer-api/token-metadata" title="Get Token Metadata" description="Get metadata information for a specific token." />

  <LinkCard href="/build/indexer/indexer-api/get-delegators" title="Count Delegators in Staking Pool" description="Retrieve the number of active delegators in a staking pool." />
</CardGrid>

## Indexer SDK

If the hosted Indexer API is not enough or if you want to index your custom contract, you can create a processor with the [Indexer SDK](/build/indexer/indexer-sdk).

<CardGrid>
  <LinkCard href="/build/indexer/indexer-sdk/quickstart" title="Quickstart Guide" description="Get started with the Indexer SDK" />

  <LinkCard href="/build/indexer/indexer-sdk/documentation" title="Documentation" description="Read documentation about the Indexer SDK" />
</CardGrid>

## Transaction Stream Service

Transaction Stream Service is a GRPC service that streams raw transactions to your processor or service.
If you're using the Indexer SDK, you'll need an authorization token to connect to Transaction Stream Service.

<CardGrid>
  <LinkCard href="/build/indexer/txn-stream/aptos-hosted-txn-stream" title="Aptos-Hosted Transaction Stream Service" description="Get access to the Aptos-hosted Transaction Stream Service" />
</CardGrid>

## Legacy Indexer

Find information about the legacy indexer [here](/build/indexer/legacy).

# Indexer API Access

> Access Aptos Indexer GraphQL API for historical data, transactions, fungible assets, and tokens with SDK integration and direct endpoints

import { Aside, CardGrid, LinkCard } from '@astrojs/starlight/components';

{/* <IndexerBetaNotice /> */}

Aptos Labs hosts a public version of the Indexer GraphQL API that anyone can use to get basic historical and aggregate data about transactions, fungible assets, and tokens from on-chain.

You can explore it by hand by viewing the Hasura Explorer below for the network you are interested in.

You can also access the API via the GraphQL endpoints below. For more information on the format of data in each field / table, please see the [table reference page](/build/indexer/indexer-api/indexer-reference).

## SDK Access (Primary Method)

The primary way to use the Indexer is to access it through the [TypeScript SDK](/build/sdks/ts-sdk/fetch-data-via-sdk).

The TypeScript SDK will automatically handle rate limits, and can seamlessly allow for both [Fullnode REST API](/build/apis/fullnode-rest-api) access and Indexer access depending on what data is needed.

## Hasura Explorer (Manual Queries)

<Aside type="note">
  For detailed reference material about the contents of these tables, see the [Indexer Table Reference page](/build/indexer).
</Aside>

Choose a network to explore the free Aptos-Hosted Indexer API using the Hasura Explorer:

<CardGrid>
  <LinkCard href="https://cloud.hasura.io/public/graphiql?endpoint=https://api.mainnet.aptoslabs.com/v1/graphql" title="Mainnet" description="Hasura GraphQL Explorer for Aptos Mainnet" target="_blank" />

  <LinkCard href="https://cloud.hasura.io/public/graphiql?endpoint=https://api.testnet.aptoslabs.com/v1/graphql" title="Testnet" description="Hasura GraphQL Explorer for Aptos Testnet" target="_blank" />

  <LinkCard href="https://cloud.hasura.io/public/graphiql?endpoint=https://api.devnet.aptoslabs.com/v1/graphql" title="Devnet" description="Hasura GraphQL Explorer for Aptos Devnet" target="_blank" />
</CardGrid>

## GraphQL API Endpoints (Direct Access)

If you need to directly make GraphQL queries to the Aptos-Labs hosted Indexer API, then use the following endpoints:

- **Mainnet:** `https://api.mainnet.aptoslabs.com/v1/graphql`
- **Testnet:** `https://api.testnet.aptoslabs.com/v1/graphql`
- **Devnet:** `https://api.devnet.aptoslabs.com/v1/graphql`

### Rate limits

Learn more about the rate limits that apply to the Aptos Labs hosted indexer API by reading the [Geomi docs](https://geomi.dev/docs/admin/billing).

If you need a higher rate limit, consider the following solutions:

1. Get an API Key from [Geomi](https://geomi.dev/). Learn more about API keys at the [Geomi docs site](https://geomi.dev/docs/api-keys).
2. Run the Aptos Indexer API yourself. See the guide to self-hosting [here](/build/indexer/txn-stream/self-hosted).

# Get Account Transactions Data

> Retrieve historical transaction data for accounts using GraphQL queries with transaction versions and chronological ordering

import { GraphQLEditor } from '~/components/react/GraphQLEditor';

import { Aside } from '@astrojs/starlight/components';

`AccountTransactionsData` retrieves `transaction_version`s of transactions that affect a specified account address, ordered in descending order. `transaction_version` is a unique id given to each transaction on-chain that increases by 1 each time.

This query is essential for applications that require a historical log of transactions for audit, tracking, or display purposes.

<Aside type="note">
  Experiment and see the results! Modify the address, limit, and offset in the variables below to customize your query.
</Aside>

<GraphQLEditor
  query={`query GetAccountTransactionsData($address: String, $limit: Int) {
  account_transactions(
    where: { account_address: { _eq: $address } }
    order_by: { transaction_version: desc }
    limit: $limit
  ) {
    transaction_version
    __typename
  }
}`}
  variables={`{
  "address": "0xffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff",
  "limit": 10
}`}
/>

#### Variables:

- `$address`: **String** - The blockchain account address for which to query transaction data. Example: `"0x1abcdef1234567890abcdef1234567890abcdef1234567890abcdef1234567890abcdef"`.
- `$limit`: **Integer** - Specifies the maximum number of transaction versions to return. Helps manage the volume of data retrieved. Example: `10`.
- `$offset`: **Integer** - The offset from which to start fetching the transaction versions. Useful for paginating results. Example: `0`.

<br />

# Getting Recent Transactions

A helpful variant of the above query limits results to just ones that happened after a specific `transaction_version`. All results will have a `transaction_version` greater than `$gt`.

<GraphQLEditor
  query={`query GetAccountTransactionsData($address: String, $limit: Int, $gt: bigint) {
  account_transactions(
    where: {
      account_address: { _eq: $address }
      transaction_version: { _gt: $gt }
    }
    order_by: { transaction_version: desc }
    limit: $limit
  ) {
    transaction_version
    __typename
  }
}`}
  variables={`{
  "address": "0xffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff",
  "limit": 10,
  "gt": 599296148
}`}
/>

#### Variables:

- `$address`: **String** - The blockchain account address for which to query transaction data. Example: `"0xffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff"`.
- `$limit`: **Integer** - Specifies the maximum number of transaction versions to return. This helps limit the results to a manageable size. Example: `10`.
- `$gt`: **bigint** - The transaction version number above which transactions should be fetched. A transaction version is a sequentially increasing number that increments for every transaction.
  Transaction version 0 is the first transaction (genesis transaction), and a transaction version 100 is the 101st transaction in the blockchain.
  Example: `599296148`.

# Get Aptos Name From Address

> Look up registered Aptos Name Service (ANS) domain names for account addresses with reverse domain lookup functionality

import { GraphQLEditor } from '~/components/react/GraphQLEditor';

import { Aside } from '@astrojs/starlight/components';

`getNameFromAddress` looks for a registered domain name for a given account. For example, a user could register their account with the Aptos Name Service to be associated to `love.apt`. You can learn more by going to [https://www.aptosnames.com/](https://www.aptosnames.com/).

<Aside type="caution">
  This only returns _active_ names. Name registrations can expire if not renewed, which may explain some empty results.
</Aside>

<Aside type="note">
  Try it yourself! You can customize the variables at the bottom of the editor.
</Aside>

<GraphQLEditor
  query={`query getNameFromAddress($registered_address: String) {
  current_aptos_names(
    where: {
      registered_address: { _eq: $registered_address }
      is_active: { _eq: true }
    }
    order_by: [
      { is_primary: desc }
      { last_transaction_version: desc }
      { expiration_timestamp: desc }
    ]
    limit: 1
  ) {
    domain
    subdomain
  }
}`}
  variables={`{
  "registered_address": "0xca4349ce902a656570a4f344cc8f360fb13fd41b5fae77bcc9ee82252d67539e"
}`}
/>

#### Variables:

- `$registered_address`: **String** - The account address you want to find any associated active domain names for. Ex. `"0xca4349ce902a656570a4f344cc8f360fb13fd41b5fae77bcc9ee82252d67539e"`.

# Indexer Architecture

> Understanding Aptos Indexer architecture: Transaction Stream Service, custom processors, database integration, and API structure

import { ThemedImage } from '~/components/ThemedImage';

The Aptos Indexer stores data from on-chain (via the Transaction Stream Service). It indexes basic data about transactions, fungible assets, tokens, collections, accounts, ANS (Aptos Name Service) names, and more. Apps can query that data via the Indexer API.

Aptos Labs hosts a free version of the Indexer API to help the community get access to data such as:

1. Historical data - Ex. [What transactions have impacted this account?](/build/indexer/indexer-api/account-transactions)
2. Aggregate data - Ex. [How many delegators are in this staking pool?](/build/indexer/indexer-api/get-delegators)
3. Specific info best searched via query - Ex. [What NFTs does an account own?](/build/indexer/indexer-api/get-nfts)

### High Level Breakdown

Here is how the Indexer creates that API at a high-level:

<center>
  <ThemedImage
    alt="Signed Transaction Flow"
    sources={{
light: '~/images/indexer-architecture-light.svg',
dark: '~/images/indexer-architecture-dark.svg',
}}
  />
</center>

The Indexer uses the [Transaction Stream Service](/build/indexer/txn-stream) and custom processors written with the [Indexer SDK](/build/indexer/indexer-sdk) to update a database with rich tables. Then it exposes an API for Aptos apps to access the consolidated data.

For situations where you need to go beyond the Aptos hosted Indexer API data, you will want to create a custom processor with the [Indexer SDK](/build/indexer/indexer-sdk).

Writing a custom processor can help you:

1. Get access to different types of data.
2. Store additional information beyond what the Aptos Labs hosted Indexer API is saving.
3. Change how transactions are processed.

If you would like to operate your own Indexer API as a service, see how to [host your own Indexer](/build/indexer/indexer-api/self-hosted).

## Detailed Overview

You can use the below diagram for a much more in-depth diagram explaining how the Indexer code actually works behind the scenes.

<div style={{textAlign:"center"}}>
  <div style={{marginBottom: 20}}>
    <iframe style={{border: "1px solid rgba(0, 0, 0, 0.1)", width: "100%", height: "750px"}} src="https://embed.figma.com/board/sVhSOGR7ZT4CdeUzlXyduD/Detailed-Overview?node-id=18675-303&embed-host=share" allowfullscreen />
  </div>
</div>

# Get Fungible Asset Balances

> Query current fungible asset balances for accounts with real-time holdings data and backwards compatibility for coins

import { GraphQLEditor } from '~/components/react/GraphQLEditor';

import { Aside } from '@astrojs/starlight/components';

`GetFungibleAssetBalances` retrieves the current balances for various fungible asset types associated with a specified account address. This is crucial for platforms requiring real-time information on account asset holdings. This is backwards compatible with looking up `Coin`s.

<Aside type="note">
  Try it yourself! Adjust the query variables below in the editor to fetch data for different addresses.
</Aside>

<GraphQLEditor
  query={`query GetFungibleAssetBalances(
  $address: String
  $offset: Int
  $token_standard: String
) {
  current_fungible_asset_balances(
    where: {
      owner_address: { _eq: $address }
      token_standard: { _eq: $token_standard }
    }
    offset: $offset
    limit: 100
    order_by: { amount: desc }
  ) {
    asset_type
    amount
    __typename
  }
}`}
  variables={`{
  "address": "0x0000000000000000000000000000000000000000000000000000000000000001",
  "token_standard": "v1",
  "offset": 0
}`}
/>

#### Variables:

- `$address`: **String** - The account address for which to fetch fungible asset balances. Example: `"0x0000000000000000000000000000000000000000000000000000000000000001"`.
- `token_standard`: **String** - The token standard for the asset: `"v1"` is the previous token standard and `"v2"` is the new standard.
- `$offset`: **Integer** (Optional) - The pagination offset to start fetching balances from. Default: `0`.

# Get Fungible Asset Info

> Query fungible asset information including symbol, name, decimals, and asset types with backwards compatibility for coins

import { GraphQLEditor } from '~/components/react/GraphQLEditor';

import { Aside } from '@astrojs/starlight/components';

By providing a list of fungible asset types, `GetFungibleAssetInfo` can fetch data such as the symbol, name, decimals, and the asset type itself. This is particularly useful for applications needing to display token details. This is backwards compatible with looking up `Coin` info.

<Aside type="note">
  Try it yourself! You can customize the variables at the bottom of the editor.
</Aside>

<GraphQLEditor
  query={`query GetFungibleAssetInfo($in: [String!], $offset: Int) {
  fungible_asset_metadata(
    where: { asset_type: { _in: $in } }
    offset: $offset
    limit: 100
  ) {
    symbol
    name
    decimals
    asset_type
    __typename
  }
}`}
  variables={`{
  "in": ["0x1::aptos_coin::AptosCoin", "0x1::example_coin::ExampleCoin"],
  "offset": 0
}`}
/>

## Variables:

- `$in`: **List of String** - This variable should contain a list of fungible asset types you want to query information for. Ex. `["0x1::aptos_coin::AptosCoin"]`
- `$offset`: **Integer** (Optional) - This variable can be used to paginate through results, specifying how many records to skip before starting to return results.

# Count Number of Active Delegators for a Pool

> Query active delegator counts for staking pools with GraphQL API for delegation pool participation analysis

import { GraphQLEditor } from '~/components/react/GraphQLEditor';

import { Aside } from '@astrojs/starlight/components';

`getNumberOfDelegators` retrieves the count of active delegators for a specified pool address. This query is especially valuable for platforms that need to analyze participation in staking or delegation pools.

<Aside type="note">
  Try it now! Customize the variable below in the editor to explore different pool addresses.
</Aside>

<GraphQLEditor
  query={`query getNumberOfDelegators($poolAddress: String) {
  num_active_delegator_per_pool(
    where: {
      pool_address: { _eq: $poolAddress }
      num_active_delegator: { _gt: "0" }
    }
    distinct_on: pool_address
  ) {
    num_active_delegator
  }
}`}
  variables={`{
  "poolAddress": "0x06099edbe54f242bad50020dfd67646b1e46282999483e7064e70f02f7ea3c15"
}`}
/>

#### Variables:

- `$poolAddress`: **String** - The address of the pool for which to query the number of active delegators. Example: `"0x06099edbe54f242bad50020dfd67646b1e46282999483e7064e70f02f7ea3c15"`.

# Retrieve NFT Collections Owned by an Account

> Fetch NFT collection details owned by accounts including collection metadata, token counts, and comprehensive collection attributes

import { GraphQLEditor } from '~/components/react/GraphQLEditor';

import { Aside } from '@astrojs/starlight/components';

`GetAccountNftCollections` fetches detailed information about NFT collections owned by a specific account address. This includes data such as the collection ID, distinct token count within each collection, and detailed attributes of each collection. It's particularly useful for applications that need to display comprehensive details about NFT holdings associated with an account.

<Aside type="note">
  Try it yourself! You can customize the variables at the bottom of the editor.
</Aside>

<GraphQLEditor
  query={`query GetAccountNftCollections($address: String) {
  current_collection_ownership_v2_view(
    where: { owner_address: { _eq: $address } }
    limit: 1000000
    offset: 0
    order_by: [{ last_transaction_version: desc }, { collection_id: asc }]
  ) {
    collection_id
    distinct_tokens
    last_transaction_version
    owner_address
    current_collection {
      collection_id
      collection_name
      creator_address
      current_supply
      description
      last_transaction_timestamp
      last_transaction_version
      max_supply
      mutable_description
      mutable_uri
      table_handle_v1
      token_standard
      total_minted_v2
      uri
      __typename
    }
    __typename
  }
}`}
  variables={`{
  "address": "0x8824ebb6e0d60656f6d4d5bbc408805d9ca6b984aad78b16f42b1dae545d6762"
}`}
/>

#### Variables:

- `$address`: **String** - The Aptos account address for which you want to query NFT collection data. Ex. `"0x8824ebb6e0d60656f6d4d5bbc408805d9ca6b984aad78b16f42b1dae545d6762"`

# Get NFTs Owned by an Account

> Query NFTs owned by an account using GraphQL API with comprehensive token details, collection metadata, and ownership information

import { GraphQLEditor } from '~/components/react/GraphQLEditor';

import { Aside } from '@astrojs/starlight/components';

`GetAccountNfts` retrieves a list of NFTs owned by a specified account address. This query provides comprehensive details about each token, including its collection details, description, and unique identifiers. It is ideal for platforms that need to display the NFT portfolios of users or provide insights into the NFT market holdings.

<Aside type="note">
  Try it yourself! You can customize the variables at the bottom of the editor.
</Aside>

<GraphQLEditor
  query={`query GetAccountNfts($address: String) {
  current_token_ownerships_v2(
    where: { owner_address: { _eq: $address }, amount: { _gt: "0" } }
  ) {
    current_token_data {
      collection_id
      largest_property_version_v1
      current_collection {
        collection_id
        collection_name
        description
        creator_address
        uri
        __typename
      }
      description
      token_name
      token_data_id
      token_standard
      token_uri
      __typename
    }
    owner_address
    amount
    __typename
  }
}`}
  variables={`{
  "address": "0x8824ebb6e0d60656f6d4d5bbc408805d9ca6b984aad78b16f42b1dae545d6762"
}`}
/>

#### Variables:

- `$address`: **String** - The Aptos account address for which you want to query NFT data. Ex. `"0x8824ebb6e0d60656f6d4d5bbc408805d9ca6b984aad78b16f42b1dae545d6762"`

# Indexer API Reference

> Complete GraphQL API reference for Aptos Indexer with table schemas, query examples, and field documentation for tokens and accounts

import { Aside } from '@astrojs/starlight/components';

The Indexer API allows you to access rich data about tokens, fungible assets, and accounts on-chain using GraphQL queries. **You can access it [here](/build/indexer/indexer-api).**

For common queries, check out the sidebar for examples to work from. When building your own, this reference guide should help you determine which tables are most relevant, and how to format your queries.

<Aside type="caution">
  Before relying on a table for production services, check the bottom of this page to see if that table is deprecated. If so, use the note section for guidance on what to do to migrate to a non-deprecated table.
</Aside>

<Aside type="note">
  If you are looking up a table with the `_by_pk` suffix, search for the table name without that suffix. `_by_pk` tables are automatically generated for convenience to allow querying by primary key.
</Aside>

<br />

# Indexer Table Reference

<Aside type="note">
  Remember to use Ctrl + F to find the table you are interested in! When in doubt, you may also want to query the Hasura tables linked in the [Indexer API Access](/build/indexer/indexer-api) page to see examples of the data inside.
</Aside>

## Filtering (with `where` clauses)

To ensure your queries filter data efficiently, check out the available indexes for each table.
Some indexes are composite B-tree indexes, meaning they consist of multiple columns.
B-tree indexes are ordered and perform optimally when queries utilize a left-most prefix of the indexed columns.

{/* Indexed columns are generated from https://docs.google.com/spreadsheets/d/1sQnMimoJnP5sVaLCCRK4H6AHXGCwsehAuBF9AD_0IyY/edit?gid=47194642#gid=47194642 */}

## General

### `user_transactions`

Transactions filtered to user\_transactions (not system).

| Index Name                                        | Indexed Columns          |
| ------------------------------------------------- | ------------------------ |
| user\_transactions\_pkey                          | version                  |
| user\_transactions\_sender\_sequence\_number\_key | sender, sequence\_number |
| ut\_epoch\_index                                  | epoch                    |
| ut\_insat\_index                                  | inserted\_at             |
| ut\_sender\_seq\_index                            | sender, sequence\_number |

### `block_metadata_transactions`

A type of system transaction emitted once per block, useful for mapping to timestamp or epoch.

| Index Name                                        | Indexed Columns |
| ------------------------------------------------- | --------------- |
| block\_metadata\_transactions\_block\_height\_key | block\_height   |
| block\_metadata\_transactions\_pkey               | version         |
| bmt\_insat\_index                                 | inserted\_at    |

### `account_transactions`

_Has an aggregate view for summary data called `account_transactions_aggregate`_

| Index Name                  | Indexed Columns                        |
| --------------------------- | -------------------------------------- |
| account\_transactions\_pkey | account\_address, transaction\_version |
| at\_insat\_index            | inserted\_at                           |
| at\_version\_index          | transaction\_version DESC              |

This table maps accounts and transactions that interact with that account.

| Field                            | Type    | Primary Key | Description                                                                                                                                       |
| -------------------------------- | ------- | ----------- | ------------------------------------------------------------------------------------------------------------------------------------------------- |
| delegated\_staking\_activities   | Join    |             | Use the [Hasura explorer](https://cloud.hasura.io/public/graphiql?endpoint=https://api.mainnet.aptoslabs.com/v1/graphql) to see these sub-fields. |
| fungible\_asset\_activities      | Join    |             | References [fungible\_asset\_activities](#fungible_asset_activities).                                                                             |
| token\_activities                | Join    |             | Use the [Hasura explorer](https://cloud.hasura.io/public/graphiql?endpoint=https://api.mainnet.aptoslabs.com/v1/graphql) to see these sub-fields. |
| token\_activities\_aggregate     | Join    |             | Use the [Hasura explorer](https://cloud.hasura.io/public/graphiql?endpoint=https://api.mainnet.aptoslabs.com/v1/graphql) to see these sub-fields. |
| token\_activities\_v2            | Join    |             | References [token\_activities\_v2](#token_activities_v2).                                                                                         |
| token\_activities\_v2\_aggregate | Join    |             | References [token\_activities\_v2](#token_activities_v2).                                                                                         |
| account\_address                 | String! | Yes         | This is an Aptos account address. Ex. "0x50bc83f01d48ab3b9c00048542332201ab9cbbea61bda5f48bf81dc506caa78a"                                        |
| transaction\_version             | bigint! | Yes         | Blockchain version of the transaction. Ex. 10000000                                                                                               |

### `ledger_infos`

This table shares what chain is currently being queried.

| Field     | Type | Primary Key | Description                                                                                       |
| --------- | ---- | ----------- | ------------------------------------------------------------------------------------------------- |
| chain\_id | int  | Yes         | The unique identifier for the chain you are accessing. Ex. 1 (for Mainnet), 2 (for Testnet), etc. |

### `processor_status`

This table shares how current this processor's data is.

gives you latest version processed per “processor”

| Field                        | Type   | Primary Key | Description                                                                              |
| ---------------------------- | ------ | ----------- | ---------------------------------------------------------------------------------------- |
| last\_success\_version       | bigint | Yes         | The version number of the last successful processor run. Ex. 5000000                     |
| last\_transaction\_timestamp | String |             | Timestamp of the last processed transaction. Ex. "2024-04-17T02:14:25.68771"             |
| last\_updated                | String |             | Timestamp of the last update to this processor's status. Ex. "2024-04-17T02:14:25.68771" |
| processor                    | String | Yes         | Name of the processor. Ex. "transaction\_processor"                                      |

## NFT

### `token_activities_v2`

_Has an aggregate view for summary data called `token_activities_v2_aggregate`_

| Index Name                  | Indexed Columns                    |
| --------------------------- | ---------------------------------- |
| ta2\_from\_type\_index      | from\_address, type                |
| ta2\_insat\_index           | inserted\_at                       |
| ta2\_owner\_type\_index     | event\_account\_address, type      |
| ta2\_tid\_index             | token\_data\_id                    |
| ta2\_to\_type\_index        | to\_address, type                  |
| token\_activities\_v2\_pkey | transaction\_version, event\_index |

This table tracks token activities and is especially useful for tracking NFT activity. This includes both v1 and v2 data.

| Field                         | Type    | Primary Key | Description                                                                                                                                                                                                                                                                                                |
| ----------------------------- | ------- | ----------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| aptos\_names\_from            | Join    |             | Use the [Hasura explorer](https://cloud.hasura.io/public/graphiql?endpoint=https://api.mainnet.aptoslabs.com/v1/graphql) to see these sub-fields.                                                                                                                                                          |
| aptos\_names\_from\_aggregate | Join    |             | Use the [Hasura explorer](https://cloud.hasura.io/public/graphiql?endpoint=https://api.mainnet.aptoslabs.com/v1/graphql) to see these sub-fields.                                                                                                                                                          |
| aptos\_names\_to              | Join    |             | Use the [Hasura explorer](https://cloud.hasura.io/public/graphiql?endpoint=https://api.mainnet.aptoslabs.com/v1/graphql) to see these sub-fields.                                                                                                                                                          |
| aptos\_names\_to\_aggregate   | Join    |             | Use the [Hasura explorer](https://cloud.hasura.io/public/graphiql?endpoint=https://api.mainnet.aptoslabs.com/v1/graphql) to see these sub-fields.                                                                                                                                                          |
| current\_token\_data          | Join    |             | Use the [Hasura explorer](https://cloud.hasura.io/public/graphiql?endpoint=https://api.mainnet.aptoslabs.com/v1/graphql) to see these sub-fields.                                                                                                                                                          |
| after\_value                  | String  |             | The value of a token property after the transaction. Ex. "100"                                                                                                                                                                                                                                             |
| before\_value                 | String  |             | The value of a token property before the transaction. Ex. "50"                                                                                                                                                                                                                                             |
| entry\_function\_id\_str      | String  |             | The identifier of the function called in this transaction. Ex. "0x1::aptos\_account::transfer"                                                                                                                                                                                                             |
| event\_account\_address       | String  |             | This is an Aptos account address related to the event. This address must be 66 characters long, and so may be 0 padded. Ex. "0x50bc83f01d48ab3b9c00048542332201ab9cbbea61bda5f48bf81dc506caa78a"                                                                                                           |
| event\_index                  | bigint  | Yes         | Index of the event within the transaction. Ex. 1                                                                                                                                                                                                                                                           |
| from\_address                 | String  |             | This is an Aptos account address from which the token was sent. This address must be 66 characters long, and so may be 0 padded. Ex. "0x50bc83f01d48ab3b9c00048542332201ab9cbbea61bda5f48bf81dc506caa78a"                                                                                                  |
| is\_fungible\_v2              | Boolean |             | Indicates whether the token is fungible. Soon to be deprecated. Ex. False for NFTs.                                                                                                                                                                                                                        |
| property\_version\_v1         | bigint  |             | The version of the token's properties under schema version 1. This field is only for token standard v1. It is always 0 for v2. Ex. 0                                                                                                                                                                       |
| to\_address                   | String  |             | This is an Aptos account address to which the token was sent. This address must be 66 characters long, and so may be 0 padded. Ex. "0x50bc83f01d48ab3b9c00048542332201ab9cbbea61bda5f48bf81dc506caa78a"                                                                                                    |
| token\_amount                 | bigint  |             | The amount of the token transferred in this activity. Ex. 3                                                                                                                                                                                                                                                |
| token\_data\_id               | String  |             | Unique identifier for this particular token's data. For token standard v1, this is derived from a combination of creator\_address, collection\_name, and token\_name. This ID must be 66 characters long, and so may be 0 padded. Ex. "0x50bc83f01d48ab3b9c00048542332201ab9cbbea61bda5f48bf81dc506caa78a" |
| token\_standard               | String  |             | Aptos standard that the collection adheres to. Ex. "v1"                                                                                                                                                                                                                                                    |
| transaction\_timestamp        | String  |             | Timestamp when the transaction occurred. Ex. "2024-04-17T02:14:25.68771"                                                                                                                                                                                                                                   |
| transaction\_version          | bigint  | Yes         | Blockchain version of the transaction. Ex. 10000000                                                                                                                                                                                                                                                        |
| type                          | String  |             | Type of transfer - like "deposit" or "withdrawal". Ex. "0x3::token::DepositEvent"                                                                                                                                                                                                                          |

### `nft_metadata_crawler_parsed_asset_uris`

This table allows you to look up the cdn and uris for NFT images / content.

| Field                              | Type   | Primary Key | Description                                                                                                                                  |
| ---------------------------------- | ------ | ----------- | -------------------------------------------------------------------------------------------------------------------------------------------- |
| animation\_optimizer\_retry\_count | Int    |             | Number of retries to optimize animation. Ex. 3                                                                                               |
| asset\_uri                         | String | Yes         | URI of the asset. Ex. "[https://example.com/nft/123](https://example.com/nft/123)"                                                           |
| cdn\_animation\_uri                | String |             | Content Delivery Network URI for animation. Ex. "[https://cdn.example.com/animations/123](https://cdn.example.com/animations/123)"           |
| cdn\_image\_uri                    | String |             | Content Delivery Network URI for image. Ex. "[https://cdn.example.com/images/123](https://cdn.example.com/images/123)"                       |
| cdn\_json\_uri                     | String |             | Content Delivery Network URI for JSON metadata. Ex. "[https://cdn.example.com/metadata/123.json](https://cdn.example.com/metadata/123.json)" |
| raw\_animation\_uri                | String |             | Original URI for animation before CDN optimization. Ex. "[https://example.com/raw/animations/123](https://example.com/raw/animations/123)"   |
| raw\_image\_uri                    | String |             | Original URI for image before CDN optimization. Ex. "[https://example.com/raw/images/123](https://example.com/raw/images/123)"               |

| Index Name                | Indexed Columns     |
| ------------------------- | ------------------- |
| nft\_inserted\_at         | inserted\_at        |
| nft\_raw\_animation\_uri  | raw\_animation\_uri |
| nft\_raw\_image\_uri      | raw\_image\_uri     |
| parsed\_asset\_uris\_pkey | asset\_uri          |

### `current_token_ownerships_v2`

_Has an aggregate view for summary data called `current_token_ownerships_v2_aggregate`_

This table tracks who owns which NFTs. This includes both v1 and v2 tokens. Fungible tokens are not tracked as consistently.

| Field                          | Type    | Primary Key | Description                                                                                                                                                                                |
| ------------------------------ | ------- | ----------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| composed\_nfts\_aggregate      | Join    |             | Aggregate information about the composed NFTs, such as count or other statistics.                                                                                                          |
| current\_token\_data           | Join    |             | Detailed information about the token's current data; structure is defined in a related table.                                                                                              |
| amount                         | bigint  |             | The amount of the token owned. Example: 1 for an NFT.                                                                                                                                      |
| composed\_nfts                 | Array   |             | An array containing the IDs of NFTs that compose this token, if applicable.                                                                                                                |
| is\_fungible\_v2               | Boolean |             | Indicates whether the token is fungible. Example: true or null                                                                                                                             |
| is\_soulbound\_v2              | Boolean |             | Indicates whether the token is soulbound (non-transferable once owned). Example: true or null                                                                                              |
| last\_transaction\_timestamp   | String  |             | Timestamp of the last transaction involving the token. Example: "2024-04-17T02:14:25.68771"                                                                                                |
| last\_transaction\_version     | bigint  |             | The version number of the last transaction involving the token. Example: 20747031                                                                                                          |
| non\_transferrable\_by\_owner  | Boolean |             | Indicates whether the token is non-transferrable by the owner. Example: true or null                                                                                                       |
| owner\_address                 | String  | Yes         | The Aptos account address that currently owns the token. Addresses must be 66 characters so may be 0 padded. Example: "0xa815a9a09105973084bfc31530e7c8f002846787c2f0521e1e34dc144ad83b89" |
| property\_version\_v1          | bigint  | Yes         | The version number of the token's properties as of the last update. This field is only for token standard v1. It is always 0 for v2. Example: 0                                            |
| storage\_id                    | String  | Yes         | A unique identifier used for storage purposes. IDs must be 66 characters long, so may be 0 padded. Ex. "0xd8d41ff9f67d17d7dee061b5b683b92013b420cb6a30c21fc7c287454792d7a8"                |
| table\_type\_v1                | String  |             | The Move function type. Example: "0x3::token::TokenStore"                                                                                                                                  |
| token\_data\_id                | String  | Yes         | A unique identifier for the token data, typically a hash or a numeric ID. Ex. "0x3d911af2dc3e47848fbba17b8694cf526942be183b84f8393a6c048232fb976d"                                         |
| token\_properties\_mutated\_v1 | Object  |             | Properties of the token that have been mutated from the original. Often in JSON or similar format. Example: { }                                                                            |
| token\_standard                | String  |             | The standard used to generate this token. Ex. "v1" or "v2"                                                                                                                                 |

| Index Name                           | Indexed Columns                                                     |
| ------------------------------------ | ------------------------------------------------------------------- |
| curr\_to2\_insat\_index              | inserted\_at                                                        |
| curr\_to2\_owner\_index              | owner\_address                                                      |
| curr\_to2\_wa\_index                 | storage\_id                                                         |
| current\_token\_ownerships\_v2\_pkey | token\_data\_id, property\_version\_v1, owner\_address, storage\_id |

### `current_token_datas_v2`

This table tracks the metadata associated with each NFT (Ex. URI, supply, etc.). This tracks both v1 and v2 tokens.

| Field                                 | Type    | Primary Key | Description                                                                                                                                       |
| ------------------------------------- | ------- | ----------- | ------------------------------------------------------------------------------------------------------------------------------------------------- |
| cdn\_asset\_uris                      | Join    |             | Use the [Hasura explorer](https://cloud.hasura.io/public/graphiql?endpoint=https://api.mainnet.aptoslabs.com/v1/graphql) to see these sub-fields. |
| current\_collection                   | Join    |             | Use the [Hasura explorer](https://cloud.hasura.io/public/graphiql?endpoint=https://api.mainnet.aptoslabs.com/v1/graphql) to see these sub-fields. |
| current\_token\_ownerships            | Join    |             | Use the [Hasura explorer](https://cloud.hasura.io/public/graphiql?endpoint=https://api.mainnet.aptoslabs.com/v1/graphql) to see these sub-fields. |
| current\_token\_ownerships\_aggregate | Join    |             | Use the [Hasura explorer](https://cloud.hasura.io/public/graphiql?endpoint=https://api.mainnet.aptoslabs.com/v1/graphql) to see these sub-fields. |
| aptos\_name                           | String  |             | This is a name tied to this token using the Aptos Name Service (ANS). Ex. "EpicDragon"                                                            |
| collection\_id                        | String  | Yes         | Identifier for the collection that includes this token. Ex. "0x360f6eeabb4d7a9d2fab1f35b01e02831e3b5c4b73c7fd6c98dcc1c301c817c8"                  |
| decimals                              | bigint  |             | Number of decimal places for token value, typically for fungible tokens. Ex. 18                                                                   |
| description                           | String  |             | Description of the token. Ex. "A legendary dragon from the mystical lands."                                                                       |
| is\_fungible\_v2                      | Boolean |             | Whether the token is fungible. Ex. False for NFTs                                                                                                 |
| largest\_property\_version\_v1        | bigint  |             | The largest version number of the token's properties under the first schema. Ex. 1                                                                |
| last\_transaction\_timestamp          | bigint  |             | Unix timestamp of the last transaction involving this token. Ex. 2024-03-27T07:41:58.800893                                                       |
| last\_transaction\_version            | bigint  |             | Blockchain version of the last transaction involving this token. Ex. 30000000                                                                     |
| maximum                               | bigint  |             | Maximum possible quantity of this token, relevant for fungibles. Ex. 1000000                                                                      |
| supply                                | bigint  |             | Current supply of the token in circulation. Ex. 500000                                                                                            |
| token\_data\_id                       | String  |             | Unique identifier for the token's data. Ex. "0xa815a9a09105973084bfc31530e7c8f002846787c2f0521e1e34dc144ad83b89"                                  |
| token\_name                           | String  |             | The formal name of the token. Ex. "Mystic Dragon"                                                                                                 |
| token\_properties                     | Object  |             | Use the [Hasura explorer](https://cloud.hasura.io/public/graphiql?endpoint=https://api.mainnet.aptoslabs.com/v1/graphql) to see these sub-fields. |
| token\_standard                       | String  |             | Aptos standard that the collection adheres to. Ex. "v1"                                                                                           |
| token\_uri                            | String  |             | URI linking to further information about the token. Ex. "[https://example.com/tokens/987654321](https://example.com/tokens/987654321)"            |

| Index Name                      | Indexed Columns             |
| ------------------------------- | --------------------------- |
| cur\_td2\_cid\_name\_index      | collection\_id, token\_name |
| cur\_td2\_insat\_index          | inserted\_at                |
| current\_token\_datas\_v2\_pkey | token\_data\_id             |

### `current_collections_v2`

This table tracks the metadata associated with each NFT collection (Ex. collection\_id, creator\_address, etc.). This tracks both v1 and v2 tokens.

| Field                        | Type    | Primary Key | Description                                                                                                                                                                                          |
| ---------------------------- | ------- | ----------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| cdn\_asset\_uris             | Join    |             | Use the [Hasura explorer](https://cloud.hasura.io/public/graphiql?endpoint=https://api.mainnet.aptoslabs.com/v1/graphql) to see these sub-fields.                                                    |
| collection\_id               | String  | Yes         | Unique identifier for the collection. IDs must be 66 characters long, and so may be 0 padded. Ex. "0xa815a9a09105973084bfc31530e7c8f002846787c2f0521e1e34dc144ad83b88"                               |
| collection\_name             | String  |             | The formal name of the collection. Ex. "Mythic Dragons"                                                                                                                                              |
| creator\_address             | String  |             | This is an Aptos account address that created the collection. Addresses must be 66 characters long, and so may be 0 padded. Ex. "0x50bc83f01d48ab3b9c00048542332201ab9cbbea61bda5f48bf81dc506caa78a" |
| current\_supply              | bigint  |             | Current supply of tokens in this collection. Ex. 500                                                                                                                                                 |
| description                  | String  |             | Description of the collection. Ex. "A collection of rare digital dragons."                                                                                                                           |
| last\_transaction\_timestamp | String  |             | Timestamp of the last transaction involving this collection. Ex. "2024-04-17T02:14:25.68771"                                                                                                         |
| last\_transaction\_version   | bigint  |             | Blockchain version of the last transaction involving this collection. Ex. 3000000002                                                                                                                 |
| max\_supply                  | bigint  |             | Maximum possible quantity of tokens in this collection. If the max supply is 0, there is no limit on the supply. Ex. 1000                                                                            |
| mutable\_description         | String  |             | Changeable description of the collection. Ex. "Updated collection description."                                                                                                                      |
| mutable\_uri                 | Boolean |             | True if the uri is changeable by the creator. Ex. True                                                                                                                                               |
| table\_handle\_v1            | String  |             | Legacy identifier handle for the collection in earlier schema versions. Ex. "handle\_12345"                                                                                                          |
| token\_standard              | String  |             | Aptos standard that the collection adheres to. Ex. "v1"                                                                                                                                              |
| total\_minted\_v2            | bigint  |             | Total number of tokens minted in this collection under schema version 2. Ex. 800                                                                                                                     |
| uri                          | String  |             | This is a URI to  where the image live. This can also be JSON data. Ex. "[https://example.com/collections/9876543210](https://example.com/collections/9876543210)"                                   |

| Index Name                     | Indexed Columns                    |
| ------------------------------ | ---------------------------------- |
| cur\_col2\_crea\_cn\_index     | creator\_address, collection\_name |
| cur\_col2\_insat\_index        | inserted\_at                       |
| current\_collections\_v2\_pkey | collection\_id                     |

### `current_collection_ownership_v2_view`

_Has an aggregate view for summary data called `current_collection_ownership_v2_view_aggregate`_

This table maps collections to who owns them and helps count how much of a collection is owned by other accounts.

| Field                      | Type   | Primary Key | Description                                                                                                                                                                                            |
| -------------------------- | ------ | ----------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| current\_collection        | Join   |             | Use the [Hasura explorer](https://cloud.hasura.io/public/graphiql?endpoint=https://api.mainnet.aptoslabs.com/v1/graphql) to see these sub-fields.                                                      |
| collection\_id             | String | Yes         | Unique identifier for the collection. IDs must be 66 characters long, and so may be 0 padded. Ex. "0xa815a9a09105973084bfc31530e7c8f002846787c2f0521e1e34dc144ad83b89"                                 |
| collection\_name           | String |             | The formal name of the collection. Ex. "Mythic Dragons"                                                                                                                                                |
| collection\_uri            | String |             | URI linking to further information about the collection. Ex. "[https://example.com/collections/9876543210](https://example.com/collections/9876543210)"                                                |
| creator\_address           | String |             | This is an Aptos account address that created the collection. Addresses must be 66 characters long, and so may be 0 padded. Ex. "0x50bc83f01d48ab3b9c00048542332201ab9cbbea61bda5f48bf81dc506caa78a"   |
| distinct\_tokens           | bigint |             | The count of distinct tokens owned within this collection. Ex. 150                                                                                                                                     |
| last\_transaction\_version | bigint |             | The version number of the last transaction involving this collection. Ex. 3000000002                                                                                                                   |
| owner\_address             | String | Yes         | This is an Aptos account address that currently owns the token. Addresses must be 66 characters long, and so may be 0 padded. Ex. "0x50bc83f01d48ab3b9c00048542332201ab9cbbea61bda5f48bf81dc506caa78a" |
| single\_token\_uri         | String |             | URI linking to information about a specific token within the collection. Ex. "[https://example.com/tokens/9876543210](https://example.com/tokens/9876543210)"                                          |
| token\_standard            | String |             | Aptos standard that the collection adheres to. Ex. "v1"                                                                                                                                                |

## Fungible Assets

### `fungible_asset_metadata`

This tracks the metadata tied to each fungible asset (ex. decimals of precision). It includes v1 token data. This is a current\_ table.

| Field                                 | Type   | Primary Key | Description                                                                                                                                                                                     |
| ------------------------------------- | ------ | ----------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| asset\_type                           | String | Yes         | The type of the asset, described by a Move resource. Ex. "0x1::aptos\_coin::AptosCoin"                                                                                                          |
| creator\_address                      | String |             | This is an Aptos account address that created the asset. Addresses must be 66 characters long, and so may be 0 padded. Ex. "0x50bc83f01d48ab3b9c00048542332201ab9cbbea61bda5f48bf81dc506caa78a" |
| decimals                              | bigint |             | Number of decimal places for token value, typically for fungible tokens. Ex. 18                                                                                                                 |
| icon\_uri                             | String |             | URI for the icon of the asset. Ex. "[https://cdn.example.com/icons/123](https://cdn.example.com/icons/123)"                                                                                     |
| last\_transaction\_timestamp          | String |             | Timestamp of the last transaction involving this asset. Ex. "2024-04-17T02:14:25.68771"                                                                                                         |
| last\_transaction\_version            | bigint |             | Blockchain version of the last transaction involving this asset. Ex. 10000000                                                                                                                   |
| name                                  | String |             | The formal name of the asset. Ex. "Digital Gold"                                                                                                                                                |
| project\_uri                          | String |             | URI linking to the project information associated with this asset. Ex. "[https://www.example.com/project\\\_name/](https://www.example.com/project\\_name/)"                                    |
| supply\_aggregator\_table\_handle\_v1 | String |             | Legacy handle for the supply aggregator table from an earlier schema version. Ex. "handle\_67890"                                                                                               |
| supply\_aggregator\_table\_key\_v1    | String |             | Legacy key for accessing the supply aggregator table in earlier schema versions. Ex. "key\_12345"                                                                                               |
| symbol                                | String |             | The trading symbol of the asset. Ex. "DGOLD"                                                                                                                                                    |
| token\_standard                       | String |             | Standard that the asset adheres to. Ex. "v1"                                                                                                                                                    |

| Index Name                      | Indexed Columns  |
| ------------------------------- | ---------------- |
| fam\_creator\_index             | creator\_address |
| fam\_insat\_index               | inserted\_at     |
| fungible\_asset\_metadata\_pkey | asset\_type      |

### `fungible_asset_activities`

This tracks the activity of fungible assets. It includes v1 token data.

| Field                          | Type    | Primary Key | Description                                                                                                                                                                                                        |
| ------------------------------ | ------- | ----------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| owner\_aptos\_names            | Join    |             | References [owner\_aptos\_names](#current_aptos_names).                                                                                                                                                            |
| owner\_aptos\_names\_aggregate | Join    |             | References [owner\_aptos\_names](#current_aptos_names).                                                                                                                                                            |
| amount                         | bigint  |             | The amount of the asset involved in the activity. Ex. 1000                                                                                                                                                         |
| asset\_type                    | String  | Yes         | The type of the asset, described by a Move resource. For fungible assets, this will be the address of the metadata object. Ex. "0x1::aptos\_coin::AptosCoin"                                                       |
| block\_height                  | bigint  |             | The blockchain id at which this activity occurred. Ex. 1500000                                                                                                                                                     |
| entry\_function\_id\_str       | String  |             | The identifier of the function called in this transaction. Ex. "0x1::aptos\_account::transfer"                                                                                                                     |
| event\_index                   | bigint  |             | Index of the event within the transaction. Ex. 1                                                                                                                                                                   |
| gas\_fee\_payer\_address       | String  |             | This is an Aptos account address that paid the gas fee for the transaction. Addresses must be 66 characters long, and so may be 0 padded. Ex. "0x50bc83f01d48ab3b9c00048542332201ab9cbbea61bda5f48bf81dc506caa78a" |
| is\_frozen                     | Boolean |             | True if this activity is a freeze asset activity. Ex. null                                                                                                                                                         |
| is\_gas\_fee                   | Boolean |             | Indicates whether this activity involved a gas fee. Ex. True                                                                                                                                                       |
| is\_transaction\_success       | Boolean |             | Indicates whether the transaction was successful. Ex. True                                                                                                                                                         |
| metadata                       | Object  |             | Use the [Hasura explorer](https://cloud.hasura.io/public/graphiql?endpoint=https://api.mainnet.aptoslabs.com/v1/graphql) to see fields for `metadata` in this table.                                               |
| owner\_address                 | String  |             | This is an Aptos account address that owns the asset. Addresses must be 66 characters long, and so may be 0 padded. Ex. "0xa815a9a09105973084bfc31530e7c8f002846787c2f0521e1e34dc144ad83b89"                       |
| storage\_id                    | String  |             | Identifier for the storage used in the transaction. IDs must be 66 characters long, and so may be 0 padded. Ex. "0xa815a9a09105973084bfc31530e7c8f002846787c2f0521e1e34dc144ad83b89"                               |
| storage\_refund\_amount        | bigint  |             | Amount refunded for storage after the transaction. This is always in APT [octas](/network/glossary#Octa). Ex. 50                                                                                                   |
| token\_standard                | String  |             | Aptos standard that the collection adheres to. Ex. "v1"                                                                                                                                                            |
| transaction\_timestamp         | String  |             | Timestamp when the transaction occurred. Ex. "2024-04-17T02:14:25.68771"                                                                                                                                           |
| transaction\_version           | bigint  |             | Blockchain version of the transaction. Ex. 2                                                                                                                                                                       |
| type                           | String  |             | Type of the transaction, described by a Move entry function. Ex. "0x1::coin::WithdrawEvent"                                                                                                                        |

| Index Name                        | Indexed Columns                    |
| --------------------------------- | ---------------------------------- |
| faa\_at\_index                    | asset\_type                        |
| faa\_gfpa\_index                  | gas\_fee\_payer\_address           |
| faa\_insat\_idx                   | inserted\_at                       |
| faa\_owner\_type\_index           | owner\_address, type               |
| faa\_si\_index                    | storage\_id                        |
| fungible\_asset\_activities\_pkey | transaction\_version, event\_index |

### `current_fungible_asset_balances`

_Has an aggregate view for summary data called `current_fungible_asset_balances_aggregate`_

This tracks the asset balances of each account on-chain. It includes v1 token data.

| Field                        | Type    | Primary Key | Description                                                                                                                                                                                                                |
| ---------------------------- | ------- | ----------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| amount                       | bigint  |             | The amount of the asset owned. Ex. 2000                                                                                                                                                                                    |
| asset\_type                  | String  |             | The type of the asset, described by a Move resource. For v2 tokens this is the address of the fungible asset metadata object. For v1 it's the fully qualified path of the move resource. Ex. "0x1::aptos\_coin::AptosCoin" |
| is\_frozen                   | Boolean |             | Indicates whether the account is frozen. Ex. False                                                                                                                                                                         |
| is\_primary                  | Boolean |             | Indicates whether this is the primary balance of the owner. Ex. True                                                                                                                                                       |
| last\_transaction\_timestamp | String  |             | Timestamp of the last transaction involving this balance. Ex. "2024-04-17T02:14:25.68771"                                                                                                                                  |
| last\_transaction\_version   | bigint  |             | Blockchain version of the last transaction involving this balance. Ex. 30000000                                                                                                                                            |
| metadata                     | Object  |             | Use the [Hasura explorer](https://cloud.hasura.io/public/graphiql?endpoint=https://api.mainnet.aptoslabs.com/v1/graphql) to see fields for `metadata` in `current_fungible_asset_balances`.                                |
| owner\_address               | String  |             | This is an Aptos account address that owns the asset. Addresses must be 66 characters long, and so may be 0 padded. Ex. "0xa815a9a09105973084bfc31530e7c8f002846787c2f0521e1e34dc144ad83b89"                               |
| storage\_id                  | String  | Yes         | Identifier for the storage associated with this balance. IDs must be 66 characters long, and so may be 0 padded. Ex. "0xa815a9a09105973084bfc31530e7c8f002846787c2f0521e1e34dc144ad83b89"                                  |
| token\_standard              | String  |             | Aptos standard that the collection adheres to. Ex. "v1"                                                                                                                                                                    |

| Index Name                                        | Indexed Columns             |
| ------------------------------------------------- | --------------------------- |
| cufab\_insat\_index                               | inserted\_at                |
| cufab\_owner\_at\_index                           | owner\_address, asset\_type |
| current\_unified\_fungible\_asset\_balances\_pkey | storage\_id                 |

## Delegated Staking

With [AIP-6](https://github.com/aptos-foundation/AIPs/blob/main/aips/aip-6.md) we added the ability for staking pools to be formed with delegated funds (delegation pools). Once these pools hold over 1M APT, they can become a staking pool (validator node).

### `current_delegated_staking_pool_balances`

This table tracks the current balances of each account in a delegated staking pool.

| Field                            | Type   | Primary Key | Description                                                                            |
| -------------------------------- | ------ | ----------- | -------------------------------------------------------------------------------------- |
| staking\_pool\_address           | String | Yes         | The address of the delegation pool.                                                    |
| total\_coins                     | bigint |             | Amount of APT in the staking pool.                                                     |
| total\_shares                    | bigint |             | The total number of shares in the delegation pool.                                     |
| operator\_commission\_percentage | bigint |             | The commission percentage taken by the staking pool operator.                          |
| inactive\_table\_handle          | String |             | The table handle for the inactive table.                                               |
| active\_table\_handle            | String |             | The table handle for the active table.                                                 |
| last\_transaction\_version       | int8   |             | Transaction version (identifier) for the last transaction involving this staking pool. |
| inserted\_at                     | String |             | The timestamp when the record was inserted.                                            |

| Index Name                                        | Indexed Columns        |
| ------------------------------------------------- | ---------------------- |
| current\_delegated\_staking\_pool\_balances\_pkey | staking\_pool\_address |

### `current_delegated_voter`

This table tracks the current delegated voters of a delegation pool.

| Field                        | Type      | Primary Key | Description                                                                             |
| ---------------------------- | --------- | ----------- | --------------------------------------------------------------------------------------- |
| delegation\_pool\_address    | String    | Yes         | The address of the delegation pool.                                                     |
| delegator\_address           | String    | Yes         | The address of the delegator.                                                           |
| table\_handle                | String    |             | The table handle tracking this position.                                                |
| voter                        | String    |             | The address of the current voter in the delegation pool.                                |
| pending\_voter               | String    |             | The address of the pending voter awaiting confirmation.                                 |
| last\_transaction\_version   | bigint    |             | The transaction version (identifier) of the last transaction involving this delegation. |
| last\_transaction\_timestamp | Timestamp |             | The block timestamp of the last transaction involving this delegation.                  |
| inserted\_at                 | Timestamp |             | The timestamp when the record was inserted into the database.                           |

| Index Name                      | Indexed Columns                               |
| ------------------------------- | --------------------------------------------- |
| current\_delegated\_voter\_pkey | delegation\_pool\_address, delegator\_address |
| cdv\_da\_index                  | delegator\_address                            |

### `current_delegator_balances`

This table tracks the current balances of each account in a delegated staking pool.

| Field                      | Type      | Primary Key | Description                                                                            |
| -------------------------- | --------- | ----------- | -------------------------------------------------------------------------------------- |
| delegator\_address         | String    | Yes         | The address of the delegator.                                                          |
| pool\_address              | String    |             | The address of the delegator pool.                                                     |
| pool\_type                 | String    |             | If the shares are active or inactive                                                   |
| table\_handle              | String    |             | The table handle for the pool.                                                         |
| shares                     | bigint    |             | The number of shares in the pool.                                                      |
| parent\_table\_handle      | String    |             | The table handle for the parent table.                                                 |
| last\_transaction\_version | bigint    |             | Transaction version (identifier) for the last transaction involving this staking pool. |
| inserted\_at               | Timestamp |             | The timestamp when the record was inserted.                                            |

| Index Name                         | Indexed Columns                                              |
| ---------------------------------- | ------------------------------------------------------------ |
| current\_delegator\_balances\_pkey | delegator\_address, pool\_address, pool\_type, table\_handle |

### `current_staking_pool_voter`

This table tracks the current voters of a staking pool.

| Field                      | Type      | Primary Key | Description                                                                            |
| -------------------------- | --------- | ----------- | -------------------------------------------------------------------------------------- |
| staking\_pool\_address     | String    | Yes         | The address of the staking pool.                                                       |
| voter\_address             | String    |             | The address of the voter.                                                              |
| operator\_address          | String    |             | The address of the operator.                                                           |
| last\_transaction\_version | bigint    |             | Transaction version (identifier) for the last transaction involving this staking pool. |
| inserted\_at               | Timestamp |             | The timestamp when the record was inserted.                                            |

| Index Name                          | Indexed Columns        |
| ----------------------------------- | ---------------------- |
| current\_staking\_pool\_voter\_pkey | staking\_pool\_address |
| ctpv\_va\_index                     | voter\_address         |
| ctpv\_insat\_index                  | inserted\_at           |

### `delegated_staking_activities`

This table tracks delegated staking events.

| Field                | Type      | Primary Key | Description                                                            |
| -------------------- | --------- | ----------- | ---------------------------------------------------------------------- |
| transaction\_version | bigint    |             | Transaction version (identifier) for activity                          |
| event\_index         | bigint    |             | The index of the event. Ex. 1                                          |
| delegator\_address   | String    |             | The address of the delegator.                                          |
| pool\_address        | String    |             | The address of the pool.                                               |
| event\_type          | String    |             | DistributeRewards, AddStake, UnlikeStake, ReactiveStake, WithdrawStake |
| amount               | bigint    |             | The amount being staked. Ex. 1000                                      |
| inserted\_at         | Timestamp |             | The timestamp when the record was inserted.                            |

| Index Name                           | Indexed Columns                                                       |
| ------------------------------------ | --------------------------------------------------------------------- |
| delegated\_staking\_activities\_pkey | transaction\_version, event\_index                                    |
| dsa\_pa\_da\_index                   | pool\_address, delegator\_address, transaction\_version, event\_index |
| dsa\_insat\_index                    | inserted\_at                                                          |

### `delegated_staking_pool_balances`

This table tracks the historical balances of each account in a delegated staking pool.

| Field                            | Type   | Primary Key | Description                                                   |
| -------------------------------- | ------ | ----------- | ------------------------------------------------------------- |
| transaction\_version             | bigint |             | Transaction version (identifier) for activity                 |
| staking\_pool\_address           | String |             | The address of the delegation pool.                           |
| total\_coins                     | bigint |             | Amount of APT in the staking pool.                            |
| total\_shares                    | bigint |             | The total number of shares in the delegation pool.            |
| operator\_commission\_percentage | bigint |             | The commission percentage taken by the staking pool operator. |
| inactive\_table\_handle          | String |             | The table handle for the inactive table.                      |
| active\_table\_handle            | String |             | The table handle for the active table.                        |
| inserted\_at                     | String |             | The timestamp when the record was inserted.                   |

| Index Name                               | Indexed Columns                              |
| ---------------------------------------- | -------------------------------------------- |
| delegated\_staking\_pool\_balances\_pkey | transaction\_version, staking\_pool\_address |

### `delegated_staking_pools`

This table tracks when a delegated pool was created.

| Field                       | Type      | Primary Key | Description                                                                  |
| --------------------------- | --------- | ----------- | ---------------------------------------------------------------------------- |
| staking\_pool\_address      | String    |             | The address of the staking pool.                                             |
| first\_transaction\_version | bigint    |             | The version number of the first transaction involving this pool. Ex. 5000000 |
| inserted\_at                | Timestamp |             | The timestamp when the record was inserted.                                  |

| Index Name                      | Indexed Columns        |
| ------------------------------- | ---------------------- |
| delegated\_staking\_pools\_pkey | staking\_pool\_address |

### `delegator_balances`

This table tracks the historical balances of each account in a delegation pool.

| Field                     | Type      | Primary Key | Description                                        |
| ------------------------- | --------- | ----------- | -------------------------------------------------- |
| transaction\_version      | bigint    |             | The version number of the transaction. Ex. 5000000 |
| write\_set\_change\_index | bigint    |             | The index of the write set change. Ex. 1           |
| delegator\_address        | String    |             | The address of the delegator.                      |
| pool\_address             | String    |             | The address of the delegator pool.                 |
| pool\_type                | String    |             | The type of the pool. Ex. "delegated"              |
| table\_handle             | String    |             | The table handle for the pool.                     |
| shares                    | bigint    |             | The number of shares in the pool.                  |
| parent\_table\_handle     | String    |             | The table handle for the parent table.             |
| inserted\_at              | Timestamp |             | The timestamp when the record was inserted.        |

| Index Name                | Indexed Columns                                 |
| ------------------------- | ----------------------------------------------- |
| delegator\_balances\_pkey | transaction\_version, write\_set\_change\_index |

## Aptos Naming Service (ANS)

### `current_aptos_names`

_Has an aggregate view for summary data called `current_aptos_names_aggregate`_

This view of [`current_ans_lookup_v2`](#current_ans_lookup_v2) helps query by name instead of account.

| Field                      | Type    | Primary Key | Description                                                                                                                                                                                       |
| -------------------------- | ------- | ----------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| domain                     | String  |             | The domain associated with this Aptos name. Ex. "example.crypto"                                                                                                                                  |
| domain\_with\_suffix       | String  |             | The full domain name including any suffix. Ex. "example.crypto.aptos"                                                                                                                             |
| expiration\_timestamp      | String  |             | Timestamp when the domain registration expires. Ex. "2024-04-17T02:14:25.68771"                                                                                                                   |
| is\_active                 | Boolean |             | Indicates whether the domain is currently active. Ex. True                                                                                                                                        |
| is\_domain\_owner          | Boolean |             | Indicates whether the registered address is the owner of the domain. Ex. False                                                                                                                    |
| is\_primary                | Boolean |             | Indicates whether this is the primary domain for the registered address. Ex. True                                                                                                                 |
| last\_transaction\_version | bigint  |             | The version number of the last transaction involving this domain. Ex. 5000000                                                                                                                     |
| owner\_address             | String  |             | This is an Aptos account address that owns the domain. Addresses must be 66 characters long, and so may be 0 padded. Ex. "0x123abc456def7890abcdef1234567890abcdef1234"                           |
| registered\_address        | String  |             | This is an Aptos account address registered to the domain. Addresses must be 66 characters long, and so may be 0 padded. Ex. "0x50bc83f01d48ab3b9c00048542332201ab9cbbea61bda5f48bf81dc506caa78a" |
| subdomain                  | String  |             | Any subdomain part of the domain name. Ex. "sub.example"                                                                                                                                          |
| token\_name                | String  |             | The name of the token associated with this domain. Ex. "ExampleToken"                                                                                                                             |
| token\_standard            | String  |             | Aptos standard that the collection adheres to. Ex. "v1"                                                                                                                                           |

### `current_ans_lookup_v2`

This table maps tokens, standards, and addresses to human readable names.

| Field                      | Type    | Primary Key | Description                                                                                                                                                                                       |
| -------------------------- | ------- | ----------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| domain                     | String  | Yes         | The domain associated with this Aptos name. Ex. "example.crypto"                                                                                                                                  |
| expiration\_timestamp      | String  |             | Timestamp when the domain registration expires. Ex. "2024-04-17T02:14:25.68771"                                                                                                                   |
| is\_deleted                | Boolean |             | Indicates whether the domain registration has been deleted. Ex. False                                                                                                                             |
| last\_transaction\_version | bigint  |             | The version number of the last transaction involving this domain. Ex. 5000000                                                                                                                     |
| registered\_address        | String  |             | This is an Aptos account address registered to the domain. Addresses must be 66 characters long, and so may be 0 padded. Ex. "0x50bc83f01d48ab3b9c00048542332201ab9cbbea61bda5f48bf81dc506caa78a" |
| subdomain                  | String  | Yes         | Any subdomain part of the domain name. Ex. "sub.example"                                                                                                                                          |
| token\_name                | String  |             | The name of the token associated with this domain. Ex. "ExampleToken"                                                                                                                             |
| token\_standard            | String  | Yes         | Aptos standard that the collection adheres to. Ex. "v1"                                                                                                                                           |

| Index Name                     | Indexed Columns                    |
| ------------------------------ | ---------------------------------- |
| ans\_v2\_et\_index             | expiration\_timestamp              |
| ans\_v2\_insat\_index          | inserted\_at                       |
| ans\_v2\_ra\_index             | registered\_address                |
| ans\_v2\_tn\_index             | token\_name, token\_standard       |
| current\_ans\_lookup\_v2\_pkey | domain, subdomain, token\_standard |

## Deprecated Tables

The following tables are planned for deprecation, or are already deprecated. See the notes section for any direct replacements or notes on how to migrate if you currently depend on one of these tables. Please do not use any of the below tables for production services.

| Table                                   | Notes                                                                                                                                                           |
| --------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| address\_version\_from\_move\_resources | Replace with account\_transactions                                                                                                                              |
| address\_events\_summary                | To query custom events, you should create a [No-Code Indexer](https://geomi.dev/docs/no-code-indexing)                                                          |
| address\_version\_from\_events          | To query custom events, you should create a [No-Code Indexer](https://geomi.dev/docs/no-code-indexing)                                                          |
| coin\_activities                        | Replace with fungible\_asset\_activities                                                                                                                        |
| coin\_balances                          | Replace with current\_fungible\_asset\_balances                                                                                                                 |
| coin\_infos                             | Replace with fungible\_asset\_metadata                                                                                                                          |
| coin\_supply                            | No replacement; non-realtime APT coin supply is available with this [query](https://github.com/aptos-labs/explorer/blob/main/analytics/apt_supply.sql)          |
| collection\_datas                       | Replace with current\_collections\_v2                                                                                                                           |
| current\_ans\_lookup                    | Replace with current\_ans\_lookup\_v2                                                                                                                           |
| current\_coin\_balances                 | Replace with current\_fungible\_asset\_balances                                                                                                                 |
| current\_collection\_datas              | Replace with current\_collections\_v2                                                                                                                           |
| current\_token\_datas                   | Replace with current\_token\_datas\_v2                                                                                                                          |
| current\_token\_ownerships              | Replace with current\_token\_ownerships\_v2                                                                                                                     |
| events\_view                            | To query custom events, you should create a [No-Code Indexer](https://geomi.dev/docs/no-code-indexing)                                                          |
| move\_resources                         | Replace with account\_transactions                                                                                                                              |
| move\_resources\_view                   | Replace with account\_transactions                                                                                                                              |
| nft\_marketplace\_v2\_\*                | Replace with [NFT Aggregator API](/build/indexer/nft-aggregator)                                                                                                |
| token\_activities                       | Replace with token\_activities\_v2                                                                                                                              |
| token\_datas                            | Replace with current\_token\_datas\_v2                                                                                                                          |
| token\_ownerships                       | Replace with current\_token\_ownerships\_v2                                                                                                                     |
| tokens                                  | Replace with current\_token\_datas\_v2                                                                                                                          |
| transactions                            | No replacement; non-realtime data is available in [BigQuery](https://console.cloud.google.com/marketplace/product/bigquery-public-data/crypto-aptos-mainnet-us) |
| transactions\_view                      | No replacement; non-realtime data is available in [BigQuery](https://console.cloud.google.com/marketplace/product/bigquery-public-data/crypto-aptos-mainnet-us) |

# Self-Hosted Indexer API

> Deploy your own Aptos Indexer API with custom processors, database setup, and transaction stream integration for private data access

import { Aside } from '@astrojs/starlight/components';

{/* <IndexerBetaNotice /> */}

This guide will walk you through setting up a self-hosted Indexer API.

<Aside type="caution">
  Currently this guide only explains how to run processor part of the Indexer API. By the end of this guide you will have a running processor that consumes transactions from the Transaction Stream Service, parses them, and stores them in the database. Unfortunately this guide does not explain how to attach an API to this system right now.
</Aside>

## Prerequisites

- A running PostgreSQL instance is required, with a valid user and database. In this example we call the user `postgres` and the database `indexer`.
- If you wish to use Docker, you must have Docker installed. [Installation Guide](https://docs.docker.com/get-docker/).

## Configuration

To run the service we need to define a config file. We will start with this template:

```yaml filename="config.yaml"
health_check_port: 8084
server_config:
  processor_config:
    type: default_processor
  postgres_connection_string: postgresql://postgres:@localhost:5432/indexer
  indexer_grpc_data_service_address: 127.0.0.1:50051
  indexer_grpc_http2_ping_interval_in_secs: 60
  indexer_grpc_http2_ping_timeout_in_secs: 10
  auth_token: AUTH_TOKEN
```

From here you will likely want to change the values of some of these fields. Let's go through some of them.

### `processor_name`

<Aside type="note">
  A single instance of the service only runs a single processor. If you want to run multiple processors, you must run multiple instances of the service. In this case, it is up to you whether to use the same database or not.
</Aside>

This is the processor you want to run. You can see what processors are available [here](https://github.com/aptos-labs/aptos-indexer-processors-v2/tree/main/processor/src/processors). Some examples:

- `coin_processor`
- `ans_processor`
- `token_v2_processor`

### `postgres_connection_string`

This is the connection string to your PostgreSQL database. It should be in the format `postgresql://<username>:<password>@<host>:<port>/<database>`.

<Aside type="caution">
  If you're running this from a Docker Desktop environment (which you likely are if you're using MacOS or Windows) you must set `postgres_connection_string` to `postgresql://host.docker.internal:5432/indexer` instead. With Docker Desktop this is how the binary can reach the host network.
</Aside>

### `indexer_grpc_data_service_address`

This is the URL for the Transaction Stream Service. If you are using the Labs-Hosted instance you can find the URLs for each network at [this page](/build/indexer/indexer-api). Make sure to select the correct URL for the network you want to index. If you are running this service locally the value should be `127.0.0.1:50051`.

### `auth_token`

This is the auth token used to connect to the Transaction Stream Service. If you are using the Labs-Hosted instance you can use the API Gateway to get an API key. Learn more at [this page](/build/indexer/indexer-api).

## Run with source code

Clone the repo:

```shellscript filename="Terminal"
# SSH
git clone git@github.com:aptos-labs/aptos-indexer-processors-v2.git

# HTTPS
git clone https://github.com/aptos-labs/aptos-indexer-processors-v2.git
```

Navigate to the directory for the service:

```shellscript filename="Terminal"
cd aptos-indexer-processors
cd rust/processor
```

Run the service:

```shellscript filename="Terminal"
cargo run --release -- -c config.yaml
```

## Run with Docker

{/* <!--
  This doesn't actually work this very moment because:

  1. We don't yet publish the image as indexer-processor-rust
  2. We don't tag it as latest.

  We'll do that soon though: https://aptos-org.slack.com/archives/C04PRP1K1FZ/p1692732083583659
  --> */}

To run the service with Docker, use the following command:

```shellscript filename="Terminal"
docker run -it --network host --mount type=bind,source=/tmp/config.yaml,target=/config.yaml aptoslabs/indexer-processor-rust -c /config.yaml
```

This command binds the container to the host network and mounts the config file from the host into the container. This specific invocation assumes that your config file in the host is at `/tmp/config.yaml`.

See the image on DockerHub here: [https://hub.docker.com/r/aptoslabs/indexer-processor-rust/tags](https://hub.docker.com/r/aptoslabs/indexer-processor-rust/tags).

# Get Token Metadata by Name

> Retrieve token metadata URIs by token names within collections for marketplace and NFT platform integration

import { GraphQLEditor } from '~/components/react/GraphQLEditor';

import { Aside } from '@astrojs/starlight/components';

`GetTokensDataByName` retrieves metadata URIs for tokens by their names within a specified collection. This query is particularly useful for marketplaces to show metadata from tokens within a collection.

<Aside type="note">
  Explore the capabilities of this query! Modify the variables below to tailor the query to your needs.
</Aside>

<GraphQLEditor
  query={`query GetTokensDataByName($token_name: String, $collectionId: String) {
  current_token_datas_v2(
    where: {
      token_name: { _eq: $token_name }
      collection_id: { _eq: $collectionId }
    }
  ) {
    token_uri
    __typename
  }
}`}
  variables={`{
  "token_name": "The Mexican",
  "collectionId": "0xe6a7399d10406b993e25d8a3bf24842413ba8f1a08444dbfa5f1c31b09f0d16e"
}`}
/>

#### Variables:

- `$token_name`: **String** - The name of the token to search within the collection. Example: `"The Mexican"`.
- `$collectionId`: **String** - The collection id calculated based on collection name and creator address. Example: `"0xe6a7399d10406b993e25d8a3bf24842413ba8f1a08444dbfa5f1c31b09f0d16e"`.

### Note

To get the collection id, you can use the python code snippet to get:

```python
import hashlib

def standardized_address(creator_address: str) -> str:
    # Strip the '0x' prefix if it exists and format the address to be 64 characters long
    handle = creator_address.removeprefix("0x") if creator_address.startswith("0x") else creator_address
    return f"0x{handle:0>64}"

def sha256_hex(creator_address: str, collection_name: str) -> str:
    # Process the creator address
    processed_address = standardized_address(creator_address)
    
    # Combine processed creator address and collection name
    combined_string = f"{creator_address}::{collection_name}"
    # Compute SHA256 hash and return as a hexadecimal string
    return standardized_address(hashlib.sha256(combined_string.encode()).hexdigest())

# Example usage
creator_address = "0xc0e3fbf8ae61056d66ce624d71ccf1888f879355cc4e364ef117249b5e3160a8"
collection_name = "Aptomingos"
# Collection Id is `0xe6a7399d10406b993e25d8a3bf24842413ba8f1a08444dbfa5f1c31b09f0d16e`
print(sha256_hex(creator_address, collection_name))

```

# Indexer SDK

> Create custom data processors for Aptos blockchain using the Indexer SDK to index smart contracts and build tailored data pipelines

import { CardGrid, LinkCard } from '@astrojs/starlight/components';

{/* <IndexerBetaNotice /> */}

While the Indexer API is a powerful tool for querying basic on-chain data, it may not always provide the exact data you need.
In most cases, you want to index your own contract and to do that, you can create your own custom processor using the Indexer SDK.

## Using the Indexer SDK

Learn how to use the Indexer SDK through guides and documentation.

<CardGrid>
  <LinkCard href="/build/indexer/indexer-sdk/quickstart" title="Quickstart Guide" description="Get started with the Indexer SDK" />

  <LinkCard href="/build/indexer/indexer-sdk/documentation" title="Documentation" description="Read documentation about the Indexer SDK" />
</CardGrid>

## Example Processors

As a reference, you can see all Aptos-Hosted processors that comprise the Indexer API [here](https://github.com/aptos-labs/aptos-indexer-processors-v2).

# Migrate to Indexer SDK

> Step-by-step migration guide from legacy custom processors to modern Aptos Indexer SDK with code examples and best practices

This guide contains instructions on how to migrate your legacy custom processor (that's written in the [old way](https://github.com/aptos-labs/aptos-indexer-processors/blob/aptos-indexer-processors-v1.20.0/rust/processor/src/processors/events_processor.rs)) to Indexer SDK.

## 1. Clone the example repo

We use example events processor in `aptos-indexer-processor-example` as a starting point for the migration.

```shellscript
git clone https://github.com/aptos-labs/aptos-indexer-processor-example.git
```

## 2. Migrate your processor config

Previously, you would create a branch of `aptos-indexer-processors` and update the processor config to include your custom processor.
This legacy approach made it very difficult to upgrade your processor.
To address this, the SDK no longer depends on `aptos-indexer-processors`.
As a result, you'll need to define your own `IndexerProcessorConfig` and `ProcessorConfig` structs.

The `IndexerProcessorConfig` defines the base configuration for all processors that you'll be running.
The `ProcessorConfig` is an enum that contains all the individual processor configs.

Update the following files in your project:

- [`ProcessorConfig`](https://github.com/aptos-labs/aptos-indexer-processor-example/blob/main/aptos-indexer-processor-example/src/config/processor_config.rs): Replace `EventsProcessor` with your processor.
- [`IndexerProcessorConfig`](https://github.com/aptos-labs/aptos-indexer-processor-example/blob/main/aptos-indexer-processor-example/src/config/indexer_processor_config.rs): Update the `.run()` method to include your processor.

If you'd like to read more about configuration in the SDK, take a look at the [Create a Processor](/build/indexer/indexer-sdk/documentation/create-processor) guide.

## 3. Migrate processing logic to steps

In the old way, you defined your processor's logic by implementing `ProcessorTrait`'s `process_transactions` method.

Example events processor written with the old way:

```rust
#[async_trait]
impl ProcessorTrait for EventsProcessor {
    async fn process_transactions(
        ...
    ) -> anyhow::Result<ProcessingResult> {
        // Extract events from transactions 
        let events: Vec<EventModel> = process_events(transactions);

        // Store the events in the database
        let tx_result = insert_to_db(
            self.get_pool(),
            self.name(),
            start_version,
            end_version,
            &events,
            &self.per_table_chunk_sizes,
        )
        .await;

        return tx_result;
    }
}

async fn insert_to_db(
    conn: ArcDbPool,
    name: &'static str,
    start_version: u64,
    end_version: u64,
    events: &[EventModel],
    per_table_chunk_sizes: &AHashMap<String, usize>,
) -> Result<(), diesel::result::Error> {
    tracing::trace!(
        name = name,
        start_version = start_version,
        end_version = end_version,
        "Inserting to db",
    );
    execute_in_chunks(
        conn,
        insert_events_query,
        events,
        get_config_table_chunk_size::<EventModel>("events", per_table_chunk_sizes),
    )
    .await?;
    Ok(())
}
```

With the SDK, we've introduced the concept of steps, which represent independent units of processing logic.
In the `EventsProcessor` example, the extraction of events and storing them in the database can be broken down into two steps.

To migrate your processor to the SDK, you'll need to define these steps in your processor.
You can use the `EventsExtractor` and `EventsStorer` steps in the example as a starting point for defining your own steps.

Make the following changes to [`events_extractor.rs`](https://github.com/aptos-labs/aptos-indexer-processor-example/blob/main/aptos-indexer-processor-example/src/processors/events/events_extractor.rs).

```rust
// TODO: Update the step name
pub struct EventsExtractor
where
    Self: Sized + Send + 'static, {}

#[async_trait]
impl Processable for EventsExtractor {
    type Input = Vec<Transaction>;
    // TODO: Update the output type
    // This should be the data model you're extracting from the transactions
    type Output = Vec<EventModel>;
    type RunType = AsyncRunType;

    async fn process(
        &mut self,
        item: TransactionContext<Vec<Transaction>>,
    ) -> Result<Option<TransactionContext<Vec<EventModel>>>, ProcessorError> {
        // TODO: Update extraction logic. 
        // This should be the same as the extraction logic in the old `process_transactions` method
        let events = item
            .data
            .par_iter()
            .map(|txn| {
                process_events(txn)
            })
            .flatten()
            .collect::<Vec<EventModel>>();

        Ok(Some(TransactionContext {
            data: events,
            metadata: item.metadata,
        }))
    }
}
```

Make the following changes to [`events_storer.rs`](https://github.com/aptos-labs/aptos-indexer-processor-example/blob/main/aptos-indexer-processor-example/src/processors/events/events_processor.rs).

```rust
pub struct EventsStorer
where
    Self: Sized + Send + 'static,
{
    conn_pool: ArcDbPool,
    processor_config: DefaultProcessorConfig,
}

impl EventsStorer {
    pub fn new(conn_pool: ArcDbPool, processor_config: DefaultProcessorConfig) -> Self {
        Self {
            conn_pool,
            processor_config,
        }
    }
}

#[async_trait]
// TODO: Update step name
impl Processable for EventsStorer {
    // TODO: Update input type for the step. 
    // The input type should match the output type of the extractor step.
    type Input = Vec<EventModel>;
    type Output = ();
    type RunType = AsyncRunType;

    async fn process(
        &mut self,
        events: TransactionContext<Vec<EventModel>>,
    ) -> Result<Option<TransactionContext<()>>, ProcessorError> {
        let per_table_chunk_sizes: AHashMap<String, usize> = AHashMap::new();
        let execute_res = execute_in_chunks(
            self.conn_pool.clone(),
            // TODO: Update this to the insertion query of your old processor
            insert_events_query,
            &events.data,
            get_config_table_chunk_size::<EventModel>("events", &per_table_chunk_sizes),
        )
        .await;
        match execute_res {
            Ok(_) => {
                Ok(Some(TransactionContext {
                    data: (),
                    metadata: events.metadata,
                }))
            },
            Err(e) => Err(ProcessorError::DBStoreError {
                message: format!(
                    "Failed to store events versions {} to {}: {:?}",
                    events.metadata.start_version, events.metadata.end_version, e,
                ),
                query: None,
            }),
        }
    }
}

impl AsyncStep for EventsStorer {}

impl NamedStep for EventsStorer {
    fn name(&self) -> String {
        "EventsStorer".to_string()
    }
}
```

## 4. Migrate your processor

Now that we've migrated the processing logic to steps, we need to also migrate the processor to instantiate the steps and connect them together.
In [`events_processor.rs`](https://github.com/aptos-labs/aptos-indexer-processor-example/blob/main/aptos-indexer-processor-example/src/processors/events/events_processor.rs), make the following changes:

```rust
// TODO: Update processor name
pub struct EventsProcessor {
    pub config: IndexerProcessorConfig,
    pub db_pool: ArcDbPool,
    // If you have any other fields in your processor, add them here
    // You can instantiate them accordingly in the processor's `new` method
}
```

In the `run_processor` method, you'll need to update the code to use the steps you created in [Step 3](#3-migrate-processing-logic-to-steps).

```rust
pub async fn run_processor(self) -> Result<()> {
    {...}

    // Define processor steps
    let transaction_stream_config = self.config.transaction_stream_config.clone();
    let transaction_stream = TransactionStreamStep::new(TransactionStreamConfig {
        starting_version: Some(starting_version),
        ..transaction_stream_config
    })
    .await?;
    // TODO: Replace the next 2 lines with your steps 
    let events_extractor = EventsExtractor {};
    let events_storer = EventsStorer::new(self.db_pool.clone());
    
    let version_tracker = VersionTrackerStep::new(
        get_processor_status_saver(self.db_pool.clone(), self.config.clone()),
        DEFAULT_UPDATE_PROCESSOR_STATUS_SECS,
    );

    // Connect processor steps together
    let (_, buffer_receiver) = ProcessorBuilder::new_with_inputless_first_step(
        transaction_stream.into_runnable_step(),
    )
    // TODO: Replace the next 2 lines with your steps
    .connect_to(events_extractor.into_runnable_step(), 10)
    .connect_to(events_storer.into_runnable_step(), 10)
    .connect_to(version_tracker.into_runnable_step(), 10)
    .end_and_return_output_receiver(10);

    {...}
}
```

## 5. Update your `config.yaml`

`IndexerProcessorConfig` reworks the format of the `config.yaml` file.
Use the example [`config.yaml`](https://github.com/aptos-labs/aptos-indexer-processor-example/blob/main/aptos-indexer-processor-example/config.yaml).

```yaml
health_check_port: 8085
server_config:
  processor_config:
    # TODO: Update with processor type
    type: "events_processor" 
  transaction_stream_config:
    indexer_grpc_data_service_address: "https://grpc.testnet.aptoslabs.com:443"
    # TODO: Update auth token
    auth_token: "AUTH_TOKEN"
    # TODO: Update with processor name
    request_name_header: "events-processor"
  db_config:
    # TODO: Update with your database connection string
    postgres_connection_string: postgresql://postgres:@localhost:5432/example
  # backfill_config:
  #   backfill_alias: "events_processor_backfill_1"
```

## 6. Run your migrated processor

```shellscript
cd ~/{DIRECTORY_OF_PROJECT}/aptos-indexer-processor-example
cargo run --release -- -c config.yaml
```

In your terminal, you should start to see logs like this:

```shellscript
{"timestamp":"2025-01-13T21:23:21.785452Z","level":"INFO","message":"[Transaction Stream] Successfully connected to GRPC stream","stream_address":"https://grpc.mainnet.aptoslabs.com/","connection_id":"ec67ecc4-e041-4f17-a2e2-441e7ff21487","start_version":2186504987,"filename":"/Users/reneetso/.cargo/git/checkouts/aptos-indexer-processor-sdk-2f3940a333c8389d/e6867c5/aptos-indexer-processors-sdk/transaction-stream/src/transaction_stream.rs","line_number":349,"threadName":"tokio-runtime-worker","threadId":"ThreadId(4)"}
{"timestamp":"2025-01-13T21:23:21.785664Z","level":"INFO","message":"Spawning polling task","step_name":"TransactionStreamStep","filename":"/Users/reneetso/.cargo/git/checkouts/aptos-indexer-processor-sdk-2f3940a333c8389d/e6867c5/aptos-indexer-processors-sdk/sdk/src/traits/pollable_async_step.rs","line_number":112,"threadName":"tokio-runtime-worker","threadId":"ThreadId(23)"}
{"timestamp":"2025-01-13T21:23:21.785693Z","level":"INFO","message":"Spawning processing task","step_name":"TransactionStreamStep","filename":"/Users/reneetso/.cargo/git/checkouts/aptos-indexer-processor-sdk-2f3940a333c8389d/e6867c5/aptos-indexer-processors-sdk/sdk/src/traits/pollable_async_step.rs","line_number":204,"threadName":"tokio-runtime-worker","threadId":"ThreadId(23)"}
{"timestamp":"2025-01-13T21:23:21.785710Z","level":"INFO","message":"Spawning processing task","step_name":"FungibleAssetExtractor","filename":"/Users/reneetso/.cargo/git/checkouts/aptos-indexer-processor-sdk-2f3940a333c8389d/e6867c5/aptos-indexer-processors-sdk/sdk/src/traits/async_step.rs","line_number":87,"threadName":"tokio-runtime-worker","threadId":"ThreadId(4)"}
{"timestamp":"2025-01-13T21:23:21.785912Z","level":"INFO","message":"Spawning processing task","step_name":"FungibleAssetStorer","filename":"/Users/reneetso/.cargo/git/checkouts/aptos-indexer-processor-sdk-2f3940a333c8389d/e6867c5/aptos-indexer-processors-sdk/sdk/src/traits/async_step.rs","line_number":87,"threadName":"tokio-runtime-worker","threadId":"ThreadId(4)"}
{"timestamp":"2025-01-13T21:23:21.785978Z","level":"INFO","message":"Spawning polling task","step_name":"VersionTrackerStep: ()","filename":"/Users/reneetso/.cargo/git/checkouts/aptos-indexer-processor-sdk-2f3940a333c8389d/e6867c5/aptos-indexer-processors-sdk/sdk/src/traits/pollable_async_step.rs","line_number":112,"threadName":"tokio-runtime-worker","threadId":"ThreadId(14)"}
{"timestamp":"2025-01-13T21:23:21.786018Z","level":"INFO","message":"Spawning processing task","step_name":"VersionTrackerStep: ()","filename":"/Users/reneetso/.cargo/git/checkouts/aptos-indexer-processor-sdk-2f3940a333c8389d/e6867c5/aptos-indexer-processors-sdk/sdk/src/traits/pollable_async_step.rs","line_number":204,"threadName":"tokio-runtime-worker","threadId":"ThreadId(14)"}
```

## 7. Backfilling with the SDK

With the SDK, we've made some improvements to the backfilling process.
There are two options on backfilling:

1. You can keep following the old way of backfilling, which is to run a second instance of the processor and updating `starting_version` to the backfill version.
2. The SDK introduces an improvement where you can track progress of a backfill and start and stop the backfill as needed.
   If you'd like to use the new backfilling process, update your `config.yaml` like so:

```yaml
health_check_port: 8085
server_config:
  processor_config:
    # TODO: Update with processor type
    type: "events_processor" 
  transaction_stream_config:
    indexer_grpc_data_service_address: "https://grpc.testnet.aptoslabs.com:443"
    # TODO: Update with backfill version
    starting_version: {backfill version}
    # TODO: Update auth token
    auth_token: "AUTH_TOKEN"
    # TODO: Update with processor name
    request_name_header: "events-processor"
  db_config:
    # TODO: Update with your database connection string
    postgres_connection_string: postgresql://postgres:@localhost:5432/example
  backfill_config:
    # TODO: Update with your backfill alias. This should be unique for each backfill
    backfill_alias: "events_processor_backfill_1"
```

# Testing Processor

> Comprehensive testing strategies for Aptos processors including validation, transformation testing, and data accuracy verification

import { Aside } from '@astrojs/starlight/components';

### What Is a Processor?

A processor is a core component of the Aptos Indexer that handles blockchain transaction processing. It validates, transforms, and stores transactions into a database, enabling downstream applications like analytics, indexing, and querying. Testing the processor ensures that all transactions are correctly handled, maintaining data accuracy and consistency.

### What Are We Testing With This?

- **Transaction correctness**: Ensure that each transaction is processed and stored accurately.
- **Schema consistency**: Verify that the database schema is correctly set up and maintained throughout the tests.

### General Flow of how Processor Testing Works

1. You specify the transactions to test
2. Testing framework SDK spins up a mock gRPC Service with the transactions you specified to return when the processor requests transactions.
3. Processor processes the transactions and writes the output to a database.
4. Optionally, you can generate expected database output for validation.

Type of Scenarios it Supports:

1. A single transaction
2. A single batch of multiple transactions
   Input \[A, B, C]
   1. Processor processes A, B, and C
3. Sequential multiple transaction batches:
   Input \[A, B, C]
   1. Processor processes A and B
   2. Processor processes C

## Prerequisites

1. Ensure Docker Desktop is running for PostgreSQL container support.
   - **Docker Desktop Installation**: Install Docker Desktop following [this guide](https://docs.docker.com/desktop/) on your machine.
   - Start Docker Desktop if it's not running
2. Identify the transactions to test.
   - Use imported transactions or write your own custom Move scripts to generate test transactions. Refer to [Importing Transaction Guide](/build/indexer/indexer-sdk/advanced-tutorials/txn-importer) and [Generating Transaction using Move Script Guide](/build/indexer/indexer-sdk/advanced-tutorials/txn-script) for detailed instructions.
3. Import aptos-indexer-testing-framework to your Cargo.toml

<Aside type="note"> - This tutorial assumes you are using Postgres as the database. </Aside>

- **Adapting to Other Databases**:
  - Replace PostgreSQL-specific code with relevant database code you intend to use (e.g., MySQL).
  - Update schema initialization and query methods.
- **References to Processor Tests**:
  - Example: [Event Processor Tests](https://github.com/aptos-labs/aptos-indexer-processors/blob/main/rust/integration-tests/src/sdk_tests/events_processor_tests.rs#L139).

## Steps to Write a Test

### 1. Set Up the Test Environment

Before setting up the test environment, it’s important to understand the configurations being used in this step:

**What Are These Configurations?**

`generate_file_flag`

- If `generate_file_flag` is true, the test will overwrite any saved database outputs from previous test runs. If `generate_file_flag` is false, the test will only compare the actual database output with the expected database output and log differences.

`custom_output_path`

- An optional configuration to specify a custom path where the expected database output will be stored.
  If not provided, the test will use the default path defined by DEFAULT\_OUTPUT\_FOLDER.

`DEFAULT_OUTPUT_FOLDER`

- This constant defines the default folder where the system stores output files for the tests.
  Example: "sdk\_expected\_db\_output\_files".
  Modify this value in your configuration if you prefer a different default directory.

```rust
let (generate_file_flag, custom_output_path) = get_test_config();
let output_path = custom_output_path.unwrap_or_else(|| format!("{}/imported_mainnet_txns", DEFAULT_OUTPUT_FOLDER));

// Setup DB and replace as needed
let mut db = PostgresTestDatabase::new();
db.setup().await.unwrap();

let mut test_context = SdkTestContext::new(&[CONST_VARIABLE_OF_YOUR_TEST_TRANSACTION]); // Replace with your test transaction
if test_context.init_mock_grpc().await.is_err() {
    panic!("Failed to initialize mock grpc");
};
```

**Explanation of Each Component:**

`get_test_config():`

This function fetches the configurations (diff\_flag and custom\_output\_path) for the test.
Modify or extend this function if you want to support additional custom flags or configurations.
output\_path:

Combines DEFAULT\_OUTPUT\_FOLDER with the subfolder imported\_mainnet\_txns if no custom\_output\_path is specified.
This ensures all output files are stored in a predictable location.

`PostgresTestDatabase::new():`

Creates a new PostgreSQL database instance for testing.
This database is isolated, ensuring no interference with production or other test environments.

`SdkTestContext::new():`

Initializes the test context with the transaction(s) you want to test.
Replace CONST\_VARIABLE\_OF\_YOUR\_TEST\_TRANSACTION with the appropriate variable or constant representing the transaction(s) to be tested.

`init_mock_grpc():`

Initializes a mock gRPC service for the test.
This allows the processor to simulate transactions without interacting with live blockchain data.

### 2. Configure the Processor

<Aside type="note">
  - Each test runs in an isolated environment using a PostgreSQL container to prevent interference.
</Aside>

```rust
let db_url = db.get_db_url();
let transaction_stream_config = test_context.create_transaction_stream_config();
let postgres_config = PostgresConfig {
    connection_string: db_url.to_string(),
    db_pool_size: 100,
};

let db_config = DbConfig::PostgresConfig(postgres_config);
let default_processor_config = DefaultProcessorConfig {
    per_table_chunk_sizes: AHashMap::new(),
    channel_size: 100,
    deprecated_tables: HashSet::new(),
};

let processor_config = ProcessorConfig::DefaultProcessor(default_processor_config);
let processor_name = processor_config.name();
```

### 3. Create the Processor

```rust
let processor = DefaultProcessor::new(indexer_processor_config)
    .await
    .expect("Failed to create processor");
```

Note: Replace `DefaultProcessor` with the processor you are testing.

### 4. Setup a Query

Set up a query to load data from the local database and compare it with expected results, see [example loading function](https://github.com/aptos-labs/aptos-indexer-processors/blob/a8f9c5915f4e3f1f596ed3412b8eb01feca1aa7b/rust/integration-tests/src/diff_test_helper/default_processor.rs#L45)

### 5. Setup a Test Context run function

Use the test\_context.run() function to execute the processor, validate outputs using your query, and optionally generate database output files:

<Aside type="note">
  Key Considerations:

  - Each test runs in an isolated environment using a PostgreSQL container to prevent interference.
  - Proper handling of versions ensures transactions are processed and validated in the correct order.
  - Validation logic must detect changes or issues by comparing processor output with the expected baseline.
</Aside>

```rust
    let txn_versions: Vec<i64> = test_context
        .get_test_transaction_versions()
        .into_iter()
        .map(|v| v as i64)
        .collect();

    let db_values = test_context
        .run(
            &processor,
            generate_file_flag,
            output_path.clone(),
            custom_file_name,
            move || {
                let mut conn = PgConnection::establish(&db_url).unwrap_or_else(|e| {
                    eprintln!("[ERROR] Failed to establish DB connection: {:?}", e);
                    panic!("Failed to establish DB connection: {:?}", e);
                });

                let db_values = match load_data(&mut conn, txn_versions.clone()) {
                    Ok(db_data) => db_data,
                    Err(e) => {
                        eprintln!("[ERROR] Failed to load data {}", e);
                        return Err(e);
                    },
                };

                if db_values.is_empty() {
                    eprintln!("[WARNING] No data found for versions: {:?}", txn_versions);
                }

                Ok(db_values)
            },
        )
```

### 6. Run the Processor Test

Once you have your test ready, run the following command to generate the expected output for validation:

```shellscript
cargo test sdk_tests -- generate-output
```

Arguments:
generate-output: Set this true if you want to generate or overwrite saved database output, or false if you want to compare database outputs in diff mode.
output-path: it's an optional argument to specify the output path for the db output.

The expected database output will be saved in the specified output\_path or `sdk_expected_db_output_files` by default.

***

## FAQ

### What Types of Tests Does It Support?

- The testing framework allows you to write tests that compare the database outputs of processors. It helps you catch changes in database output when you're updating or developing your processor.

### What Is `TestContext`?

`TestContext` is a struct that manages:

- `transaction_batches`: A collection of transaction batches.
- `postgres_container`: A PostgreSQL container for test isolation.

It initializes and manages the database and transaction context for tests.

#### What Does `TestContext.run` Do?

This function executes the processor, applies validation logic, and optionally generates output files.

#### Key Features:

- Flexible Validation: Accepts a user-provided verification function.
- Multi-Table Support: Handles data across multiple tables.
- Retries: Uses exponential backoff and timeout for retries.
- Optional File Generation: Controlled by a flag.

#### Example Usage:

```rust
pub async fn run<F>(
    &mut self,
    processor: &impl ProcessorTrait,
    txn_version: u64,
    generate_files: bool,             // Flag to control file generation
    output_path: String,              // Output path
    custom_file_name: Option<String>, // Custom file name
    verification_f: F,                // Verification function
) -> anyhow::Result<HashMap<String, Value>>
where
```

### How to Generate Expected DB Output?

Run the following command:

```shellscript
cargo test sdk_tests -- --nocapture generate-output
```

Supported Test Args:

1. `generate-output`
2. `output_path`

***

## Troubleshooting and Tips

1. **Isolate Tests**: Use Docker containers for database isolation.
2. **Handle Non-Deterministic Fields**: Use helpers like `remove_inserted_at` to clean up timestamps before validation.
3. **Enable Debugging**: Use `eprintln!` for detailed error logging.

#### How to Debug Test Failures?

run following command to get detailed logs:

```shellscript
cargo test sdk_tests -- --nocapture
```

# Aptos Indexer Testing Framework Overview

> Testing framework for Aptos indexer processors with transaction import and Move script generation for comprehensive testing scenarios

The Aptos Indexer Testing Framework provides two ways to generate test transactions: **by Importing Transactions from Network** and **By writing a Move Scripts**. Both approaches are suited for specific scenarios based on your development and testing requirements, enabling you to test how your system handles various transaction types.

## When to Import transactions

Imported transactions are primarily used to validate processor logic or database integrity by replaying transactions from live networks.

## When to Use **Move Script** to generate transactions

Scripted transactions are primarily used to create and test transaction scenarios that are not yet live on the network. In most cases, you should use transaction importing to test your processor logic.

## Summary

Aptos-indexer-transaction-generator tool is an essential tool in the Aptos Indexer Testing Framework. Import transactions for replaying and analyzing real-world transactions, while generating transactions with **Move Scripts** is best for testing new AIPs that may impact processing logic. Choose the method that aligns with your testing goals to ensure a comprehensive validation process.

## Next Steps

For detailed instructions on how to use these methods, refer to the following guides:

1. [Importing Transactions](/build/indexer/indexer-sdk/advanced-tutorials/txn-importer)
2. [Generating Transactions with Move Scripts](/build/indexer/indexer-sdk/advanced-tutorials/txn-script)

# Importing Transactions

> Import Aptos network transactions for processor testing using transaction generator tools with local development support

## Overview

This guide explains how to import Aptos transactions for testing using the `aptos-indexer-transaction-generator` tool. These test transactions can be used to test your custom processors and support their local development.

## General Flow of Transaction Importing

First, identify the transaction versions you need to fetch from the Aptos network. This tool interacts with the [Transaction Stream](/build/indexer/txn-stream) to retrieve transaction data in JSON format. The transactions are then consolidated into a Rust file, where each transaction is represented as a constant variable. These constants can be seamlessly used as mocked inputs in processor automated tests. During testing, the processor fetches the specified transactions, processes them, and writes the results to a database. You can then verify the outcomes by loading the written data and validating it against the expected data.

## Prerequisites

1. A valid API key to connect to [Transaction Stream](/build/indexer/txn-stream/aptos-hosted-txn-stream)
2. Clone the [aptos-core](https://github.com/aptos-labs/aptos-core) repository:
   - Navigate to the `aptos-core/ecosystem/indexer-grpc/indexer-transaction-generator` directory.

## How to Import Test Transactions

### 1. Specify Versions to Import

Locate and make a copy of the file:

```shellscript
ecosystem/indexer-grpc/indexer-transaction-generator/imported_transactions/imported_transactions.yaml
```

In this file, specify the versions to import from Devnet|Testnet|Mainnet by configuring the appropriate endpoint, API key, and mapping version numbers to descriptive output names. An example configuration is shown below:

```yaml
testnet:
  transaction_stream_endpoint: https://grpc.testnet.aptoslabs.com:443
  api_key: TESTNET_API_KEY  # <--- Replace this with your API key to generate files locally
  versions_to_import:
    # Replace these with the versions you want to import
    1: 1_genesis
    2: 2_new_block_event
    3: 3_empty_txn
    278556781: 278556781_v1_coin_register_fa_metadata
    1255836496: 1255836496_v2_fa_metadata
    5979639459: 5979639459_coin_register
    5992795934: 5992795934_fa_activities
    5523474016: 5523474016_validator_txn

mainnet:
  transaction_stream_endpoint: https://grpc.mainnet.aptoslabs.com:443
  api_key: MAINNET_API_KEY
  versions_to_import:
    308783012: 308783012_fa_transfer
```

### 2. Run the Command to Import Transactions

Navigate to the `indexer-transaction-generator` directory:

```shellscript
cd aptos-core/ecosystem/indexer-grpc/indexer-transaction-generator
```

To import the specified transaction versions, execute the following command:

```shellscript
cargo run -- --testing-folder  /path/to/your/imported_transactions.yaml --output-folder /path/to/your/processor-repo/src --mode=import --network=testnet
```

This command will:

1. Read the configuration from the `imported_transactions.yaml` file located in the folder specified by the --testing-folder flag.
2. Fetch the specified transaction versions from the selected network (Devnet, Testnet or Mainnet).
3. Store the resulting JSON files in the specified output folder (/path/to/your/processor-repo/src/json\_transactions).
4. Generate a Rust file (`generated_transactions.rs`) that converts the generated transaction JSON files into constant variables for use in tests.

Note: Replace /path/to/your/processor-repo with the path to your processor repository or preferred storage location.

**Explanation of Command Flags**

1. `--testing-folder`
   What is the --testing-folder flag?
   The --testing-folder flag specifies the directory containing the imported\_transactions.yaml configuration file. The tool uses this folder to read versions you wish to import.

- Ensure the folder path matches the location of your imported\_transactions.yaml file.
- By default, this guide assumes the configuration is stored in ./imported\_transactions. Adjust the flag value if you place the file elsewhere.

2. `--output-folder`
   Specifies the destination directory where the generated transaction JSON files and Rust constants will be saved.

- Replace /path/to/your/processor-repo with your processor repository src directory or desired storage location.
- Ensure this folder is part of your version control setup if these files need to be shared.

3. `--mode`
   Specifies that the transaction generator should operate in script mode, meaning it will execute Move scripts and generate corresponding transaction data. By default, the mode is set to import, which fetches transactions from the network.
   or Use Import Mode to fetch transactions from the network. By default, the mode is set to import.

Options available:

- import
- script

4. `--network`
   Specifies the network to fetch transactions from. Options available:

- devnet
- testnet
- mainnet

## How to Use the Testing Transactions

### Export the Generated File

Update the `mod.rs` file to include the generated Rust file containing the transaction constants. If `mod.rs` doesn’t exist, create one in the target folder:

[Reference mod.rs](https://github.com/aptos-labs/aptos-indexer-processor-example/blob/main/test-transactions-example/src/json_transactions/mod.rs).

### Export the `json_transactions` Folder

Since the `generated_transactions.rs` reles on the `json_transactions` Ensure the `json_transactions` folder is properly exported in the library file for your tests have direct access to the transaction data.

[Reference lib.rs](https://github.com/aptos-labs/aptos-indexer-processor-example/blob/main/test-transactions-example/src/lib.rs).

### Integrate into Test Cases

Use the exported transaction constants directly in your test cases to simulate real transactions and validate processing logic.

[Example Crate](https://github.com/aptos-labs/aptos-indexer-processor-example/tree/main/test-transactions-example).

## Next Steps

Once the transaction constants are integrated, you can use them in processor tests to validate functionality. For detailed instructions on writing processor tests, refer to Writing Processor Tests.

# Generating Transactions with Move Scripts

> Create custom test transactions using Move scripts for processor testing with smart contract interaction simulation

import { Aside, Steps } from '@astrojs/starlight/components';

## Overview:

This section outlines how to create test transactions with Move scripts.

## Prerequisites

1. Clone the [aptos-core](https://github.com/aptos-labs/aptos-core) repository:
   - Navigate to the `aptos-core/ecosystem/indexer-grpc/indexer-transaction-generator` directory.

## How to Generate Test Transactions using Move Script

<Steps>
  1. Set up move\_fixtures folder

     Before proceeding, ensure you have the `move_fixtures` folder set up in the appropriate location:

     1. Location:
        The `move_fixtures` folder should be created in the `aptos-core/ecosystem/indexer-grpc/indexer-transaction-generator/imported_transactions` directory. This is the folder where Move scripts and their configurations for test transactions will be stored.

        <Aside type="note"> **Note:** Do not create the `move_fixtures` folder in your processor repository. All Move-related files should reside in the `aptos-core` repository under the specified directory. </Aside>
     2. Steps to set up the folder:
        - if starting fresh, remove all existing files and projects in the `move_fixtures` folder in the aptos-core repo
        - Create your own Move projects/scripts in the move\_fixtures folder (detailed in the next step)

  2. Create Your Move Project and Write your Move Script

     Create your Move project and write a module to output the scenario that you would like to test in your processor. You can refer to an example [here](https://github.com/aptos-labs/aptos-core/tree/main/ecosystem/indexer-grpc/indexer-transaction-generator/imported_transactions/move_fixtures).

  3. Set Up Test Accounts

     1. These accounts will be used to deploy your module.
     2. Set up as many accounts as you need. These accounts will be used to send the scripted transactions. Refer to the guide [here](/build/cli/setup-cli) to create accounts.
     3. Update [`aptos-core/ecosystem/indexer-grpc/indexer-transaction-generator/imported_transactions/testing_accounts.yaml`](https://github.com/aptos-labs/aptos-core/blob/main/ecosystem/indexer-grpc/indexer-transaction-generator/imported_transactions/testing_accounts.yaml) with your accounts.

     <Aside type="note"> **Note:** Do not use real accounts here. Only use **test accounts** created in the CLI specifically for testing. Always select **devnet** when setting up a test account, as it will be required later in the script to configure the account profile and fund it using the faucet. </Aside>

  4. Create a Configuration File

     Each configuration file defines a sequences of transactions for a test scenario.

     1. Create a configuration file in the `move_fixtures` [directory](https://github.com/aptos-labs/aptos-core/blob/main/ecosystem/indexer-grpc/indexer-transaction-generator/imported_transactions/move_fixtures). Name the configuration file according to the test scenario it corresponds to.
     2. This configuration file should contain unique transaction names and details for each transaction. The transactions should be listed in the order they are to be executed.
        The configuration file should be structured like this:

        - output\_name: This field specifies the name of the output file where the results of the transaction will be saved.
        - script\_path: This field holds the path to the Move script file that will be executed as part of the transaction.
        - sender\_address: : This field contains the address of the account that will send the transaction.

        The number of output is totally up to you, but the output name should be unique for each transaction. Add as many transactions as you need to test your processor.

        ```yaml
        transactions:
          - output_name: simple_user_script1
            script_path: simple_user_script
            sender_address: <account_address>
          - output_name: simple_user_script2
            script_path: simple_user_script2
            sender_address: <account_address>
        ```

  5. Generate JSON Files and Rust File

     Once the Move files and configuration are set up, run the same command used to import transactions but with extra flag `mode`:

     - testing-folder is where your Move files are stored.
     - output-folder can be set to any folder where you want to store the generated files.
     - The `--mode=script` flag specifies that the transaction generator should operate in script mode, meaning it will execute Move scripts and generate corresponding transaction data. By default, the mode is set to import, which fetches transactions from the network.

     ```shellscript
         cd ~/aptos-core/ecosystem/indexer-grpc/indexer-transaction-generator
         cargo run -- --testing-folder ./imported_transactions --output-folder ../indexer-test-transactions/src/ --script
     ```

     This command will:

     1. Read the configuration in the `move_fixtures` folder.
     2. Execute the specified Move scripts.
     3. Output the generated JSON files to the designated folder (`~/aptos-core/ecosystem/indexer-grpc/indexer-test-transactions/src/json_transactions`).
     4. Overwrite `generated_transactions.rs` with the new transaction data based on the generated JSON files. This file contains the transaction constants that can be used in tests.

  6. Verification

     Verify that the json\_transactions folder in the target directory contains the generated JSON files with the specified names from the configuration file, and ensure that generated\_transactions.rs has been updated accordingly.
</Steps>

## How to Use Test Transactions

### Export the Generated File

Update the `mod.rs` file to include the generated Rust file containing the transaction constants. If `mod.rs` doesn't exist, create one in the target folder:

[Reference mod.rs](https://github.com/aptos-labs/aptos-indexer-processor-example/blob/main/test-transactions-example/src/json_transactions/mod.rs).

### Export the `json_transactions` Folder

Since the `generated_transactions.rs` relies on the `json_transactions` Ensure the `json_transactions` folder is properly exported in the library file for your tests have direct access to the transaction data.

[Reference lib.rs](https://github.com/aptos-labs/aptos-indexer-processor-example/blob/main/test-transactions-example/src/lib.rs).

### Integrate into Test Cases

If you decided to output the rust file in a different crate, you can update you cargo.toml to import the crate containing the generated file as a dependency. Otherwise, you can simply import the generated file directly in your test file.
[Example](https://github.com/aptos-labs/aptos-indexer-processors-v2/blob/main/integration-tests/Cargo.toml#L19).

## Next Steps

Once the transaction constants are integrated, you can use them in processor tests to validate functionality. For detailed instructions on writing processor tests, refer to Writing Processor Tests.

[Example](https://github.com/aptos-labs/aptos-indexer-processors-v2/blob/main/integration-tests/src/sdk_tests/events_processor_tests.rs)

# Documentation

> Complete documentation for Aptos Indexer SDK architecture, processor development, and custom data pipeline implementation

import { ThemedImage } from '~/components/ThemedImage';

## Architecture of the Indexer SDK

In the Aptos indexing stack, a processor indexes a specific subset of data from the blockchain and writes the data into an external database.

Each processor follows this general flow:

1. Receive a stream of transactions from [Transaction Stream](/build/indexer/txn-stream)
2. Extract the relevant data from the transactions and transform it into a standardized schema
3. Store the transformed data into a database
4. Keep track of the transaction versions that have been processed

The Indexer SDK allows you to write a processor as a directed graph of independent steps.
Each `Step` has an input and output, and the output of each `Step` is connected to the input of the next `Step` by a [Kanal channel](https://github.com/fereidani/kanal).

<center>
  <ThemedImage
    alt="Indexer SDK Custom Processor Architecture"
    sources={{
light: '~/images/indexer-custom-processor-light.svg',
dark: '~/images/indexer-custom-processor-dark.svg',
}}
  />
</center>

## When to use the Indexer SDK

The Indexer SDK is useful when you want to index a custom contract or you realize you need a new kind of data that isn't available in the [Indexer API](/build/indexer/indexer-api).

The general flow to write a custom processor with the Indexer SDK is:

1. Define your database schema
2. Create a new processor
3. Create `Step`s that extract and transform data into your storage schema
4. Customize your processor by adding and connecting steps
5. Run your processor and see the data indexed into your database

## Benefits of the Indexer SDK

The Indexer SDK's architecture simplifies writing custom processors in several ways:

1. You can reuse `Step` implementations across processors which reduces duplication of common data extraction logic.
2. The SDK collects basic performance metrics, like the number of transactions processed, for each `Step`, which enables observability into subcomponents of the processor.
3. Since each `Step` is independent, you can safely customize parts of the processor without breaking the other pieces.
   For example, you can add additional `Step`'s to pre/post-process data or batch data writes. Each `Step` can also be tested in isolation from the rest of the processor.

# Advanced Tutorials

> Advanced processor development tutorials for migration, testing, and complex data processing scenarios with Aptos Indexer SDK



# Connecting Steps

> Connect processing steps in Aptos Indexer SDK to build complete data pipelines with step chaining and flow control

## Pre-requisite

At this point, you'd have already followed the [Creating a Processor](/build/indexer/indexer-sdk/documentation/create-processor) and [Creating a Step](/build/indexer/indexer-sdk/documentation/steps) guides.
Our next goal is to put those two pieces together and connect steps within the processor.

## How to connect steps

Now that you have created a step, you can connect it to other steps.
To do so, we use a builder class called `ProcessorBuilder` to specify a sequence of steps that make up a processor.

1. After you've instantiated your steps, you need to convert them into [`RunnableStep`](https://github.com/aptos-labs/aptos-indexer-processor-sdk/blob/main/aptos-indexer-processors-sdk/sdk/src/traits/runnable_step.rs#L6).
   `RunnableStep` is a trait that wraps around a step.
   It provides the necessary input and output channels that feed into the step and allows the step to be spawned in a task.
   The SDK provides a helper function [`.into_runnable_step`](https://github.com/aptos-labs/aptos-indexer-processor-sdk/blob/main/aptos-indexer-processors-sdk/sdk/src/traits/into_runnable_step.rs#L13) to convert a step into a `RunnableStep`.
2. Setup your first step with [`ProcessorBuilder::new_with_inputless_first_step`](https://github.com/aptos-labs/aptos-indexer-processor-sdk/blob/main/aptos-indexer-processors-sdk/sdk/src/builder/processor_builder.rs#L222).
   In almost all cases, the first step should be a `TransactionStreamStep`. {/* <!-- Add link --> */}
3. Connect the previous step to the next step using [`.connect_to`](https://github.com/aptos-labs/aptos-indexer-processor-sdk/blob/main/aptos-indexer-processors-sdk/sdk/src/builder/processor_builder.rs#L303).
   `connect_to` uses trait bounds to ensure at compile time that the output type of the previous step matches the input type of the next step.
   When calling `.connect_to`, a channel gets created with size `channel_size` and connects the previous and next steps.
   It also spawns a task that continuously loops the previous step -- reading data from its input channel, processing the data, and sending the output to its output channel.
4. Repeat step 3 for each step in your processor.
5. To close off the `ProcessorBuilder`, use [`.end_and_return_output_receiver`](https://github.com/aptos-labs/aptos-indexer-processor-sdk/blob/main/aptos-indexer-processors-sdk/sdk/src/builder/processor_builder.rs#L400).
   This returns an [`InstrumentedAsyncReceiver`](https://github.com/aptos-labs/aptos-indexer-processor-sdk/blob/main/aptos-indexer-processors-sdk/instrumented-channel/src/lib.rs#L88) which you can use to process the output of the last step in the graph.

Here's a simple example of connecting two steps:

```rust
let (processor_builder, buffer_receiver) = ProcessorBuilder::new_with_inputless_first_step(
      transaction_stream_step.into_runnable_step(),
  )
  .connect_to(extractor_step.into_runnable_step(), 10)
  .end_and_return_output_receiver(10);
```

Here's a [full example](https://github.com/aptos-labs/aptos-indexer-processor-example/blob/main/aptos-indexer-processor-example/src/processors/events/events_processor.rs#L75) from `aptos-indexer-processor-example`.

{/* <!-- Add link to common SDK steps --> */}

## Visualizing the processor

As you connect steps, `ProcessorBuilder` in the background is constructing a graphical representation of the steps in your processor using [`petgraph`](https://docs.rs/petgraph/latest/petgraph/).
You can see the visual representation of the graph by calling

```rust
let dot = processor_builder.graph.dot();
println!("{}", dot);
```

This will output a graph in the [DOT language](https://graphviz.gitlab.io/_pages/doc/info/lang.html) that you can visualize using tools like [Graphviz](https://graphviz.org/).

# Creating a Processor

> Step-by-step guide to create custom processors using Aptos Indexer SDK with templates and implementation patterns

This guide will walk you through setting up the basic template for a new processor.

## Pre-requisites

You've already set up your environment and have the Indexer SDK `aptos-indexer-sdk` installed.
If you haven't, follow the [Indexer SDK installation guide](/build/indexer/indexer-sdk/documentation/setup).

## Overview

Creating and running a processor will require several pieces:

1. `IndexerProcessorConfig`
2. `ProcessorConfig`
3. The processor itself. This is where you'll define a processor's config, the processor setup, and the steps that will be run to index transactions.
4. `main.rs` - The main file that will run the processor.

The next section goes through each of these pieces more explicitly and provides code examples.

## How to define `IndexerProcessorConfig`

The `IndexerProcessorConfig` defines the base configuration for all processors that you'll be running.
It should include configuration for things that are shared across multiple processors, like the database configuration and [Transaction Stream](/build/indexer/txn-stream) configuration.

`ServerArgs` parses a `config.yaml` file and bootstraps a server with all the common pieces to run a processor.

To setup the configuration for your processor and make it work with `ServerArgs`, you'll need to define a `IndexerProcessorConfig` that implements the `RunnableConfig` trait.
It also triggers a run method, which can be invoked in `main.rs`.

For basic cases, you can copy the [`IndexerProcessorConfig` from the `aptos-indexer-processor-example`](https://github.com/aptos-labs/aptos-indexer-processor-example/blob/main/aptos-indexer-processor-example/src/config/indexer_processor_config.rs) repository and modify it to fit your needs.

## How to define `ProcessorConfig`

`ProcessorConfig` is an enum that contains all the individual processor configs.
It's used by `IndexerProcessorConfig.run()` to map the processor name to the right `ProcessorConfig`.

You can see a basic example of a `ProcessorConfig` [here](https://github.com/aptos-labs/aptos-indexer-processor-example/blob/main/aptos-indexer-processor-example/src/config/processor_config.rs).
An example of a more complex setup that includes multiple processors and configurations is [`aptos-indexer-processors`](https://github.com/aptos-labs/aptos-indexer-processors-v2/blob/main/processor/src/config/processor_config.rs#L84).

## How to create a processor

Now that you've got the configuration pieces set up, the next step is to create the processor.
The processor is represented by a struct and is usually named `{PROCESSOR_NAME}Processor`, like `EventsProcessor` or `TokenV2Processor`, depending on the type of data it's indexing.

```rust
pub struct EventsProcessor {
    pub config: IndexerProcessorConfig,
    pub db_pool: ArcDbPool,
}
```

The processor's constructor should be defined like so:

```rust
pub async fn new(config: IndexerProcessorConfig) -> Result<Self> {
    // Processor setup code here, if needed
}
```

It takes in the `IndexerProcessorConfig` that you've defined and performs any setup required to instantiate the processor.
Next, your processor needs to implement the [`ProcessorTrait`](https://github.com/aptos-labs/aptos-indexer-processor-sdk/blob/a56b641a6aaca60092fcc9bbd98252f3cd703299/aptos-indexer-processors-sdk/sdk/src/traits/processor_trait.rs#L4).

```rust
#[async_trait::async_trait]
impl ProcessorTrait for EventsProcessor {
    fn name(&self) -> &'static str {
        self.config.processor_config.name()
    }

    async fn run_processor(&self) -> Result<()> {
        // Processor logic here
    }
}
```

The `run_processor` method is the most important method in the processor.

If you're using a migration-based database, like PostgreSQL, running the migrations can go inside of `run_processor`.
This is also where we implement logic to determine the appropriate starting version for the processor, verify the chain ID using [Transaction Stream](/build/indexer/txn-stream), and validate the processor's configuration.

`run_processor` also contains the instantiation of the processor's `Step`s and the specification of how these `Step`s are connected together by channels.

```rust
// Instantiate processor steps
let transaction_stream = TransactionStreamStep::new(TransactionStreamConfig {
    starting_version: Some(starting_version),
    ..self.config.transaction_stream_config.clone()
})
.await?;
// ... Instantiate the rest of your processor's steps ...

// Connect processor steps
let (_, buffer_receiver) = ProcessorBuilder::new_with_inputless_first_step(
    transaction_stream.into_runnable_step(),
)
.connect_to(extractor_step.into_runnable_step(), channel_size)
.connect_to(storer_step.into_runnable_step(), channel_size)
.connect_to(version_tracker_step.into_runnable_step(), channel_size)
.end_and_return_output_receiver(channel_size);

// Read the results from the output of the last step
loop {
    match buffer_receiver.recv().await {
        // Do something with th output
    }
}
```

You can see a full example of a processor that indexes raw Aptos events in [`aptos-indexer-processor-example`](https://github.com/aptos-labs/aptos-indexer-processor-example/blob/main/aptos-indexer-processor-example/src/processors/events/events_processor.rs).
As a reference, you can also see all of the processors that make up the [Indexer API](/build/indexer/indexer-api) in [`aptos-indexer-processors`](https://github.com/aptos-labs/aptos-indexer-processors-v2/tree/main/processor/src/processors).

## How to define `main.rs`

You may copy the [`main.rs`](https://github.com/aptos-labs/aptos-indexer-processor-example/blob/main/aptos-indexer-processor-example/src/main.rs) file from the `aptos-indexer-processor-example`.

These lines of code uses the `ServerArgs` and the `IndexerProcessorConfig` that we've defined earlier:

```rust
let args = ServerArgs::parse();
args.run::<IndexerProcessorConfig>(tokio::runtime::Handle::current())
    .await
```

# Defining a Data Schema

> Design optimized database schemas for custom processors with performance considerations and query optimization strategies

The first step with indexing is choosing a database and defining a schema for the data that you want to store.

## Schema Considerations

When designing an indexer data schema, consider the following:

- Customizability: A schema serves as an interface for your dApp to access data tailored to your specific contract or application.
  Ensure your schema is customized to meet your dApp's unique requirements.
- Query Optimization: A well-designed schema can enable more efficient data retrieval, supporting advanced operations such as aggregations, complex filtering, and table joins.
- Enhanced Performance: Schema design can significantly improve your dApp's performance.
  By using the indexer, a single indexer query can often replace multiple queries to the fullnode.

## Aptos Core Processors

All data exposed by the [Indexer API](/build/indexer/indexer-api) is initially indexed using custom processors.
Each core processor indexes a specific type of data.
You can explore the [full list of processors](https://github.com/aptos-labs/aptos-indexer-processors-v2/blob/main/processor/src/db/schema.rs).

The Aptos core processors and the [Quickstart Guide](/build/indexer/indexer-sdk/quickstart) use [PostgreSQL](https://www.postgresql.org/) as the database and [Diesel](https://diesel.rs/) as the ORM.
If you'd also like to use PostgreSQL and Diesel, you can follow the instructions in [PostgreSQL Installation](/build/indexer/indexer-sdk/quickstart#postgresql-installation).

You're free to use whatever database and ORM you prefer.
Popular alternatives include [SeaORM](https://www.sea-ql.org/SeaORM/) and [SQLx](https://github.com/launchbadge/sqlx).
If you need guidance, refer to the tutorials linked above for more information.

# Running Your Processor

> Configure and run custom processors with config.yaml setup, database connections, and production deployment guidelines

## Pre-requisites

Please first read [Creating a Processor](/build/indexer/indexer-sdk/documentation/create-processor), [Creating a Step](/build/indexer/indexer-sdk/documentation/steps), and [Connecting Steps](/build/indexer/indexer-sdk/documentation/connect-steps), which will set up your processor and connect your processor steps.

## How to setup your `config.yaml`

To run a processor, you'll need to create a `config.yaml` file.
The format of the `config.yaml` file should follow the format you've defined in your `IndexerProcessorConfig`.
For example, if you're using the `IndexerProcessorConfig` from [`aptos-indexer-processor-example`](https://github.com/aptos-labs/aptos-indexer-processor-example/blob/main/aptos-indexer-processor-example/src/config/indexer_processor_config.rs), a basic `config.yaml` would look like this:

```yaml
health_check_port: 8085
server_config:
  processor_config:
    type: "events_processor"
  transaction_stream_config:
    indexer_grpc_data_service_address: "https://grpc.mainnet.aptoslabs.com:443"
    starting_version: 0
    auth_token: "{AUTH_TOKEN}"
    request_name_header: "events-processor"
  db_config:
    postgres_connection_string: postgresql://postgres:@localhost:5432/example
```

The `processor_config` field should match how `ProcessorConfig` is defined in the `IndexerProcessorConfig`, and the same applies for `db_config` and `DbConfig`.

`TransactionStreamConfig` is a config provided by the `transaction-stream` crate.
It requires `indexer_grpc_data_service_address`, `auth_token`, and `request_name_header` to be set.
To get the `indexer_grpc_data_service_address` and `auth_token`, you can follow the guide [here](/build/indexer/txn-stream/aptos-hosted-txn-stream).

`TransactionStreamConfig` also supports more optional fields to modify the connection to [Transaction Stream](/build/indexer/txn-stream), which you can learn more about [here](https://github.com/aptos-labs/aptos-indexer-processor-sdk/tree/main/aptos-indexer-processors-sdk/transaction-stream).

## Running your processor

Once your `config.yaml` is setup, you can run your processor with:

```shellscript
cd /path/to/your/processor/crate
cargo run --release -- -c config.yaml
```

In your terminal, you should start to see logs like this:

```shellscript
{"timestamp":"2025-01-13T21:23:21.785452Z","level":"INFO","message":"[Transaction Stream] Successfully connected to GRPC stream","stream_address":"https://grpc.mainnet.aptoslabs.com/","connection_id":"ec67ecc4-e041-4f17-a2e2-441e7ff21487","start_version":2186504987,"filename":"/Users/reneetso/.cargo/git/checkouts/aptos-indexer-processor-sdk-2f3940a333c8389d/e6867c5/aptos-indexer-processors-sdk/transaction-stream/src/transaction_stream.rs","line_number":349,"threadName":"tokio-runtime-worker","threadId":"ThreadId(4)"}
{"timestamp":"2025-01-13T21:23:21.785664Z","level":"INFO","message":"Spawning polling task","step_name":"TransactionStreamStep","filename":"/Users/reneetso/.cargo/git/checkouts/aptos-indexer-processor-sdk-2f3940a333c8389d/e6867c5/aptos-indexer-processors-sdk/sdk/src/traits/pollable_async_step.rs","line_number":112,"threadName":"tokio-runtime-worker","threadId":"ThreadId(23)"}
{"timestamp":"2025-01-13T21:23:21.785693Z","level":"INFO","message":"Spawning processing task","step_name":"TransactionStreamStep","filename":"/Users/reneetso/.cargo/git/checkouts/aptos-indexer-processor-sdk-2f3940a333c8389d/e6867c5/aptos-indexer-processors-sdk/sdk/src/traits/pollable_async_step.rs","line_number":204,"threadName":"tokio-runtime-worker","threadId":"ThreadId(23)"}
{"timestamp":"2025-01-13T21:23:21.785710Z","level":"INFO","message":"Spawning processing task","step_name":"FungibleAssetExtractor","filename":"/Users/reneetso/.cargo/git/checkouts/aptos-indexer-processor-sdk-2f3940a333c8389d/e6867c5/aptos-indexer-processors-sdk/sdk/src/traits/async_step.rs","line_number":87,"threadName":"tokio-runtime-worker","threadId":"ThreadId(4)"}
{"timestamp":"2025-01-13T21:23:21.785912Z","level":"INFO","message":"Spawning processing task","step_name":"FungibleAssetStorer","filename":"/Users/reneetso/.cargo/git/checkouts/aptos-indexer-processor-sdk-2f3940a333c8389d/e6867c5/aptos-indexer-processors-sdk/sdk/src/traits/async_step.rs","line_number":87,"threadName":"tokio-runtime-worker","threadId":"ThreadId(4)"}
{"timestamp":"2025-01-13T21:23:21.785978Z","level":"INFO","message":"Spawning polling task","step_name":"VersionTrackerStep: ()","filename":"/Users/reneetso/.cargo/git/checkouts/aptos-indexer-processor-sdk-2f3940a333c8389d/e6867c5/aptos-indexer-processors-sdk/sdk/src/traits/pollable_async_step.rs","line_number":112,"threadName":"tokio-runtime-worker","threadId":"ThreadId(14)"}
{"timestamp":"2025-01-13T21:23:21.786018Z","level":"INFO","message":"Spawning processing task","step_name":"VersionTrackerStep: ()","filename":"/Users/reneetso/.cargo/git/checkouts/aptos-indexer-processor-sdk-2f3940a333c8389d/e6867c5/aptos-indexer-processors-sdk/sdk/src/traits/pollable_async_step.rs","line_number":204,"threadName":"tokio-runtime-worker","threadId":"ThreadId(14)"}
```

# Initial Setup

> Set up Aptos Indexer SDK dependencies and configuration for custom processor development with Cargo.toml configuration

If you're creating a custom processor from scratch, we recommend following the [Quickstart Guide](/build/indexer/indexer-sdk/quickstart).
The quickstart guide provides a template processor and includes all of this setup.

If you're migrating an existing processor to the Indexer SDK, follow the steps below.

Add `aptos-indexer-processor-sdk` to your `Cargo.toml`.

```toml
[dependencies]
aptos-indexer-processor-sdk = { git = "https://github.com/aptos-labs/aptos-indexer-processor-sdk.git", rev = "aptos-indexer-processor-sdk-v1.0.0" }
```

`aptos-indexer-processor-sdk` includes the following features:

1. `postgres_full` - Interface layer to integrate Postgres with your processor.
2. `testing_framework` - An e2e testing framework for testing processors. If you want to write tests for your processor, add this feature to the crate.

{/* <!-- Add list of SDK releases once we have that --> */}

# Creating a Step

> Build processing steps as building blocks for Aptos Indexer SDK processors with extraction, transformation, and storage logic

import { Aside } from '@astrojs/starlight/components';

## What is a step?

A step is a unit of processing logic in the SDK and can be used to define logic for the extraction, transformation, or storing of data.
Steps are the building blocks of a processor.
The Aptos core processors represent (1) getting a stream of transactions from [Transaction Stream](/build/indexer/txn-stream), (2) extracting the data, (3) writing to a database, and (4) tracking the progress, each as separate steps.

There are two types of steps in the SDK:

1. **AsyncStep**: Processes a batch of input items and returns a batch of output items.
2. **PollableAsyncStep**: Does the same as `AsyncStep`, but it also periodically polls its internal state and returns a batch of output items if available.

## How to create a Step

To create a step with the SDK, follow these instructions:

1. Implement the `Processable` trait. This trait defines several important details about the step: the input and output types, the processing logic, and the run type (either `AsyncStepRunType` or `PollableAsyncStepRunType`).

   ```rust
   #[async_trait]
   impl Processable for MyExtractorStep {
       // The Input is a vec of Transaction 
       type Input = Vec<Transaction>;
       // The Output is a vec of MyData
       type Output = Vec<MyData>;

       // Depending on the type of step this is, the RunType is either
       // - AsyncRunType
       // - PollableAsyncRunType
       type RunType = AsyncRunType;

   	// Processes a batch of input items and returns a batch of output items.
       async fn process(
           &mut self,
           input: TransactionContext<Vec<Transaction>>,
       ) -> Result<Option<TransactionContext<Vec<MyData>>>, ProcessorError> {
           let transactions = input.data;
           let data = transactions.iter().map(|transaction| {
               // Define the processing logic to extract MyData from a Transaction
           }).collect();
           
           Ok(Some(TransactionContext {
               data,
               metadata: input.metadata,
           }))
       }
   }
   ```

   <Aside type="note">
     In most cases, you're going to be processing a list of inputs to a list of outputs.
     To speed up the processing, we recommend using [`rayon`](https://docs.rs/rayon/latest/rayon/) to process sequential computations in parallel.
     You can see an example of how we use [`rayon.par_iter`](https://docs.rs/rayon/latest/rayon/#basic-usage-and-the-rayon-prelude) to parallelize the processing [here](https://github.com/aptos-labs/aptos-indexer-processor-example/blob/main/aptos-indexer-processor-example/src/processors/events/events_extractor.rs#L30).
   </Aside>

   In the example code above, you'll notice that the input and output types are wrapped within a `TransactionContext`.
   `TransactionContext` contains relevant metadata about the batch of data being processed, such as the transaction versions and timestamp, and are used for metrics and logging.

2. Implement the `NamedStep` trait. This is used for logging.

   ```rust
   impl NamedStep for MyExtractorStep {
       fn name(&self) -> String {
           "MyExtractorStep".to_string()
       }
   }
   ```

3. Implement either `AsyncStep` trait or `PollableAsyncStep` trait, which defines how the step will be run in the processor.
   1. If you're using `AsyncStep`, add this to your code:

      ```rust
      impl AsyncStep for MyExtractorStep {}
      ```

   2. If you're creating a `PollableAsyncStep`, you will need to define the poll interval and what the step should do every time it polls.

      ```rust
      #[async_trait]
      impl<T: Send + 'static> PollableAsyncStep for MyPollStep<T>
      where
          Self: Sized + Send + Sync + 'static,
          T: Send + 'static,
      {
          fn poll_interval(&self) -> std::time::Duration {
              // Define duration
          }

          async fn poll(&mut self) -> Result<Option<Vec<TransactionContext<T>>>, ProcessorError> {
              // Define code here on what this step should do every time it polls
              // Optionally return a batch of output items
          }
      }
      ```

## Parsing Transactions

When building the extractor step, you'll need to define how you want to parse your data from transactions.
Read more about how to parse your data from transactions [here](/build/indexer/indexer-sdk/documentation/steps/parsing-txns).

## Common SDK steps

The SDK comes with a set of [common steps](https://github.com/aptos-labs/aptos-indexer-processor-sdk/tree/main/aptos-indexer-processors-sdk/sdk/src/common_steps) that you can use to build your processor.

1. `TransactionStreamStep` provides a stream of Aptos transactions to the processor. Read more about it [here](/build/indexer/indexer-sdk/documentation/steps/transaction-stream).
2. `TimedBufferStep` buffers a batch of items and periodically polls to release the items to the next step
3. `VersionTrackerStep` tracks the progress of the processor and checkpoints the processor's progress. Read more about it [here](/build/indexer/indexer-sdk/documentation/version-tracking).
4. `OrderByVersionStep` orders transaction contextx by their starting versions. It buffers ordered these contexts and releases them at every poll interval.
5. `WriteRateLimitStep` limits the number of bytes written to the database per second.

# Parsing Transactions

> Parse and extract data from Aptos blockchain transactions using processor steps with event filtering and data transformation

import { Aside } from '@astrojs/starlight/components';

{/* <IndexerBetaNotice /> */}

{/* <!--
  Things to add:
  - We should have tabs for each language that mentions helper functions for extracting the thing you want. For example, if the user is trying to extract the entry function arguments, there should be a function like `get_entry_function_arguments` and we show how to use it in each language and where it comes from in the SDK.
  --> */}

Fundamentally an indexer processor is just something that consumes a stream of a transactions and writes processed data to storage. Let's dive into what a transaction is and what kind of information you can extract from one.

## What is a transaction?

A transaction is a unit of execution on the Aptos blockchain. If the execution of the program in a transaction (e.g. starting with an entry function in a Move module) is successful, the resulting change in state will be applied to the ledger. Learn more about the transaction lifecycle at [this page](/network/blockchain/blockchain-deep-dive#life-of-a-transaction).

There are four types of transactions on Aptos:

- Genesis
- Block metadata transactions
- State checkpoint transactions
- User transactions

The first 3 of these are internal to the system and are not relevant to most processors; we do not cover them in this guide.

Generally speaking, most user transactions originate from a user calling an entry function in a Move module deployed on chain, for example `0x1::coin::transfer`. In all other cases they originate from [Move scripts](/build/smart-contracts/scripts). You can learn more about the different types of transactions [here](/network/blockchain/txns-states#types-of-transaction-payloads).

A user transaction that a processor handles contains a variety of information. At a high level it contains:

- The payload that was submitted.
- The changes to the ledger resulting from the execution of the function / script.

We'll dive into this in the following sections.

## What is important in a transaction?

### Payload

The payload is what the user submits to the blockchain when they wish to execute a Move function. Some of the key information in the payload is:

- The sender address
- The address + module name + function name of the function being executed.
- The arguments to the function.

There is other potentially interesting information in the payload that you can learn about at [this page](/network/blockchain/txns-states#contents-of-a-transaction).

### Events

Events are emitted during the execution of a transaction. Each Move module can define its own events and choose when to emit the events during execution of a function.

For example, in Move you might have the following:

```move filename="member_invited_event.move"
struct MemberInvitedEvent has store, drop {
    member: address,
}

public entry fun invite_member(member: address) {
    event::emit_event(
        &mut member_invited_events,
        MemberInvitedEvent { member },
    );
}
```

If `invite_member` is called, you will find the `MemberInvitedEvent` in the transaction.

<Aside type="note">
  Why emit events?

  This is a good question! In some cases, you might find it unnecessary to emit events since you can just parse the writesets. However, sometimes it is quite difficult to get all the data you need from the different "locations" in the transaction, or in some cases it might not even be possible, e.g. if you want to index data that isn't included in the writeset. In these cases, events are a convenient way to bundle together everything you want to index.
</Aside>

### Writesets

When a transaction executes, it doesn't directly affect on-chain state right then. Instead, it outputs a set of changes to be made to the ledger, called a writeset. The writeset is applied to the ledger later on after all validators have agreed on the result of the execution.

Writesets show the end state of the on-chain data after the transaction has occurred. They are the source of truth of what data is stored on-chain. There are several types of write set changes:

- Write module / delete module
- Write resource / delete resource
- Write table item / delete table item

{/* <!-- Add more information about writesets, ideally once have the helper functions. --> */}

# Transaction Stream Step

> Core transaction streaming step for processors with gRPC connections, batch fetching, and resilient connection management

The `TransactionStreamStep` is a foundational component in the transaction processing pipeline. It establishes a gRPC connection with the `TransactionStream` service, fetches transactions in batches, and outputs them for further processing. This step also manages connection retries and reconnections in case of transient failures. Typically, this is the initial step in a processor, responsible for streaming transactions for downstream steps.

## Key Responsibilities

1. **Fetch Transactions**: Retrieves transaction batches from a gRPC service.
2. **Manage Connections**: Handles gRPC reconnections to ensure a resilient stream.
3. **Provide Metadata**: Attaches contextual information like versions and timestamps to the transactions.

## Struct Definition

The `TransactionStreamStep` struct is defined as follows:

```rust
pub struct TransactionStreamStep
where
    Self: Sized + Send + 'static,
{
    transaction_stream_config: TransactionStreamConfig,
    pub transaction_stream: Mutex<TransactionStreamInternal>,
}
```

## How It Works

- The `TransactionStreamStep` connects to the gRPC `TransactionStream` service.
- It continuously polls for new transactions using the `poll` method.
- Each batch is wrapped in a `TransactionContext`, which includes metadata such as:
  - Start and end versions.
  - Timestamps of transactions.
  - Batch size in bytes.
- If the connection is interrupted, it attempts to reconnect seamlessly.

# Version Tracking

> Implement version tracking in processors using VersionTrackerStep for reliable data processing state management

## Version Tracking

The `VersionTrackerStep` is a [common step in the SDK](https://github.com/aptos-labs/aptos-indexer-processors-v2/blob/main/processor/src/processors/events/events_processor.rs#L125) method as other steps.  Upon a successfully processed batch, the `VersionTrackerStep` will [call](https://github.com/aptos-labs/aptos-indexer-processor-sdk/blob/main/aptos-indexer-processors-sdk/sdk/src/common_steps/version_tracker_step.rs#L57) the trait implementation of `save_processor_status()`.

### ProcessorStatusSaver

The `ProcessorStatusSaver` trait requires the implementation of the method `save_processor_status` with the following signature:

```rust
async fn save_processor_status(
        &self,
        last_success_batch: &TransactionContext<()>,
    ) -> Result<(), ProcessorError>;
```

This method is where checkpointing should be written.
If you're writing to Postgres, you can use the SDK's Postgres implementation [here](https://github.com/aptos-labs/aptos-indexer-processor-sdk/blob/main/aptos-indexer-processors-sdk/sdk/src/postgres/utils/checkpoint.rs#L66).
It is possible to checkpoint progress in different ways by using enums.
The SDK's Postgres implementation inserts using a simple [`processor_status` model](https://github.com/aptos-labs/aptos-indexer-processor-sdk/blob/main/aptos-indexer-processors-sdk/sdk/src/postgres/models/processor_status.rs).

## Restart Behavior

Now that the processor successfully writes to the chosen store for version tracking, upon restarting it needs to retrieve the latest successful version from that store.
[Here is an example](https://github.com/aptos-labs/aptos-indexer-processor-sdk/blob/main/aptos-indexer-processors-sdk/sdk/src/postgres/utils/checkpoint.rs#L118) of a `get_starting_version()` method that returns the latest processed version saved.
This `starting_version: u64` can then be used as below.
If there is no checkpoint, the processor will start from the beginning of the chain.

```rust
 let transaction_stream = TransactionStreamStep::new(TransactionStreamConfig {
            starting_version: Some(starting_version),
            ..self.config.transaction_stream_config.clone()
        })
        .await?;
```

## Backfilling

The SDK does not provide an implementation of `ProcessorStatusSaver` that will save backfill progress.
To enable saving backfill progress, `IndexerProcessorConfig`, `ProcessorStatusSaver` and `get_starting_version()` need some updates.
Without these changes, it is difficult to run a live processor at the latest transaction version as well as a backfill processor.

### Updates to Config

[Add an additional field](https://github.com/aptos-labs/aptos-indexer-processors-v2/blob/main/processor/src/config/processor_mode.rs#L29) on your `IndexerProcessorConfig` for a `BackfillConfig`.
In this implementation, the `BackfillConfig` is part of an enum `ProcessorMode` that is used to determine the mode the processor is running in.
In backfill mode, the processor starts from a different version and the progress is saved in a separate table.

### Updates to `config.yaml`

Add the `backfill_config` section to `server_config` in your yaml file to set `backfill_alias`. [Example](https://github.com/aptos-labs/aptos-indexer-processors-v2/blob/main/processor/example-backfill-config.yaml)

### Backfill Processor Status Table

Use a separate table for backfill processor status to avoid write conflicts. This table (`backfill_processor_status_table`) uses `backfill_alias` as the primary key instead of `processor_name` to prevent conflicts with the main `processor_status` table when running head and backfill processors concurrently.
Create multiple backfill processors with differing `backfill_alias` and transaction version ranges for a faster backfill.
Expand on this [implementation](https://github.com/aptos-labs/aptos-indexer-processors-v2/blob/main/processor/src/db/backfill_processor_status.rs).
This model introduces a new state, `BackfillStatus`, which is either `InProgress` or `Complete` which will determine the backfilling restart behavior.

### Updates to ProcessorStatusSaver

Expand your `ProcessorStatusSaver` implementation to include a `Backfill` variant that extracts the `backfill_alias` from the `BackfillConfig`, and the `backfill_start_version` `backfill_end_version` from `IndexerProcessorConfig.transaction_stream_config` [like this](https://github.com/aptos-labs/aptos-indexer-processors-v2/blob/main/processor/src/processors/processor_status_saver.rs#L96).
Update the corresponding write query to write to the new `backfill_processor_status` table.

### Updates to get\_starting\_version

Add a [statement](https://github.com/aptos-labs/aptos-indexer-processors-v2/blob/main/processor/src/processors/processor_status_saver.rs#L190) in your `get_starting_version` method to query the `backfill_processor_status_table` when the `BackfillConfig` field is present in `IndexerProcessorConfig` .

# Quickstart Guide on Aptos Indexer SDK

> Get started with Aptos Indexer SDK to build custom Rust processors for indexing blockchain events into PostgreSQL databases

## What to expect from this guide

This guide will walk you through setting up and running a Rust processor to index events on the Aptos blockchain into PostgreSQL.
We provide a template processor that you can customize to index events from your custom contracts.
By the end of the guide, you should have a basic understanding of how a processor works and be able to customize the processor for your indexing needs.

## Get started

To get started, clone
the [aptos-indexer-processor-sdk](https://github.com/aptos-labs/aptos-indexer-processor-sdk) repo.

```text
# HTTPS
https://github.com/aptos-labs/aptos-indexer-processor-sdk.git

# SSH
git@github.com:aptos-labs/aptos-indexer-processor-sdk.git
```

Processors consume transactions from the Transaction Stream Service. In order to use the Labs-Hosted Transaction Stream
Service you need an authorization token.
Follow [this guide](/build/indexer/txn-stream/aptos-hosted-txn-stream#authorization-via-api-key)
to guide to get a token from the Developer Portal. Create an API Key for `Testnet`, as this tutorial is for `Testnet`.
Once you’re done, you should have a token that looks like this:

```text
aptoslabs_yj4bocpaKy_Q6RBP4cdBmjA8T51hto1GcVX5ZS9S65dx
```

You also need the following tools:

- Rust 1.79: [Installation Guide](https://www.rust-lang.org/tools/install)
- Cargo: [Installation Guide](https://doc.rust-lang.org/cargo/getting-started/installation.html#install-rust-and-cargo)

We use [PostgreSQL](https://www.postgresql.org/) as our database and [Diesel](https://diesel.rs/guides/getting-started) as our ORM in this tutorial. You’re free to use whatever you want, but this tutorial is geared
towards PostgreSQL for the sake of simplicity. We use the following database configuration and tools:

### PostgreSQL Installation (for macOS)

1. `brew install libpq` ([this is a postgres C API library](https://formulae.brew.sh/formula/libpq)). Also perform all export commands post-installation

```shellscript
export PATH="/opt/homebrew/opt/libpq/bin:$PATH"
export LDFLAGS="-L/opt/homebrew/opt/libpq/lib"
export CPPFLAGS="-I/opt/homebrew/opt/libpq/include"
```

2. `brew install postgres`
3. `pg_ctl -D /opt/homebrew/var/postgres start` or `brew services start postgresql`
4. `/opt/homebrew/bin/createuser -s postgres`
5. Ensure you're able to do: `psql postgres`
6. `cargo install diesel_cli --no-default-features --features postgres`
7. Make sure that you're in the DB folder (run `cd src/db/postgres` from base directory), run `diesel migration run --database-url postgresql://localhost/postgres`
   a. If for some reason this database is already being used, try a different db. e.g.
   `DATABASE_URL=postgres://postgres@localhost:5432/indexer_v2 diesel database reset`

- We will use a database hosted on `localhost` on the port `5432`, which should be the default.
- When you create your username, keep track of it and the password you use for it.
- To easily view your database data, consider using a GUI like [DBeaver](https://dbeaver.io/)
  _recommended_, [pgAdmin](https://www.pgadmin.org/), or [Postico](https://eggerapps.at/postico2/).

## Set up your environment

Make sure to start the `postgresql` service:

The command for Linux/WSL might be something like:

```shellscript
sudo service postgresql start
```

For mac, if you’re using brew, start it up with:

```shellscript
brew services start postgresql
```

## **Configure your processor**

Now let’s set up the configuration details for the actual indexer processor we’re going to use.

### **Set up your config.yaml file**

In the example folder, there is a sample config.yaml file that should look something like this:

```yaml
# This is a template yaml for the processor
health_check_port: 8085
server_config:
  transaction_stream_config:
    indexer_grpc_data_service_address: "https://grpc.mainnet.aptoslabs.com:443"
    auth_token: "AUTH_TOKEN"
    request_name_header: "events-processor"
    starting_version: 0
  postgres_config:
    connection_string: postgresql://postgres:@localhost:5432/example
```

Open the `config.yaml` file and update these fields:

- `auth_token` - the auth token you got from the Developer Portal
- `postgres_connection_string` - connection string to your PostgreSQL database

### More customization with config.yaml

You can customize additional configuration with the `config.yaml` file.

To start at a specific ledger version, you can specify the version in the `config.yaml` file with:

```yaml
starting_version: <Starting Version>
```

To stop processing at a specific ledger version, you can specify the ending version with:

```yaml
request_ending_version: <Ending Version>
```

If you want to use a different network, change the `indexer_grpc_data_service_address` field to the corresponding
desired value:

```yaml
# Devnet
indexer_grpc_data_service_address: grpc.devnet.aptoslabs.com:443

# Testnet
indexer_grpc_data_service_address: grpc.testnet.aptoslabs.com:443

# Mainnet
indexer_grpc_data_service_address: grpc.mainnet.aptoslabs.com:443
```

In this tutorial, we are using `testnet` so update the `indexer_grpc_data_service_address` to `grpc.testnet.aptoslabs.com:443`.

## Create the events processor

At a high level, each processor is responsible for receiving a stream of transactions, parsing and transforming the
relevant data, and storing the data into a database.

### Define the database schema

In `src/db/migrations`, you will see the events migration, which defines the database schema that will be used to store the events.

```sql up.sql
CREATE TABLE events (
    sequence_number BIGINT NOT NULL,
    creation_number BIGINT NOT NULL,
    account_address VARCHAR(66) NOT NULL,
    transaction_version BIGINT NOT NULL,
    transaction_block_height BIGINT NOT NULL,
    type TEXT NOT NULL,
    data JSONB NOT NULL,
    inserted_at TIMESTAMP NOT NULL DEFAULT NOW(),
    event_index BIGINT NOT NULL,
    indexed_type VARCHAR(300) NOT NULL,
    PRIMARY KEY (transaction_version, event_index)
);
```

When you apply migrations, diesel will re-generate the `schema.rs` file, which looks like this:

```rust schema.rs
diesel::table! {
    events (transaction_version, event_index) {
        sequence_number -> Int8,
        creation_number -> Int8,
        #[max_length = 66]
        account_address -> Varchar,
        transaction_version -> Int8,
        transaction_block_height -> Int8,
        #[sql_name = "type"]
        type_ -> Text,
        data -> Jsonb,
        inserted_at -> Timestamp,
        event_index -> Int8,
        #[max_length = 300]
        indexed_type -> Varchar,
    }
}
```

In `schema.rs`, you'll see two other important tables:

- `ledger_infos` which tracks the chain id of the ledger being indexed
- `processor_status` which tracks the `last_success_version` of the processor

### Define the processing logic

The file `src/main.rs` contains the code which defines the events processor. The key components are:

1. `insert_events_query` defines the diesel query to insert events into the database.
   ```rust
   fn insert_events_query(
       items_to_insert: Vec<EventModel>,
   ) -> impl QueryFragment<Pg> + diesel::query_builder::QueryId + Send {
       use crate::schema::events::dsl::*;
       diesel::insert_into(crate::schema::events::table)
           .values(items_to_insert)
           .on_conflict((transaction_version, event_index))
           .do_nothing()
   }
   ```
2. `process` is a helper function that wraps around a regular processor.
   In the background, this powerful function handles connecting to Transaction Stream, processing transactions given a transform function that you define, applying database migrations, and tracking the processor's status.

   ```rust
   process(
           "events_processor".to_string(), // name of the processor that will be used to track the processor status
           MIGRATIONS, // migrations to be applied to the database
           async |transactions, conn_pool| {
             // transform from transaction to events and insert the events into the database
           },
   ).await?;
   ```

## Run the processor

With the `config.yaml` you created earlier, you’re ready to run the events processor:

```shellscript
cd examples/postgres-basic-events-example
cargo run --release -- -c config.yaml
```

You should see the processor start to index Aptos blockchain events!

```text
{"timestamp":"2024-08-15T01:06:35.169217Z","level":"INFO","message":"[Transaction Stream] Received transactions from GRPC.","stream_address":"https://grpc.testnet.aptoslabs.com/","connection_id":"5575cb8c-61fb-498f-aaae-868d1e8773ac","start_version":0,"end_version":4999,"start_txn_timestamp_iso":"1970-01-01T00:00:00.000000000Z","end_txn_timestamp_iso":"2022-09-09T01:49:02.023089000Z","num_of_transactions":5000,"size_in_bytes":5708539,"duration_in_secs":0.310734,"tps":16078,"bytes_per_sec":18371143.80788713,"filename":"/Users/reneetso/.cargo/git/checkouts/aptos-indexer-processor-sdk-2f3940a333c8389d/e1e1bdd/rust/transaction-stream/src/transaction_stream.rs","line_number":400,"threadName":"tokio-runtime-worker","threadId":"ThreadId(6)"}
{"timestamp":"2024-08-15T01:06:35.257756Z","level":"INFO","message":"Events version [0, 4999] stored successfully","filename":"src/processors/events/events_storer.rs","line_number":75,"threadName":"tokio-runtime-worker","threadId":"ThreadId(10)"}
{"timestamp":"2024-08-15T01:06:35.257801Z","level":"INFO","message":"Finished processing events from versions [0, 4999]","filename":"src/processors/events/events_processor.rs","line_number":90,"threadName":"tokio-runtime-worker","threadId":"ThreadId(17)"}
```

## Customize the processor

In most cases, you want to index events from your own contracts. The example processor offers a good starting point to
creating your own custom processor.

To customize the processor to index events from your custom contract, you can make these changes:

1. Change the database schema to a format that better matches your dapp or API.
   a. Create a new migration with diesel:

```shellscript
  diesel migration generate {migration_name}
```

b. Add your migration changes to `up.sql` and `down.sql`, then apply the migration:

```shellscript
  diesel migration run --database-url={YOUR_DATABASE_URL}
```

c. The `schema.rs` file will be updated automatically. You can then create a diesel query that uses the new schema.
2\. Update the transform logic in `process()`. You can filter by specific event types and extract specific event data from your custom contract

## Migrate from legacy processors

If you're migrating from the legacy processors, you can still start with the same steps above to create a new processor with the Indexer SDK.

You'll also need to follow these:

1. Copy your migration files to `src/db/`.
2. With the legacy processors, the processing logic is defined inside the `process_transactions` method.

```rust
// Example with the legacy processors
#[async_trait]
impl ProcessorTrait for EventsProcessor {
    async fn process_transactions(
        ...
    ) -> anyhow::Result<ProcessingResult> {
        // Extract events from transactions
        let events: Vec<EventModel> = process_events(transactions);

        // Store the events in the database
        let tx_result = insert_to_db(
            self.get_pool(),
            self.name(),
            start_version,
            end_version,
            &events,
            &self.per_table_chunk_sizes,
        )
        .await;

        return tx_result;
    }
}
```

Migrate to the SDK by copying over the logic in `process_transactions` method to the SDK `process` transform function.

```rust
// Example with SDK processor
    process(
        "events_processor".to_string(),
        MIGRATIONS,
        async |transactions, conn_pool| {
          // Extract events from transactions
          let events: Vec<EventModel> = process_events(transactions);

          // Store events in the database
          let execute_res = execute_in_chunks(
              conn_pool.clone(),
              insert_events_query,
              &events,
              MAX_DIESEL_PARAM_SIZE / EventModel::field_count(),
          )
          .await;
        },
    )
    .await?;
```

3. Update the `config.yaml` file to the new format. Update `starting_version` to the version that is last saved in the `processor_status` table.

# Legacy Indexer

> Deprecated indexer system for Aptos blockchain - migration guide to new Transaction Stream Service and Indexer SDK

import { Aside } from '@astrojs/starlight/components';

<Aside type="caution">
  Deprecation Alert

  From Now - end of Q2, 2024: We will not be adding any new features to the legacy Indexer. However, we will continue to generally support the community, and will make sure that any changes made on the blockchain level does not break the existing legacy processors.

  After Q2, 2024: We will remove the indexer crates from the [aptos-core](https://github.com/aptos-labs/aptos-core) repo and the legacy indexer will no longer be supported. Please look at our new [Transaction Stream Service](/build/indexer/txn-stream) and updated [Indexer API](/build/indexer)
</Aside>

# Custom Data Model

> Legacy custom data model documentation for deprecated indexer - migrate to modern Indexer SDK for custom processors

import { Aside } from '@astrojs/starlight/components';

<Aside type="caution">
  This is documentation for the legacy indexer. To learn how to write a custom processor with the latest indexer stack, see [Custom Processors](/build/indexer/indexer-sdk/documentation/create-processor).
</Aside>

## Define your own data model

Use this method if you want to develop your custom indexer for the Aptos ledger data.

<Aside type="note">
  When should you use the custom indexer?

  Currently Aptos-provided indexing service (see above) supports the following core Move modules:

  - `0x1::coin`.
  - `0x3::token`.
  - `0x3::token_transfers`.

  If you need an indexed database for any other Move modules and contracts, then you should develop your custom indexer.
</Aside>

Creating a custom indexer involves the following steps. Refer to the indexing block diagram at the start of this document.

1. Define new table schemas, using an ORM like [Diesel](https://diesel.rs/). In this document Diesel is used to describe the custom indexing steps ("Business logic" and the data queries in the diagram).
2. Create new data models based on the new tables ("Business logic" in the diagram).
3. Create a new transaction processor, or optionally add to an existing processor. In the diagram this step corresponds to processing the ledger database according to the new business logic and writing to the indexed database.
4. Integrate the new processor. Optional if you are reusing an existing processor.

In the below detailed description, an example of indexing and querying for the coin balances is used. You can see this in the [`coin_processor.rs`](https://github.com/aptos-labs/aptos-core/blob/main/crates/indexer/src/processors/coin_processor.rs).

### 1. Define new table schemas

In this example we use [PostgreSQL](https://www.postgresql.org/) and [Diesel](https://diesel.rs/) as the ORM. To make sure that we make backward-compatible changes without having to reset the database at every upgrade, we use [Diesel migrations](https://docs.rs/diesel_migrations/latest/diesel_migrations/) to manage the schema. This is why it is very important to start with generating a new Diesel migration before doing anything else.

Make sure you clone the Aptos-core repo by running `git clone https://github.com/aptos-labs/aptos-core.git` and then `cd` into `aptos-core/tree/main/crates/indexer` directory. Then proceed as below.

a. The first step is to create a new Diesel migration. This will generate a new folder under [migrations](https://github.com/aptos-labs/aptos-core/tree/main/crates/indexer/migrations) with `up.sql` and `down.sql`

```shellscript filename="Terminal"
DATABASE_URL=postgres://postgres@localhost:5432/postgres diesel migration generate add_coin_tables
```

b. Create the necessary table schemas. This is just PostgreSQL code. In the code shown below, the `up.sql` will have the new changes and `down.sql` will revert those changes.

```sql filename="up.sql / down.sql"
-- up.sql
-- coin balances for each version
CREATE TABLE coin_balances (
  transaction_version BIGINT NOT NULL,
  owner_address VARCHAR(66) NOT NULL,
  -- Hash of the non-truncated coin type
  coin_type_hash VARCHAR(64) NOT NULL,
  -- creator_address::name::symbol<struct>
  coin_type VARCHAR(5000) NOT NULL,
  amount NUMERIC NOT NULL,
  transaction_timestamp TIMESTAMP NOT NULL,
  inserted_at TIMESTAMP NOT NULL DEFAULT NOW(),
  -- Constraints
  PRIMARY KEY (
    transaction_version,
    owner_address,
    coin_type_hash
  )
);
-- latest coin balances
CREATE TABLE current_coin_balances {...}
-- down.sql
DROP TABLE IF EXISTS coin_balances;
DROP TABLE IF EXISTS current_coin_balances;
```

See the [full source for `up.sql` and `down.sql`](https://github.com/aptos-labs/aptos-core/tree/main/crates/indexer/migrations/2022-10-04-073529_add_coin_tables).

c. Run the migration. We suggest running it multiple times with `redo` to ensure that both `up.sql` and `down.sql` are implemented correctly. This will also modify the [`schema.rs`](https://github.com/aptos-labs/aptos-core/blob/main/crates/indexer/src/schema.rs) file.

```shellscript filename="Terminal"
DATABASE_URL=postgres://postgres@localhost:5432/postgres diesel migration run
DATABASE_URL=postgres://postgres@localhost:5432/postgres diesel migration redo
```

### 2. Create new data schemas

We now have to prepare the Rust data models that correspond to the Diesel schemas. In the case of coin balances, we will define `CoinBalance` and `CurrentCoinBalance` as below:

```rust filename="coin_balance.rs"
#[derive(Debug, Deserialize, FieldCount, Identifiable, Insertable, Serialize)]
#[diesel(primary_key(transaction_version, owner_address, coin_type))]
#[diesel(table_name = coin_balances)]
pub struct CoinBalance {
    pub transaction_version: i64,
    pub owner_address: String,
    pub coin_type_hash: String,
    pub coin_type: String,
    pub amount: BigDecimal,
    pub transaction_timestamp: chrono::NaiveDateTime,
}

#[derive(Debug, Deserialize, FieldCount, Identifiable, Insertable, Serialize)]
#[diesel(primary_key(owner_address, coin_type))]
#[diesel(table_name = current_coin_balances)]
pub struct CurrentCoinBalance {
    pub owner_address: String,
    pub coin_type_hash: String,
    pub coin_type: String,
    pub amount: BigDecimal,
    pub last_transaction_version: i64,
    pub last_transaction_timestamp: chrono::NaiveDateTime,
}
```

We will also need to specify the parsing logic, where the input is a portion of the transaction. In the case of coin balances, we can find all the details in `WriteSetChanges`, specifically where the write set change type is `write_resources`.

**Where to find the relevant data for parsing**: This requires a combination of understanding the Move module and the structure of the transaction. In the example of coin balance, the contract lives in [coin.move](https://github.com/aptos-labs/aptos-core/blob/main/aptos-move/framework/aptos-framework/sources/coin.move), specifically the coin struct (search for `struct Coin`) that has a `value` field. We then look at an [example transaction](https://api.testnet.aptoslabs.com/v1/transactions/by_version/259518) where we find this exact structure in `write_resources`:

```shellscript filename="Terminal"
"changes": [
  {
    ...
    "data": {
      "type": "0x1::coin::CoinStore<0x1::aptos_coin::AptosCoin>",
      "data": {
        "coin": {
          "value": "49742"
      },
      ...
```

See the full code in [coin\_balances.rs](https://github.com/aptos-labs/aptos-core/blob/main/crates/indexer/src/models/coin_models/coin_balances.rs).

### 3. Create a new processor

Now that we have the data model and the parsing function, we need to call that parsing function and save the resulting model in our Postgres database. We do this by creating (or modifying) a `processor`. We have abstracted a lot already from that class, so the only function that should be implemented is `process_transactions` (there are a few more functions that should be copied, those should be obvious from the example).

The `process_transactions` function takes in a vector of transactions with a start and end version that are used for tracking purposes. The general flow should be:

- Loop through transactions in the vector.
- Aggregate relevant models. Sometimes deduping is required, e.g. in the case of `CurrentCoinBalance`.
- Insert the models into the database in a single Diesel transaction. This is important, to ensure that we do not have partial writes.
- Return status (error or success).

<Aside type="note">
  See [coin\_processor.rs](https://github.com/aptos-labs/aptos-core/blob/main/crates/indexer/src/processors/coin_processor.rs) for a relatively straightforward example. You can search for `coin_balances` in the page for the specific code snippet related to coin balances.
</Aside>

**How to decide whether to create a new processor:** This is completely up to you. The benefit of creating a new processor is that you are starting from scratch, so you will have full control over exactly what gets written to the indexed database. The downside is that you will have to maintain a new fullnode, since there is a 1-to-1 mapping between a fullnode and the processor.

### 4. Integrate the new processor

This is the easiest step and involves just a few additions.

1. To start with, make sure to add the new processor in the Rust code files: [`mod.rs`](https://github.com/aptos-labs/aptos-core/blob/main/crates/indexer/src/processors/mod.rs) and [`runtime.rs`](https://github.com/aptos-labs/aptos-core/blob/main/crates/indexer/src/runtime.rs). See below:

[**mod.rs**](https://github.com/aptos-labs/aptos-core/blob/main/crates/indexer/src/processors/mod.rs)

```rust filename="mod.rs"
pub enum Processor {
  CoinProcessor,
  ...
}
...
  COIN_PROCESSOR_NAME => Self::CoinProcessor,
```

[**runtime.rs**](https://github.com/aptos-labs/aptos-core/blob/main/crates/indexer/src/runtime.rs)

```rust filename="runtime.rs"
Processor::CoinProcessor => Arc::new(CoinTransactionProcessor::new(conn_pool.clone())),
```

2. Create a `fullnode.yaml` with the correct configuration and test the custom indexer by starting a fullnode with this `fullnode.yaml`.

**fullnode.yaml**

```yaml filename="fullnode.yaml"
storage:
  enable_indexer: true
  storage_pruner_config:
    ledger_pruner_config:
      enable: false

indexer:
  enabled: true
  check_chain_id: true
  emit_every: 1000
  postgres_uri: "postgres://postgres@localhost:5432/postgres"
  processor: "coin_processor"
  fetch_tasks: 10
  processor_tasks: 10
```

Test by starting an Aptos fullnode by running the below command. You will see many logs in the terminal output, so use the `grep` filter to see only indexer log output, as shown below:

```shellscript filename="Terminal"
cargo run -p aptos-node --features "indexer" --release -- -f ./fullnode_coin.yaml | grep -E "_processor"
```

See the full instructions on how to start an indexer-enabled fullnode in [Indexer Fullnode](/build/indexer/legacy/indexer-fullnode).

# Run an Indexer Fullnode

> Legacy indexer fullnode setup documentation - deprecated in favor of modern Transaction Stream Service architecture

import { Aside } from '@astrojs/starlight/components';

<Aside type="caution">
  This is documentation for the legacy indexer. To learn how to run the underlying infrastructure for the latest indexer stack, see [Transaction Stream Service](/build/indexer/txn-stream).
</Aside>

<Aside type="caution">
  The below installation steps are verified only on macOS with Apple Silicon. They might require minor tweaking when running on other builds.
</Aside>

## Summary

To run an indexer fullnode, these are the steps in summary:

1. Make sure that you have all the required tools and packages described below in this document.
2. Follow the instructions to [set up a public fullnode](/network/nodes/full-node/verify-pfn) but do not start the fullnode yet.
3. Edit the `fullnode.yaml` as described below in this document.
4. Run the indexer fullnode per the instructions below.

## Prerequisites

Install the packages below. Note, you may have already installed many of these while [preparing your development environment](/network/nodes/building-from-source). You can confirm by running `which command-name` and ensuring the package appears in the output (although `libpq` will not be returned even when installed).

> Important: If you are on macOS, you will need to [install Docker following the official guidance](https://docs.docker.com/desktop/install/mac-install/) rather than `brew`.

For an Aptos indexer fullnode, install these packages:

- [`brew`](https://brew.sh/) - `/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"` Run the commands emitted in the output to add the command to your path and install any dependencies
- [`cargo` Rust package manager](https://www.rust-lang.org/tools/install) - `curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh`
- [`docker`](https://docs.docker.com/get-docker/) - `brew install docker`
- [libpq Postgres C API library containing the `pg_ctl` command](https://formulae.brew.sh/formula/libpq) - `brew install libpq`
  Make sure to perform all export commands after the installation.
- [`postgres` PostgreSQL server](https://www.postgresql.org/) - `brew install postgresql`
- [`diesel`](https://diesel.rs/) - `brew install diesel`

## Set up the database

1. Start the PostgreSQL server:
   `brew services start postgresql`
2. Ensure you can run `psql postgres` and then exit the prompt by entering: `\q`
3. Create a PostgreSQL user `postgres` with the `createuser` command (find it with `which`):
   ```shellscript filename="Terminal"
   /path/to/createuser -s postgres
   ```
4. Clone `aptos-core` repository if you have not already:
   ```shellscript filename="Terminal"
   git clone https://github.com/aptos-labs/aptos-core.git
   ```
5. Navigate (or `cd`) into `aptos-core/crates/indexer` directory.
6. Create the database schema:
   ```shellscript filename="Terminal"
   diesel migration run --database-url postgresql://localhost/postgres
   ```
   This will create a database schema with the subdirectory `migrations` located in this `aptos-core/crates/indexer` directory. If for some reason this database is already in use, try a different database. For example: `DATABASE_URL=postgres://postgres@localhost:5432/indexer_v2 diesel database reset`

## Start the fullnode indexer

1. Follow the instructions to set up a [public fullnode](/network/nodes/full-node/verify-pfn) and prepare the setup, but **do not** yet start the indexer (with `cargo run` or `docker run`).
2. Pull the latest indexer Docker image with:
   ```shellscript filename="Terminal"
   docker pull aptoslabs/validator:nightly_indexer
   ```
3. Edit the `./fullnode.yaml` and add the following configuration:

   ```yaml filename="fullnode.yaml"
   storage:
     enable_indexer: true
     # This is to avoid the node being pruned
     storage_pruner_config:
       ledger_pruner_config:
         enable: false

   indexer:
     enabled: true
     postgres_uri: "postgres://postgres@localhost:5432/postgres"
     processor: "default_processor"
     check_chain_id: true
     emit_every: 500
   ```

<Aside type="note">
  Bootstrapping the fullnode

  Instead of syncing your indexer fullnode from genesis, which may take a long period of time, you can choose to bootstrap your fullnode using backup data before starting it. To do so, follow the instructions to [restore from a backup](/network/nodes/bootstrap-fullnode/aptos-db-restore).

  Note: indexers cannot be bootstrapped using [a snapshot](/network/nodes/bootstrap-fullnode) or [fast sync](/network/nodes/configure/state-sync#fast-syncing).
</Aside>

1. Run the indexer fullnode with either `cargo run` or `docker run` depending upon your setup. Remember to supply the arguments you need for your specific node:
   ```shellscript filename="Terminal"
   docker run -p 8080:8080 \
     -p 9101:9101 -p 6180:6180 \
     -v $(pwd):/opt/aptos/etc -v $(pwd)/data:/opt/aptos/data \
     --workdir /opt/aptos/etc \
     --name=aptos-fullnode aptoslabs/validator:nightly_indexer aptos-node \
     -f /opt/aptos/etc/fullnode.yaml
   ```
   or:
   ```shellscript filename="Terminal"
   cargo run -p aptos-node --features "indexer" --release -- -f ./fullnode.yaml
   ```

## Restart the indexer

To restart the PostgreSQL server:

1. [Shut down the server](https://www.postgresql.org/docs/8.1/postmaster-shutdown.html) by searching for the `postmaster` process and killing it:

   ```shellscript filename="Terminal"
   ps -ef | grep -i postmaster
   ```

2. Copy the process ID (PID) for the process and pass it to the following command to shut it down:

   ```shellscript filename="Terminal"
   kill -INT PID
   ```

3. Restart the PostgreSQL server with:
   ```shellscript filename="Terminal"
   brew services restart postgresql@14
   ```

# Migrate to Transaction Stream Service

> Migration guide from legacy indexer to modern Transaction Stream Service with hosted and self-hosted deployment options

This guide contains information on how to migrate to using the Transaction Stream Service if you are currently running a legacy indexer.

The old indexer stack requires running an archival fullnode with additional threads to process the transactions which is difficult and expensive to maintain. Adding more custom logic either requires a bulkier machine, or running several fullnodes that scale linearly.

This new way of indexing uses the [Transaction Stream Service](/build/indexer/txn-stream). You can either use the [Labs-Hosted Transaction Stream Service](/build/indexer/txn-stream/aptos-hosted-txn-stream) or [run your own instance of Transaction Stream Service](/build/indexer/txn-stream/self-hosted).

## 1. Clone the repo

```shellscript filename="Terminal"
# SSH
git clone git@github.com:aptos-labs/aptos-indexer-processors.git

# HTTPS
git clone https://github.com/aptos-labs/aptos-indexer-processors.git
```

Navigate to the directory for the service:

```shellscript filename="Terminal"
cd aptos-indexer-processors
cd rust/processor
```

## 2. Migrate processors to Transaction Stream Service

For each processor you're migrating, you'll need to create a config file using the template below. You can find more information about each field of the config file [here](/build/indexer/txn-stream/self-hosted#configuration).

```yaml filename="config.yaml"
health_check_port: 8084
server_config:
  processor_config:
    type: default_processor
  postgres_connection_string: <postgres_uri, e.g. postgresql://postgres:@localhost:5432/indexer>
  indexer_grpc_data_service_address: <url_from_api_gateway>
  indexer_grpc_http2_ping_interval_in_secs: 60
  indexer_grpc_http2_ping_timeout_in_secs: 10
  auth_token: <auto_token_from_api_gateway>
  starting_version: 0 # optional
  ending_version: 0 # optional
```

To connect the processor to the Transaction Stream Service, you need to set the URL for `indexer_grpc_data_service_address`. Choose one of the following options.

### Option A: Connect to Labs-Hosted Transaction Stream Service

The main benefit of using the Labs-Hosted Transaction Stream Service is that you no longer need to run an archival fullnode to get a stream of transactions. This service is rate-limited. Instructions to connect to Labs-Hosted Transaction Stream can be found [here](/build/indexer/txn-stream/aptos-hosted-txn-stream).

### Option B: Run a Self-Hosted Transaction Stream Service

If you choose to, you can run a self-hosted instance of the Transaction Stream Service and connect your processors to it. Instructions to run a Self-Hosted Transaction Stream can be found [here](/build/indexer/txn-stream/self-hosted).

## 3. (Optional) Migrate custom processors to Transaction Stream Service

If you have custom processors written with the old indexer, we highly recommend starting from scratch with a new database. Using a new database ensures that all your custom database migrations will be applied during this migration.

### a. Migrate custom table schemas

Migrate your custom schemas by copying over each of your custom migrations to the [`migrations`](https://github.com/aptos-labs/aptos-indexer-processors/tree/main/rust/processor/src/db/postgres/migrations) folder.

### b. Migrate custom processors code

Migrate the code by copying over your custom processors to the [`processors`](https://github.com/aptos-labs/aptos-indexer-processors/tree/main/rust/processor) folder and any relevant custom models to the [`models`](https://github.com/aptos-labs/aptos-indexer-processors/tree/main/rust/processor/src/db/common/models) folder. Integrate the custom processors with the rest of the code by adding them to the following Rust code files.

[`mod.rs`](https://github.com/aptos-labs/aptos-indexer-processors/blob/main/rust/processor/src/processors/mod.rs)

```rust filename="mod.rs"
pub enum Processor {
    ...
    CoinProcessor,
    ...
}

impl Processor {
    ...
    COIN_PROCESSOR_NAME => Self::CoinProcessor,
    ...
}
```

[`worker.rs`](https://github.com/aptos-labs/aptos-indexer-processors/blob/main/rust/processor/src/worker.rs)

```rust filename="worker.rs"
Processor::CoinProcessor => {
    Arc::new(CoinTransactionProcessor::new(self.db_pool.clone()))
},
```

## 4. Backfill Postgres database with Diesel

Even though the new processors have the same Postgres schemas as the old ones, we recommend you do a complete backfill (ideally writing to a new DB altogether) because some fields are a bit different as a result of the protobuf conversion.

These instructions assume you are familiar with using [Diesel migrations](https://docs.rs/diesel_migrations/latest/diesel_migrations/). Run the full database migration with the following command:

```shellscript filename="Terminal"
DATABASE_URL=postgres://postgres@localhost:5432/postgres diesel migration run
```

## 5. Run the migrated processors

To run a single processor, use the following command:

```shellscript filename="Terminal"
cargo run --release -- -c config.yaml
```

If you have multiple processors, you'll need to run a separate instance of the service for each of the processors.

{/* TODO: Add instructions for running with Docker */}

If you'd like to run the processor as a Docker image, the instructions are listed here.

## FAQs

### 1. Will the protobuf ever be updated, and what do I need to do at that time?

The protobuf schema may be updated in the future. Backwards incompatible changes will be communicated in release notes.

### 2. What if I already have custom logic written in the old indexer? Is it easy to migrate those?

Since the new indexer stack has the same Postgres schema as the old indexer stack, it should be easy to migrate your processors. We still highly recommend creating a new DB for this migration so that any custom DB migrations are applied.

Follow Step 3 in this guide to migrate your custom logic over to the new processors stack.

# NFT Aggregator API

> Universal NFT aggregator for Aptos ecosystem with normalized marketplace data, GraphQL API, and analytics for all major NFT platforms

import { Aside } from '@astrojs/starlight/components';

We've built a **universal NFT aggregator** for the Aptos ecosystem - normalized activity across all major marketplaces, including **Tradeport**, **Wapal**, **Bluemove**, **Rarible**, and more. We also maintain historical data for deprecated marketplaces like **Topaz**.

At its core, the aggregator captures marketplace events in real-time (like listings, token offers, and collection-wide offers) and converts them into clean, structured data. This allows developers to work with a unified data format — no need to handle different marketplace-specific formats manually.

## What We Provide

- **[GraphQL API](/build/indexer/nft-aggregator/graphql-api)** for querying activities
- **[Analytics REST API](/build/indexer/nft-aggregator/analytics-api)** for aggregated analytics
- Marketplace integration support for new partners

Our API supports two main use cases:

- **Historical Data tracking:** Listings, NFT Token Offers, NFT Collection Offers — in real-time.
- **Analytics & Trends:** Aggregate data, market insights, top traders, and more.

## GraphQL API

Query real-time marketplace activity across all integrated marketplaces. Use this for:

- Basic historical data
- Aggregated data (e.g. how many listings are there for a given collection)

You can explore it by hand by viewing the Hasura Explorer below for the network you are interested in.

- Hasura Console: [https://cloud.hasura.io/public/graphiql?endpoint=https://api.mainnet.aptoslabs.com/nft-aggregator/v1/graphql](https://cloud.hasura.io/public/graphiql?endpoint=https://api.mainnet.aptoslabs.com/nft-aggregator/v1/graphql)

For direct GraphQL queries to the Aptos-Labs hosted Indexer API, use these endpoints:

- Mainnet Graphql Endpoint: [https://api.mainnet.aptoslabs.com/nft-aggregator/v1/graphql](https://api.mainnet.aptoslabs.com/nft-aggregator/v1/graphql)

you can find the full API reference [here](/build/indexer/nft-aggregator/graphql-api).

## Analytics REST API

<Aside type="note">
  Analytics API is currently in beta.\
  For custom analytics pipelines, we recommend using our gRPC stream for raw structured events.
</Aside>

Get high-level insights and historical data on the NFT market. Use this analytics API for:

- Total sales volumes
- Top buyers and sellers
- Marketplace trends

You can find the full API reference [here](/build/indexer/nft-aggregator/analytics-api).

## Integrated Marketplaces

See the full list of marketplaces currently integrated with the NFT Aggregator [here](/build/indexer/nft-aggregator/marketplaces).

## Add Your Marketplace

<Aside type="note">
  We handle most integrations directly with your support
</Aside>

If you'd like your marketplace to be included, please reach out to our team. We support both public integrations and private beta partners.

## Next Steps

Ready to dive deeper?

- 👉 [GraphQL API](/build/indexer/nft-aggregator/graphql-api)
- 👉 [Analytics REST API](/build/indexer/nft-aggregator/analytics-api)
- 👉 [Integrated Marketplaces](/build/indexer/nft-aggregator/marketplaces)

# Analytics REST API

> NFT marketplace analytics and insights via REST API with collection performance, volume data, and top trader metrics for Aptos ecosystem

import { Aside } from '@astrojs/starlight/components';

<Aside type="note">
  The REST API provides collection and marketplace-level insights across the Aptos ecosystem.
</Aside>

Use this API to access:

- 📊 Marketplace performance metrics
- 🥉 Collection-level sales and volume data
- 🏆 Top buyers and sellers
- 📈 Historical trends and leaderboard data

> **Base URL:** `https://api.mainnet.aptoslabs.com/v1/analytics/nft`

***

## Marketplace Endpoints

### **Get Marketplace Total Sales Count**

- **GET** `/nft/marketplace/total_sales_count`
- **Parameters:**
  - `marketplace` _(string, required)_ — Marketplace identifier (e.g. `topaz`, `wapal`)
- **Description:** Returns the total number of completed sales for a given marketplace.
- **Example:**

```shellscript
curl "https://api.mainnet.aptoslabs.com/v1/analytics/nft/marketplace/total_sales_count?marketplace=topaz"
```

***

## Collection Endpoints

### **Get Collection Total Sales**

- **GET** `/nft/collection/total_sales_count`
- **Parameters:**
  - `collection_id` _(string, required)_
- **Description:** Returns total number of completed sales for the specified collection.
- **Example:**

```shellscript
curl "https://api.mainnet.aptoslabs.com/v1/analytics/nft/collection/total_sales_count?collection_id=123"
```

***

### **Get Collection Top Buyers**

- **GET** `/nft/collection/top_buyer`
- **Parameters:**
  - `collection_id` _(string, required)_
  - `limit` _(integer, optional, default: 10)_
  - `offset` _(integer, optional, default: 0)_
- **Description:** Returns top buyers in the collection, ranked by total amount spent.
- **Example:**

```shellscript
curl "https://api.mainnet.aptoslabs.com/v1/analytics/nft/collection/top_buyer?collection_id=123&limit=5"
```

***

### **Get Collection Top Sellers**

- **GET** `/nft/collection/top_seller`
- **Parameters:**
  - `collection_id` _(string, required)_
  - `limit` _(integer, optional, default: 10)_
  - `offset` _(integer, optional, default: 0)_
- **Description:** Returns top sellers in the collection, ranked by total volume sold.
- **Example:**

```shellscript
curl "https://api.mainnet.aptoslabs.com/v1/analytics/nft/collection/top_seller?collection_id=123&limit=5"
```

***

### **Get Collection Total Sales Volume**

- **GET** `/nft/collection/total_sales_volume`
- **Parameters:**
  - `collection_id` _(string, required)_
- **Description:** Returns total trading volume (in APT) for the collection.
- **Example:**

```shellscript
curl "https://api.mainnet.aptoslabs.com/v1/analytics/nft/collection/total_sales_volume?collection_id=123"
```

***

### **List Collections by Trading Volume**

- **GET** `/nft/collection/list_by_volume`
- **Parameters:**
  - `limit` _(integer, max: 10)_
  - `offset` _(integer)_
  - `time_period` _(string)_ — `1h`, `6h`, `24h`, `7d`, `30d`
- **Description:** Returns collections sorted by total sales volume within a selected time period.
- **Example:**

```shellscript
curl "https://api.mainnet.aptoslabs.com/v1/analytics/nft/collection/list_by_volume?limit=10&offset=0&time_period=1d"
```

***

### **List Collections by Number of Sales**

- **GET** `/nft/collection/list_by_sales`
- **Parameters:** Same as above
- **Description:** Returns collections sorted by total number of sales.
- **Example:**

```shellscript
curl "https://api.mainnet.aptoslabs.com/v1/analytics/nft/collection/list_by_sales?limit=10&offset=0&time_period=1d"
```

***

### **List Collections by Floor Price**

- **GET** `/nft/collection/list_by_floor_price`
- **Parameters:** Same as above
- **Description:** Returns collections sorted by floor price within the selected time period.
- **Example:**

```shellscript
curl "https://api.mainnet.aptoslabs.com/v1/analytics/nft/collection/list_by_floor_price?limit=10&offset=0&time_period=1d"
```

***

### **Get Number of Unique Token Holders**

- **GET** `/nft/collection/unique_holders_count`
- **Parameters:**
  - `collection_id` _(string, required)_
- **Description:** Returns the number of unique wallet addresses currently holding at least one token from the specified collection. Only current holders (amount > 0) are counted.
- **Example:**

```shellscript
curl "https://api.mainnet.aptoslabs.com/v1/analytics/nft/collection/unique_holders_count?collection_id=<collection_id>"
```

# GraphQL API

> Real-time NFT marketplace activity data via GraphQL API with queries for listings, offers, and marketplace events across Aptos ecosystem

import { GraphQLEditor } from '~/components/react/GraphQLEditor';

import { Aside } from '@astrojs/starlight/components';

<Aside type="note">
  The GraphQL API provides real-time access to NFT marketplace activity across the Aptos ecosystem.
</Aside>

## Endpoints

- **Hasura Console:** [API Explorer](https://cloud.hasura.io/public/graphiql?endpoint=https://api.mainnet.aptoslabs.com/nft-aggregator/v1/graphql)
- **Mainnet Endpoint:**\
  `https://api.mainnet.aptoslabs.com/nft-aggregator/v1/graphql`

## Schema

The API follows standard GraphQL conventions.

You can explore all available queries and types using the API Explorer linked above.

Some key types:

- `current_nft_marketplace_listings`: Get active listings
- `current_nft_marketplace_token_offers`: See offers made on specific tokens
- `current_nft_marketplace_collection_offers`: Track offers made at the collection level
- `nft_marketplace_activities`: Monitor all marketplace activities

➡️ [Full Schema Reference](/build/indexer/nft-aggregator/nft-aggregator-table)

## Example Queries

### Get Active Listings with Token Metadata

Retrieve **active NFT listings** enriched with token metadata.

<Aside type="note">
  Try it yourself! Adjust filters like `marketplace`, `limit`, and sorting to explore more results.
</Aside>

<GraphQLEditor
  query={`query ActiveListingsWithMetadata {
  current_nft_marketplace_listings(
    limit: 10
    where: { is_deleted: { _eq: false } }
    order_by: { last_transaction_timestamp: desc }
  ) {
    listing_id
    price
    marketplace
    seller
    last_transaction_version
    last_transaction_timestamp
    token_data_id
    current_token_data {
      token_name
      token_uri
      description
      token_properties
      token_standard
      supply
      maximum
      collection_id
    }
  }
}`}
  endpoint="https://api.mainnet.aptoslabs.com/nft-aggregator-staging/v1/graphql"
/>

_No variables are required for this query by default, but you can adjust the `where` filter directly inside the editor._

***

### Get Collection Offers with Collection Metadata

Retrieve **active collection-level offers** along with detailed collection metadata.

<Aside type="note">
  Try it yourself! Adjust marketplace filter and pagination to explore more offers.
</Aside>

<GraphQLEditor
  query={`query CollectionOffersWithMetadata($marketplace: String!) {
  current_nft_marketplace_collection_offers(
    where: { is_deleted: { _eq: false }, marketplace: { _eq: $marketplace } }
    limit: 2
    order_by: { last_transaction_timestamp: desc }
  ) {
    collection_offer_id
    collection_id
    price
    marketplace
    buyer
    last_transaction_timestamp
    token_data_id
    current_collection {
      collection_id
      collection_name
      creator_address
      current_supply
      max_supply
      description
    }
  }
}`}
  variables={`{
  "marketplace": "wapal"
}`}
  endpoint="https://api.mainnet.aptoslabs.com/nft-aggregator-staging/v1/graphql"
/>

### Get Token Offers with Token Metadata

Retrieve **active token-specific offers** along with detailed token metadata.

<Aside type="note">
  Try it yourself! Adjust the token\_data\_id and other filters to explore specific token offers.
</Aside>

<GraphQLEditor
  query={`query TokenOffersWithMetadata($token_data_id: String!) {
  current_nft_marketplace_token_offers(
    where: {
      is_deleted: { _eq: false }
      token_data_id: { _eq: $token_data_id }
    }
    limit: 5
    order_by: { last_transaction_timestamp: desc }
  ) {
    offer_id
    token_data_id
    price
    marketplace
    buyer
    last_transaction_timestamp
    current_token_datas {
      token_name
      token_uri
      description
      collection_id
    }
  }
}`}
  variables={`{
  "token_data_id": "0x8142a7fde5039839509e81615e456413bac1c53a1d6c6447f2daf64e84665948"
}`}
  endpoint="https://api.mainnet.aptoslabs.com/nft-aggregator-staging/v1/graphql"
/>

# Integrated Marketplaces

> Complete list of NFT marketplaces integrated with Aptos NFT Aggregator including event mappings and transaction examples

import { Aside, CardGrid, LinkCard } from '@astrojs/starlight/components';

<Aside type="note">
  Explore all marketplaces currently supported by the NFT Aggregator API.\
  For each marketplace, we provide detailed event type mappings, example transactions, and integration notes.
</Aside>

<CardGrid>
  <LinkCard href="/build/indexer/nft-aggregator/marketplaces/tradeport" title="Tradeport" description="View supported events and example transactions for Tradeport" />

  <LinkCard href="/build/indexer/nft-aggregator/marketplaces/bluemove" title="Bluemove" description="View supported events and example transactions for Bluemove" />

  <LinkCard href="/build/indexer/nft-aggregator/marketplaces/wapal" title="Wapal" description="View supported events and example transactions for Wapal" />

  <LinkCard href="/build/indexer/nft-aggregator/marketplaces/rarible" title="Rarible" description="View supported events and example transactions for Rarible" />
</CardGrid>

## Deprecated Marketplaces

These marketplaces are no longer operational, but their historical data remains available through our API.

<CardGrid>
  <LinkCard href="/build/indexer/nft-aggregator/marketplaces/topaz" title="Topaz (Deprecated)" description="Historical data available for reference" />
</CardGrid>

## Notes

- ✅ All marketplaces share the same core event schema for consistency.
- 📖 Use individual marketplace pages for detailed event-to-type mappings and example transactions.
- 🚀 Add your marketplace: Reach out to our team!

# Bluemove

> Bluemove marketplace integration with supported NFT events, transaction examples, and aggregator API compatibility for Aptos ecosystem

import { Aside } from '@astrojs/starlight/components';

<Aside type="note">
  This page details all supported event types and example transactions for the Bluemove marketplace.
</Aside>

## Contract Address

| Contract Version | Account Address                                                                                                                                                                                                       |
| ---------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Mainnet          | [`0xd1fd99c1944b84d1670a2536417e997864ad12303d19eac725891691b04d614e`](https://explorer.aptoslabs.com/account/0xd1fd99c1944b84d1670a2536417e997864ad12303d19eac725891691b04d614e/modules/code/events?network=mainnet) |

_This address is the on-chain account for Bluemove's contract deployment._

***

## Supported Event Types

| Standard Event Type          | Raw On-Chain Event Type (entry function) | Example Txn Version                                                         |
| ---------------------------- | ---------------------------------------- | --------------------------------------------------------------------------- |
| **Offers**                   |                                          |                                                                             |
| `token_offer_created`        | `offer_lib::OfferEvent`                  | [2299166354](https://explorer.aptoslabs.com/txn/2299166354?network=mainnet) |
| `token_offer_cancelled`      | `offer_lib::CancelOfferEvent`            | [2299053567](https://explorer.aptoslabs.com/txn/2299053567?network=mainnet) |
| `token_offer_filled`         | `offer_lib::AcceptOfferEvent`            | [1809917526](https://explorer.aptoslabs.com/txn/1809917526?network=mainnet) |
| **Collection Offers**        |                                          |                                                                             |
| `collection_offer_created`   | `offer_lib::OfferCollectionEvent`        | [2403214712](https://explorer.aptoslabs.com/txn/2403214712?network=mainnet) |
| `collection_offer_cancelled` | `offer_lib::CancelOfferCollectionEvent`  | [2397017908](https://explorer.aptoslabs.com/txn/2397017908?network=mainnet) |
| `collection_offer_filled`    | `offer_lib::AcceptOfferCollectionEvent`  | [2382717056](https://explorer.aptoslabs.com/txn/2382717056?network=mainnet) |
| **Listings**                 |                                          |                                                                             |
| `listing_created`            | `marketplaceV2::ListEvent`               | [2404863839](https://explorer.aptoslabs.com/txn/2404863839?network=mainnet) |
| `listing_cancelled`          | `marketplaceV2::DeListEvent`             | [2399933805](https://explorer.aptoslabs.com/txn/2399933805?network=mainnet) |
| `listing_filled`             | `listings_v2::BuyEvent`                  | [2396025485](https://explorer.aptoslabs.com/txn/2396025485?network=mainnet) |

***

## Related Docs

- 👉 [NFT Aggregator Table Reference](/build/indexer/nft-aggregator/nft-aggregator-table)
- 👉 [GraphQL API: Real-time Activity](/build/indexer/nft-aggregator/graphql-api)
- 👉 [Integrated Marketplaces Overview](/build/indexer/nft-aggregator/marketplaces)

# Rarible

> Rarible marketplace integration with NFT event types, transaction examples, and compatibility with Aptos aggregator data pipeline

import { Aside } from '@astrojs/starlight/components';

<Aside type="note">
  This page details all supported event types and example transactions for the Rarible marketplace.
</Aside>

## Contract Address

| Contract Version | Account Address                                                                                                                                                                                                       |
| ---------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Mainnet          | [`0x465a0051e8535859d4794f0af24dbf35c5349bedadab26404b20b825035ee790`](https://explorer.aptoslabs.com/account/0x465a0051e8535859d4794f0af24dbf35c5349bedadab26404b20b825035ee790/modules/code/events?network=mainnet) |

_This address is the on-chain account for Rarible's contract deployment._

***

## Supported Event Types

| Standard Event Type          | Raw On-Chain Event Type (entry function) | Example Txn Version                                                         |
| ---------------------------- | ---------------------------------------- | --------------------------------------------------------------------------- |
| **Offers**                   |                                          |                                                                             |
| `token_offer_created`        | `events::TokenOfferPlaced`               | [1647787967](https://explorer.aptoslabs.com/txn/1647787967?network=mainnet) |
| `token_offer_cancelled`      | `events::TokenOfferCancelled`            | _No example provided (optional)_                                            |
| `token_offer_filled`         | `events::TokenOfferFilled`               | [1647790684](https://explorer.aptoslabs.com/txn/1647790684?network=mainnet) |
| **Collection Offers**        |                                          |                                                                             |
| `collection_offer_created`   | `events::CollectionOfferPlaced`          | [2202390104](https://explorer.aptoslabs.com/txn/2202390104?network=mainnet) |
| `collection_offer_cancelled` | `events::CollectionOfferCanceled`        | _No example provided (optional)_                                            |
| `collection_offer_filled`    | `events::CollectionOfferFilled`          | [2205653354](https://explorer.aptoslabs.com/txn/2205653354?network=mainnet) |
| **Listings**                 |                                          |                                                                             |
| `listing_created`            | `events::ListingPlaced`                  | [2417694028](https://explorer.aptoslabs.com/txn/2417694028?network=mainnet) |
| `listing_cancelled`          | `events::ListingCanceled`                | [2403151598](https://explorer.aptoslabs.com/txn/2403151598?network=mainnet) |
| `listing_filled`             | `events::ListingFilled`                  | [2395762995](https://explorer.aptoslabs.com/txn/2395762995?network=mainnet) |

***

## Related Docs

- 👉 [NFT Aggregator Table Reference](/build/indexer/nft-aggregator/nft-aggregator-table)
- 👉 [GraphQL API: Real-time Activity](/build/indexer/nft-aggregator/graphql-api)
- 👉 [Integrated Marketplaces Overview](/build/indexer/nft-aggregator/marketplaces)

# Topaz (Deprecated)

> Historical Topaz marketplace data integration - deprecated marketplace with legacy NFT transaction support in aggregator

import { Aside } from '@astrojs/starlight/components';

<Aside type="caution">
  **Marketplace Deprecated**\
  Topaz is no longer operational as a marketplace. However, we continue to include its historical data in our NFT Aggregator for reference.
</Aside>

## Contract Address

| Contract Version | Account Address                                                                                                                                                                                                       |
| ---------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Mainnet          | [`0x2c7bccf7b31baf770fdbcc768d9e9cb3d87805e255355df5db32ac9a669010a2`](https://explorer.aptoslabs.com/account/0x2c7bccf7b31baf770fdbcc768d9e9cb3d87805e255355df5db32ac9a669010a2/modules/code/events?network=mainnet) |

_This address is the on-chain account for Topaz's contract deployment._

***

## Supported Event Types

| Standard Event Type          | Raw On-Chain Event Type (entry function) | Example Txn Version                                                         |
| ---------------------------- | ---------------------------------------- | --------------------------------------------------------------------------- |
| **Offers**                   |                                          |                                                                             |
| `token_offer_created`        | `events::BidEvent`                       | [1645629583](https://explorer.aptoslabs.com/txn/1645629583?network=mainnet) |
| `token_offer_cancelled`      | `events::CancelBidEvent`                 | [86119627](https://explorer.aptoslabs.com/txn/86119627?network=mainnet)     |
| `token_offer_filled`         | `events::SellEvent`                      | [984827420](https://explorer.aptoslabs.com/txn/984827420?network=mainnet)   |
| **Collection Offers**        |                                          |                                                                             |
| `collection_offer_created`   | `events::CollectionBidEvent`             | [85566357](https://explorer.aptoslabs.com/txn/85566357?network=mainnet)     |
| `collection_offer_cancelled` | `events::CancelCollectionBidEvent`       | [2787969](https://explorer.aptoslabs.com/txn/2787969?network=mainnet)       |
| `collection_offer_filled`    | `events::FillCollectionBidEvent`         | [2367804069](https://explorer.aptoslabs.com/txn/2367804069?network=mainnet) |
| **Listings**                 |                                          |                                                                             |
| `listing_created`            | `events::ListEvent`                      | [1964348978](https://explorer.aptoslabs.com/txn/1964348978?network=mainnet) |
| `listing_cancelled`          | `events::DelistEvent`                    | [2331658551](https://explorer.aptoslabs.com/txn/2331658551?network=mainnet) |
| `listing_filled`             | `events::BuyEvent`                       | [2379182335](https://explorer.aptoslabs.com/txn/2379182335?network=mainnet) |

***

## Historical Data Access

While Topaz is no longer operational, all historical marketplace events remain accessible through the NFT Aggregator API. You can filter specifically for Topaz marketplace activity using the `marketplace` field:

```graphql
query GetTopazHistoricalActivity {
  nft_marketplace_activities(
    where: {marketplace: {_eq: "topaz"}}
  ) {
    txn_version
    standard_event_type
    token_data_id
    }
}
```

***

## Related Docs

- 👉 [NFT Aggregator Table Reference](/build/indexer/nft-aggregator/nft-aggregator-table)
- 👉 [GraphQL API: Real-time Activity](/build/indexer/nft-aggregator/graphql-api)
- 👉 [Integrated Marketplaces Overview](/build/indexer/nft-aggregator/marketplaces)

# Tradeport

> Tradeport marketplace integration details including V1 and V2 contract event types, transaction examples, and NFT aggregator support

import { Aside } from '@astrojs/starlight/components';

<Aside type="note">
  This page details all supported event types and example transactions for the Tradeport marketplace, across both V1 and V2 contracts.
</Aside>

## Contract Address

| Contract Version | Account Address                                                                                                                                                                                                            |
| ---------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Mainnet          | [`0xe11c12ec495f3989c35e1c6a0af414451223305b579291fc8f3d9d0575a23c26`](https://explorer.aptoslabs.com/account/0xe11c12ec495f3989c35e1c6a0af414451223305b579291fc8f3d9d0575a23c26/modules/code/bluemove_v2?network=mainnet) |

_This address is the on-chain account for Tradeport's contract deployment, used across both V1 and V2 versions._

<Aside type="note">
  **Important:**\
  While most marketplaces aim for a single contract supporting both v1 and v2 tokens, Tradeport is an exception.

  - The **V1 contract exclusively handles Aptos v1 tokens**.
  - The **V2 contract exclusively handles Aptos v2 tokens**.
  - Both contracts share the same account but operate independently.

  When building queries, make sure to align your token standard with the correct contract version for accurate results.
</Aside>

***

## Supported Event Types

### V1 Contract

| Standard Event Type          | Raw On-Chain Event Type (i.e. entry function) | Example Txn Version                                                         |
| ---------------------------- | --------------------------------------------- | --------------------------------------------------------------------------- |
| **Offers**                   |                                               |                                                                             |
| `token_offer_created`        | `biddings::InsertTokenBidEvent`               | [2377828353](https://explorer.aptoslabs.com/txn/2377828353?network=mainnet) |
| `token_offer_cancelled`      | `biddings::DeleteTokenBidEvent`               | [2361990811](https://explorer.aptoslabs.com/txn/2361990811?network=mainnet) |
| `token_offer_filled`         | `biddings::AcceptTokenBidEvent`               | [2332332877](https://explorer.aptoslabs.com/txn/2332332877?network=mainnet) |
| **Collection Offers**        |                                               |                                                                             |
| `collection_offer_created`   | `biddings::InsertCollectionBidEvent`          | [2386877471](https://explorer.aptoslabs.com/txn/2386877471?network=mainnet) |
| `collection_offer_cancelled` | `biddings::DeleteCollectionBidEvent`          | [2386876427](https://explorer.aptoslabs.com/txn/2386876427?network=mainnet) |
| `collection_offer_filled`    | `biddings::AcceptCollectionBidEvent`          | [2386481933](https://explorer.aptoslabs.com/txn/2386481933?network=mainnet) |
| **Listings**                 |                                               |                                                                             |
| `listing_created`            | `listings::InsertListingEvent`                | [2386786871](https://explorer.aptoslabs.com/txn/2386786871?network=mainnet) |
| `listing_cancelled`          | `listings::DeleteListingEvent`                | [2386786127](https://explorer.aptoslabs.com/txn/2386786127?network=mainnet) |
| `listing_filled`             | `listings::BuyEvent`                          | [2386133110](https://explorer.aptoslabs.com/txn/2386133110?network=mainnet) |

***

### V2 Contract

| Standard Event Type          | Raw On-Chain Event Type (i.e. entry function) | Example Txn Version                                                         |
| ---------------------------- | --------------------------------------------- | --------------------------------------------------------------------------- |
| **Offers**                   |                                               |                                                                             |
| `token_offer_created`        | `biddings_v2::InsertTokenBidEvent`            | [2386133936](https://explorer.aptoslabs.com/txn/2386133936?network=mainnet) |
| `token_offer_cancelled`      | `biddings_v2::DeleteTokenBidEvent`            | [2386142672](https://explorer.aptoslabs.com/txn/2386142672?network=mainnet) |
| `token_offer_filled`         | `biddings_v2::AcceptTokenBidEvent`            | [2298838662](https://explorer.aptoslabs.com/txn/2298838662?network=mainnet) |
| **Collection Offers**        |                                               |                                                                             |
| `collection_offer_created`   | `biddings_v2::InsertCollectionBidEvent`       | [2386891051](https://explorer.aptoslabs.com/txn/2386891051?network=mainnet) |
| `collection_offer_cancelled` | `biddings_v2::DeleteCollectionBidEvent`       | [2386889884](https://explorer.aptoslabs.com/txn/2386889884?network=mainnet) |
| `collection_offer_filled`    | `biddings_v2::AcceptCollectionBidEvent`       | [2386021136](https://explorer.aptoslabs.com/txn/2386021136?network=mainnet) |
| **Listings**                 |                                               |                                                                             |
| `listing_created`            | `listings_v2::InsertListingEvent`             | [2386809975](https://explorer.aptoslabs.com/txn/2386809975?network=mainnet) |
| `listing_cancelled`          | `listings_v2::DeleteListingEvent`             | [2386716658](https://explorer.aptoslabs.com/txn/2386716658?network=mainnet) |
| `listing_filled`             | `listings_v2::BuyEvent`                       | [2386455218](https://explorer.aptoslabs.com/txn/2386455218?network=mainnet) |

***

## Related Docs

- 👉 [NFT Aggregator Table Reference](/build/indexer/nft-aggregator/nft-aggregator-table)
- 👉 [GraphQL API: Real-time Activity](/build/indexer/nft-aggregator/graphql-api)
- 👉 [Integrated Marketplaces Overview](/build/indexer/nft-aggregator/marketplaces)

# Wapal

> Wapal marketplace integration details with event type mappings, transaction examples, and NFT aggregator data structure support

import { Aside } from '@astrojs/starlight/components';

<Aside type="note">
  This page details all supported event types and example transactions for the Wapal marketplace.
</Aside>

## Contract Address

| Contract Version | Account Address                                                                                                                                                                                                       |
| ---------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Mainnet          | [`0x584b50b999c78ade62f8359c91b5165ff390338d45f8e55969a04e65d76258c9`](https://explorer.aptoslabs.com/account/0x584b50b999c78ade62f8359c91b5165ff390338d45f8e55969a04e65d76258c9/modules/code/events?network=mainnet) |

_This address is the on-chain account for Wapal's contract deployment._

***

## Supported Event Types

| Standard Event Type          | Raw On-Chain Event Type (entry function) | Example Txn Version                                                         |
| ---------------------------- | ---------------------------------------- | --------------------------------------------------------------------------- |
| **Offers**                   |                                          |                                                                             |
| `token_offer_created`        | `TokenOfferPlacedEvent`                  | [2382313982](https://explorer.aptoslabs.com/txn/2382313982?network=mainnet) |
| `token_offer_cancelled`      | `TokenOfferCanceledEvent`                | [2381810159](https://explorer.aptoslabs.com/txn/2381810159?network=mainnet) |
| `token_offer_filled`         | `TokenOfferFilledEvent`                  | [2313248448](https://explorer.aptoslabs.com/txn/2313248448?network=mainnet) |
| **Collection Offers**        |                                          |                                                                             |
| `collection_offer_created`   | `CollectionOfferPlacedEvent`             | [2382373209](https://explorer.aptoslabs.com/txn/2382373209?network=mainnet) |
| `collection_offer_cancelled` | `CollectionOfferCanceledEvent`           | [2382373978](https://explorer.aptoslabs.com/txn/2382373978?network=mainnet) |
| `collection_offer_filled`    | `CollectionOfferFilledEvent`             | [2382219668](https://explorer.aptoslabs.com/txn/2382219668?network=mainnet) |
| **Listings**                 |                                          |                                                                             |
| `listing_created`            | `ListingPlacedEvent`                     | [2382251863](https://explorer.aptoslabs.com/txn/2382251863?network=mainnet) |
| `listing_cancelled`          | `ListingCanceledEvent`                   | [2381742315](https://explorer.aptoslabs.com/txn/2381742315?network=mainnet) |
| `listing_filled`             | `ListingFilledEvent`                     | [2382221134](https://explorer.aptoslabs.com/txn/2382221134?network=mainnet) |

***

## Related Docs

- 👉 [NFT Aggregator Table Reference](/build/indexer/nft-aggregator/nft-aggregator-table)
- 👉 [GraphQL API: Real-time Activity](/build/indexer/nft-aggregator/graphql-api)
- 👉 [Integrated Marketplaces Overview](/build/indexer/nft-aggregator/marketplaces)

# NFT Aggregator Table Reference

> Database schema reference for NFT aggregator PostgreSQL tables with marketplace activity data structure and field documentation

import { Aside } from '@astrojs/starlight/components';

This page documents the PostgreSQL tables generated and updated by the NFT Aggregator.\
These tables power both the **GraphQL API** and **REST API**, and reflect the live state of marketplace activity on Aptos.

For querying, refer to:

- **[NFT Aggregator API GraphQL](/build/indexer/nft-aggregator/graphql-api)**
- **[NFT Aggregator REST API](/build/indexer/nft-aggregator/analytics-api)**

<Aside type="note">
  When exploring the GraphQL API, you can view these tables in the schema explorer. Tables with `_by_pk` suffixes are automatically generated for primary key lookups.
</Aside>

<br />

# NFT Aggregator Table Overview

| Table Name                                 | Description                                   |
| ------------------------------------------ | --------------------------------------------- |
| `nft_marketplace_activities`               | Historical data of all NFT marketplace events |
| `current_nft_marketplace_listing`          | Latest active listings per token              |
| `current_nft_marketplace_token_offer`      | Latest active offers per token and buyer      |
| `current_nft_marketplace_collection_offer` | Latest active offers per collection           |
| `current_collections_v2`                   | Latest active collections                     |
| `current_token_datas_v2`                   | Latest active tokens                          |
| `current_token_ownerships_v2`              | Latest active token ownerships                |
| `current_collection_ownerships_v2_view`    | Latest active collection ownerships           |

## Notes

- Use `is_deleted = false` to query **only active** records in current state tables.
- The `nft_marketplace_activities` table is your **source of truth** for historical marketplace activity.

## `nft_marketplace_activities`

Historical table capturing all NFT marketplace events — listings, offers, sales, and more. Has an aggregate view for summary data called `nft_marketplace_activities_aggregate`.

**Primary Key:** `txn_version, index, marketplace`

### Indexes

| Index Name                | Columns                                                      |
| ------------------------- | ------------------------------------------------------------ |
| `idx_collection_event_ts` | collection\_id, standard\_event\_type, block\_timestamp DESC |
| `idx_token_id`            | token\_data\_id                                              |
| `idx_buyer`               | buyer                                                        |
| `idx_seller`              | seller                                                       |
| `idx_listing_id`          | listing\_id                                                  |
| `idx_offer_id`            | offer\_id                                                    |
| `idx_timestamp`           | block\_timestamp DESC                                        |

### Fields

<Aside type="note">
  Many fields use `Option` types because marketplace events may not emit complete data for all fields. The processor captures what's available while maintaining type safety.
</Aside>

| Field                 | Type               | Description                             |
| --------------------- | ------------------ | --------------------------------------- |
| txn\_version          | i64                | Blockchain version of the transaction   |
| index                 | i64                | Event index in the transaction          |
| listing\_id           | Option\<String>    | Listing ID (if applicable)              |
| offer\_id             | Option\<String>    | Offer ID (if applicable)                |
| raw\_event\_type      | String             | Raw marketplace event type              |
| standard\_event\_type | String             | Normalized event type                   |
| creator\_address      | Option\<String>    | Collection creator address              |
| collection\_id        | Option\<String>    | Collection identifier                   |
| collection\_name      | Option\<String>    | Collection name                         |
| token\_data\_id       | Option\<String>    | Token identifier                        |
| token\_name           | Option\<String>    | Token name                              |
| price                 | i64                | Price in Octas                          |
| token\_amount         | Option\<i64>       | Token amount (for bundles etc.)         |
| buyer                 | Option\<String>    | Buyer's address                         |
| seller                | Option\<String>    | Seller's address                        |
| expiration\_time      | Option\<String>    | Listing/offer expiration time           |
| marketplace           | String             | Marketplace name                        |
| contract\_address     | String             | Contract address of the marketplace     |
| json\_data            | serde\_json::Value | Internal raw event payload (not public) |
| block\_timestamp      | NaiveDateTime      | Block timestamp of the event            |

<Aside type="caution">
  `json_data` is internal and not exposed in public APIs.
</Aside>

## `current_nft_marketplace_listing`

Tracks current active listings. Updated in real-time.

**Primary Key:** `token_data_id, marketplace`

### Indexes

| Index Name                                                 | Columns               |
| ---------------------------------------------------------- | --------------------- |
| `idx_current_nft_marketplace_listings_token_data_id`       | token\_data\_id       |
| `idx_current_nft_marketplace_listings_collection_id`       | collection\_id        |
| `idx_current_nft_marketplace_listings_collection_id_price` | collection\_id, price |
| `idx_current_nft_marketplace_listings_seller`              | seller                |

### Fields

| Field                        | Type            | Description                     |
| ---------------------------- | --------------- | ------------------------------- |
| token\_data\_id              | String          | Token identifier                |
| listing\_id                  | Option\<String> | Listing ID                      |
| collection\_id               | Option\<String> | Collection identifier           |
| seller                       | String          | Seller address                  |
| price                        | i64             | Listing price                   |
| token\_amount                | i64             | Number of tokens listed         |
| token\_name                  | Option\<String> | Token name                      |
| standard\_event\_type        | String          | Normalized event type           |
| is\_deleted                  | bool            | True if the listing is inactive |
| marketplace                  | String          | Marketplace name                |
| contract\_address            | String          | Marketplace contract address    |
| last\_transaction\_version   | i64             | Last transaction version        |
| last\_transaction\_timestamp | NaiveDateTime   | Last update timestamp           |

## `current_nft_marketplace_token_offer`

Tracks current active token offers by token and buyer.

**Primary Key:** `token_data_id, buyer, marketplace`

### Indexes

| Index Name                                               | Columns         |
| -------------------------------------------------------- | --------------- |
| `idx_current_nft_marketplace_token_offers_token_data_id` | token\_data\_id |
| `idx_current_nft_marketplace_token_offers_price`         | price           |
| `idx_current_nft_marketplace_token_offers_buyer`         | buyer           |

### Fields

| Field                        | Type            | Description                  |
| ---------------------------- | --------------- | ---------------------------- |
| token\_data\_id              | String          | Token identifier             |
| offer\_id                    | Option\<String> | Offer ID                     |
| buyer                        | String          | Buyer's address              |
| collection\_id               | String          | Collection identifier        |
| price                        | i64             | Offer price                  |
| token\_amount                | Option\<i64>    | Token quantity               |
| token\_name                  | Option\<String> | Token name                   |
| standard\_event\_type        | String          | Normalized event type        |
| bid\_key                     | Option\<i64>    | Unique bid key               |
| is\_deleted                  | bool            | Offer active status          |
| marketplace                  | String          | Marketplace name             |
| contract\_address            | String          | Marketplace contract address |
| last\_transaction\_version   | i64             | Last transaction version     |
| last\_transaction\_timestamp | NaiveDateTime   | Last update timestamp        |

## `current_nft_marketplace_collection_offer`

Tracks current active collection-wide offers.

**Primary Key:** `collection_offer_id`

### Indexes

| Index Name                                                                        | Columns                                |
| --------------------------------------------------------------------------------- | -------------------------------------- |
| `idx_current_nft_marketplace_collection_offers_collection_id`                     | collection\_id                         |
| `idx_current_nft_marketplace_collection_offers_token_data_id`                     | token\_data\_id                        |
| `idx_current_nft_marketplace_collection_offers_collection_offer_id_token_data_id` | collection\_offer\_id, token\_data\_id |

### Fields

| Field                        | Type          | Description                     |
| ---------------------------- | ------------- | ------------------------------- |
| collection\_offer\_id        | String        | Unique collection offer ID      |
| token\_data\_id              | String        | Token identifier                |
| collection\_id               | String        | Collection identifier           |
| buyer                        | String        | Buyer's address                 |
| price                        | i64           | Offer price                     |
| remaining\_token\_amount     | Option\<i64>  | Remaining quantity in the offer |
| standard\_event\_type        | String        | Normalized event type           |
| is\_deleted                  | bool          | Offer active status             |
| marketplace                  | String        | Marketplace name                |
| contract\_address            | String        | Marketplace contract address    |
| last\_transaction\_version   | i64           | Last transaction version        |
| last\_transaction\_timestamp | NaiveDateTime | Last update timestamp           |

## Other Tables

More info on tables (e.g. `current_token_datas_v2`, `current_collections_v2`, `current_token_ownerships_v2`, `current_collection_ownerships_v2_view`) are available [here](/build/indexer/indexer-api/indexer-reference)

# Transaction Stream Service

> Real-time transaction streaming service for Aptos blockchain data, supporting both Aptos-hosted and self-hosted deployment options

{/* <IndexerBetaNotice /> */}

The Transaction Stream Service is a service that listens to the Aptos blockchain and emits transactions as they are processed. These docs explain how this system works, how to use the Labs-Hosted instance of the service, and how to deploy it yourself.

You can get API access to a transaction stream hosted by Aptos Labs [here](/build/indexer/txn-stream/aptos-hosted-txn-stream).

# Hosted Transaction Stream Service

> Access Aptos Labs hosted transaction stream service with gRPC endpoints for mainnet, testnet, and devnet blockchain data streaming

{/* <IndexerBetaNotice /> */}

If you are running your own instance of the [Indexer API](/build/indexer), or an [Indexer SDK](/build/indexer/indexer-sdk) custom processor, you must have access to an instance of the Transaction Stream Service. This page contains information about how to use the Aptos Labs Hosted Transaction Stream Service.

## Endpoints

All endpoints are in GCP us-central1 unless otherwise specified.

- **Mainnet:** grpc.mainnet.aptoslabs.com:443
- **Testnet:** grpc.testnet.aptoslabs.com:443
- **Devnet:** grpc.devnet.aptoslabs.com:443

You can learn about the rate limits for this service by reading the [Geomi docs](https://geomi.dev/docs/admin/billing).

## Authorization via API Key

In order to use the Labs-Hosted Transaction Stream Service you must have an API key. To get an API key, do the following:

1. Go to [https://geomi.dev](https://geomi.dev).
2. Sign in and select "API Resource".
3. Create a new key. You will see the API key secret in the first table.

You can provide the API key by setting the `Authorization` HTTP header ([MDN](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Authorization)). For example, with curl:

```shellscript filename="Terminal"
curl -H 'Authorization: Bearer aptoslabs_yj4donpaKy_Q6RBP4cdBmjA8T51hto1GcVX5ZS9S65dx'
```

Learn more about API keys at the [Geomi docs site](https://geomi.dev/docs/api-keys).

For more comprehensive information about how to use the Transaction Stream Service, see the docs for the downstream systems:

- [Indexer API](/build/indexer/indexer-api)
- [Indexer SDK](/build/indexer/indexer-sdk)

# Running Locally

> Set up local development environment for transaction stream service using Docker compose and Python scripts for custom processor development

import { Aside } from '@astrojs/starlight/components';

{/* <IndexerBetaNotice /> */}

<Aside type="note">
  This has been tested on macOS 13 on ARM and Debian 11 on x86\_64.
</Aside>

When building a custom processor, you might find it helpful to develop against a local development stack. The Transaction Stream Service is a complicated, multi-component system. To assist with local development, we offer a Python script that wraps a Docker compose file to set up the entire system.

This script sets up the following:

- Single node testnet with the indexer GRPC stream enabled.
- A Redis instance.
- Transaction Stream Service, including the following components:
  - [cache-worker](https://github.com/aptos-labs/aptos-core/tree/main/ecosystem/indexer-grpc/indexer-grpc-cache-worker): Pulls transactions from the node and stores them in Redis.
  - [file-store](https://github.com/aptos-labs/aptos-core/tree/main/ecosystem/indexer-grpc/indexer-grpc-file-store): Fetches transactions from Redis and stores them in a filesystem.
  - [data-service](https://github.com/aptos-labs/aptos-core/tree/main/ecosystem/indexer-grpc/indexer-grpc-data-service): Serves transactions via a GRPC stream to downstream clients. It pulls from either the cache or the file store depending on the age of the transaction.
- Shared volumes and networking to hook it all up.

You can learn more about the Transaction Stream Service architecture [here](/build/indexer/txn-stream) and the Docker compose file [here](https://github.com/aptos-labs/aptos-core/blob/main/docker/compose/indexer-grpc/docker-compose.yaml).

## Prerequisites

In order to use the local development script you must have the following installed:

- Python 3.8+: [Installation Guide](https://docs.python-guide.org/starting/installation/#python-3-installation-guides).
- Poetry: [Installation Guide](https://python-poetry.org/docs/#installation).
- Docker: [Installation Guide](https://docs.docker.com/get-docker/).
- Docker Compose v2: This should be installed by default with modern Docker installations, verify with this command:

```shellscript filename="Terminal"
docker-compose version --short
```

- grpcurl: [Installation Guide](https://github.com/fullstorydev/grpcurl#installation)
- OpenSSL

## Preparation

Clone the aptos-core repo:

```shellscript filename="Terminal"
# HTTPS
git clone https://github.com/aptos-labs/aptos-core.git

# SSH
git clone git@github.com:aptos-labs/aptos-core.git
```

Navigate to the `testsuite` directory:

```shellscript filename="Terminal"
cd aptos-core
cd testsuite
```

Install the Python dependencies:

```shellscript filename="Terminal"
poetry install
```

## Running the script

### Starting the service

```shellscript filename="Terminal"
poetry run python indexer_grpc_local.py start
```

You will know this succeeded if the command exits, and you see the following:

```shellscript filename="Terminal"
Attempting to stream from indexer grpc for 10s
Stream finished successfully
```

### Stopping the service

```shellscript filename="Terminal"
poetry run python indexer_grpc_local.py stop
```

### Wiping the data

When you start, stop, and start the service again, it will re-use the same localnet data. If you wish to wipe the locnet and start from scratch you can run the following command:

```shellscript filename="Terminal"
poetry run python indexer_grpc_local.py wipe
```

## Using the local service

You can connect to the local Transaction Stream Service, e.g. from a custom processor, using the following configuration values:

```shellscript filename="Terminal"
indexer_grpc_data_service_address: 127.0.0.1:50052
auth_token: dummy_token
```

You can connect to the node at the following address:

```shellscript filename="Terminal"
http://127.0.0.1:8080/v1
```

## Debugging

### Usage on ARM systems

If you have a machine with an ARM processor, e.g. an M1/M2 Mac, the script should detect that and set the appropriate environment variables to ensure that the correct images will be used. If you have issues with this, try setting the following environment variable:

```shellscript filename="Terminal"
export DOCKER_DEFAULT_PLATFORM=linux/amd64
```

Additionally, make sure the following settings are correct in Docker Desktop:

- Enabled: Preferences > General > Use Virtualization framework
- Enabled: Preferences > General > Use Docker Compose V2
- Disabled: Features in development -> Use Rosetta for x86/amd64 emulation on Apple Silicon

This script has not been tested on Linux ARM systems.

### Redis fails to start

Try setting the following environment variable before running the script:

```shellscript filename="Terminal"
export REDIS_IMAGE_REPO=arm64v8/redis
```

### Cache worker is crash-looping or `Redis latest version update failed.` in log

Wipe the data:

```shellscript filename="Terminal"
poetry run python indexer_grpc_local.py wipe
```

This means historical data will be lost.

# Self-Hosted Transaction Stream Service

> Deploy your own transaction stream service with indexer fullnode, gRPC manager, and data service components for private blockchain data access

{/* <IndexerBetaNotice /> */}

In order to run Self-Hosted Transaction Stream Service, you will need to run the following components.

Indexer FN \[[https://github.com/aptos-labs/aptos-core/tree/main/ecosystem/indexer-grpc/indexer-grpc-fullnode](https://github.com/aptos-labs/aptos-core/tree/main/ecosystem/indexer-grpc/indexer-grpc-fullnode)]: A FN with indexer grpc functionality enabled. Typically your data service will need to access all historical data, therefore your FN need to sync from genesis in order to bootstrap the whole stack. The pruner can be deleted (through pruner) later on once the data is persisted into file store.

GrpcManager \[[https://github.com/aptos-labs/aptos-core/tree/main/ecosystem/indexer-grpc/indexer-grpc-manager](https://github.com/aptos-labs/aptos-core/tree/main/ecosystem/indexer-grpc/indexer-grpc-manager)]: A centrilized component that manages all the components in the stack. It can run in two mode (master and non-master), only 1 master is allowed. When it is running as master mode, it will also pull data from the upstream FN, and persistent the data into file store (which can be a local file system or Gooogle Cloud Storage).

DataService \[[https://github.com/aptos-labs/aptos-core/tree/main/ecosystem/indexer-grpc/indexer-grpc-data-service-v2](https://github.com/aptos-labs/aptos-core/tree/main/ecosystem/indexer-grpc/indexer-grpc-data-service-v2)]: Provides the client facing streaming Grpc service. It can run in 2 modes. The live mode serves data from its local cache, the historical mode serves data from the file store.

## Example Configs

- FN

```
indexer_grpc:
  enabled: true
  address: 0.0.0.0:50051 # The address to service Grpc request.
  processor_task_count: 32
  processor_batch_size: 100
  output_batch_size: 100

indexer_table_info:
  table_info_service_mode: IndexingOnly
  parser_task_count: 10
  parser_batch_size: 100
```

- GrpcManager

```
health_check_port: 8081 # The port for monitoring purpose.
server_config:
  is_master: true # Whether running in master mode.
  service_config:
    listen_address: 0.0.0.0:50052 # The port that serves Grpc requests.
  self_advertised_address: 0.0.0.0:50052
  grpc_manager_addresses: # All GrpcManager addresses in the stack, need to point to the server_config.self_advertised_address in GrpcManager config.
    - >-
      http://0.0.0.0:50052
  fullnode_addresses: # All upstream FN addresses in the stack, need to point to the indexer_grpc.address in FN config.
    - >-
      http://0.0.0.0:50051
    - >-
      http://other-fullnode.xyz:50051
  file_store_config:
    file_store_type: GcsFileStore
    gcs_file_store_bucket_name: indexer
    gcs_file_store_service_account_key_path: /secrets/indexer-sa-key
  chain_id: 1
```

- DataService

```
health_check_port: 8081 # The port for monitoring purpose.
server_config:
  chain_id: 1
  self_advertised_address: 0.0.0.0:50053
  grpc_manager_addresses: # All GrpcManager addresses in the stack, need to point to the server_config.self_advertised_address in GrpcManager config.
    - >-
      http://0.0.0.0:50052
  service_config:
    listen_address: 0.0.0.0:50053
  live_data_service_config: # For live data service.
    enabled: true
    num_slots: 5000000 # Max number of transactions to cache.
    size_limit_bytes: 10000000000 # Cache size in bytes.
  historical_data_service_config: # For historical data service.
    enabled: true
    file_store_config:
      file_store_type: GcsFileStore
      gcs_file_store_bucket_name: indexer
      gcs_file_store_service_account_key_path: /secrets/indexer-sa-key
```

## Usage

- Use GrpcManager for routing / load balancing

Call GrpcManager.GetDataServiceForRequest first, it will return the address of a data service instance.

Then Call DataService.GetTransactions.

- Use DataService directly

Call DataService.GetTransactions directly. In this case you might want to run both live data service and historical data service together.

## Advanced Usage
